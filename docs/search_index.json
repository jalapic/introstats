[["index.html", "PSY317L &amp; PSY120R Textbook Chapter 1 Welcome to PSY317 / PSY120R ! 1.1 What this book includes and what it doesnt 1.2 How to use this guide 1.3 Acknowledgements 1.4 References 1.5 Other places to find help about R and Statistics", " PSY317L &amp; PSY120R Textbook James P. Curley &amp; Tyler M. Milewski 2021-08-21 Chapter 1 Welcome to PSY317 / PSY120R ! This book is written to help students enrolled in the University of Texas at Austin Introduction to Statistics for the Behavioral Sciences (PSY317L) course or R Programming for Behavioral Sciences (PSY120R) led by Professor James Curley. We hope that the book will be a useful resource to help you learn both R and statistics. If you have any suggestions for improvements, please get in touch with Professor Curley. 1.1 What this book includes and what it doesnt This is in between a textbook and a study guide. Were trying to build materials that will enable students to quickly find what theyre looking for to help them understand these statistical concepts. This book is primarily aimed at the content in PSY317L and PSY120R, but occasionally we describe concepts that we dont directly cover in these courses. This is when we feel that its worth explaining things in a bit more detail for those students that want to know a bit more about a subject. 1.2 How to use this guide This guide covers the material presented in the video segments. In some parts, we go beyond what is presented. This is to supplement the material and to help you understand the content or to round out some subjects a little more. You will not be tested directly on information that is in this guide if it did not appear in the video segments. Quizzes and tests are only based on the videos. This guidebook is exactly that - an extra reference to help you understand that material and a place to look for help or answers. 1.3 Acknowledgements This guidebook is built upon the work of several others teaching materials. In particular, Danielle Navarros book Learning statistics with R: A tutorial for psychology students and other beginners. A couple of data examples also come from the book Discovering Statistics Using R Fields intro to statistics using R. 1.4 References The following books are all freely available online (like this one!) and are helpful resources. For Statistics and R Help: Danielle Navarro - Learning statistics with R: A tutorial for psychology students and other beginners For more on Data Visualization including using R: Claus O Wilke - Fundamentals of Data Visualization Hadley Wickham - ggplot2: Elegant Graphics for Data Analysis For more on using R as a programming language: Garrett Grolemund &amp; Hadley Wickham - R for Data Science 1.5 Other places to find help about R and Statistics With statistics stuff, you can never really have enough resources. Although throughout this course we try to explain difficult concepts in terms we hope all students can understand, sometimes you just need to read or hear about these concepts several times before they sink in. Other times, its not until somebody else explains it in a slightly different way that it finally finds a home in your brain. To that end, we recommend looking over some other help tutorials and reading. You certainly dont have to go looking at these materials - but if you feel like hearing a different voice, then these are some that I recommend. R stuff Rstudio Cheatsheets R Community R Cookbook R for Data Science Stats stuff Modern Dive by Chester Ismay &amp; Albert Kim Khan Academy videos "],["introduction.html", "Chapter 2 Introduction 2.1 Downloading R 2.2 Downloading RStudio 2.3 Using RCloud 2.4 The RStudio Environment 2.5 Running Code 2.6 Packages 2.7 Working with RStudio in PSY317L 2.8 Quitting RStudio", " Chapter 2 Introduction This chapter will show you how to get started with R and Rstudio. R is the programming language. It is what enables us to write code and make things happen. RStudio is the piece of software that we use to make working with R simpler. It is a graphical interface. When both of these are downloaded and installed, we always work by opening RStudio. R just lives happily in the background and we never need to directly open it. The first thing to do is to download and install R and RStudio. You should do it in that order - download R first, and then RStudio. The exact steps you need to take for each will differ depending on if youre using a Windows, Mac or Linux. This is explained below. One extra thing to note. At the time of writing this book, the latest R version is R4.0.2 and the latest RStudio version is RStudio.1.3.1056. These numbers will almost certainly be different by the time you come to install each of these. Thats not a problem - just download the most recent version that exists - these will be at the top of the screen at the links we explain below. 2.1 Downloading R Go to the R website. The first box has options for downloading R for Mac, Windows or Linux. Click on the appropriate link for your operating system. R for Windows For windows users, after you have clicked the appropriate link below, you will see the following screen and you should click on \\(\\color{blue}{\\text{&quot;install R for the first time&quot;}}\\). After this page, you will see the following page: You want to click the link that says \\(\\color{blue}{\\text{&quot;Download R 4.0.2 for Windows&quot;}}\\) that is right at the top of the screen. If the version is more recent than 4.0.2, then click on that instead - just make sure its the link at the very top of the page in the gray box. This will start downloading R. Just follow the instructions. You do not need to change any of the default settings, just click accept for each. One piece of advice is to try and keep R in a folder on your machine such as Documents and dont put it in a cloud folder. We have seen some students have issues when R has been located in cloud folders. R for Mac OS X If you have a Mac operating system, then you will see the image below. You want to click on \\(\\color{blue}{\\text{&quot;R-4.0.2.pkg&quot;}}\\). If a more recent version of R is available, then the version number will be higher than 4.0.2 - click on this instead. Again, follow the instructions. You can accept all the default settings for the installation, you dont need to change anything. It is also best to leave this in a folder located on your machine such as Documents and not in a cloud based folder. 2.2 Downloading RStudio Go to the Rstudio website. Scroll down so you see something that looks like the image below. Click DOWNLOAD under RStudio Desktop, open source license FREE. After you click on that box, you should see something that looks like the below image on the next page: This will name the most recent version of RStudio (here it is 1.3.1056), dont worry if yours is a higher number than this. It will remind you that you should have installed R already (thats the number 1 in the image). Then it says you should download RStudio Desktop. It usually gives you a suggested download and that will be in the blue rectangle under point 2. You can just click that to start your RStudio download. You can then download the software. If you dont see that blue rectangle, then you can choose an appropriate installer for your operating system (Mac, Windows, or Linux) from the list that comes further down that webpage. It looks like this: If you have Windows, you will want to click on the first link \\(\\color{blue}{\\text{&quot;RStudio-1.3.1056.exe&quot;}}\\). If you have Mac, you should click on the second link \\(\\color{blue}{\\text{&quot;RStudio-1.3.1056.dmg&quot;}}\\). Once you have downloaded Rstudio installer, click on it to start the full installation of the program. Again you can just accept all the default settings. We also advise that, as with R, you save RStudio to a location on your machine such as your Documents folder rather than in a cloud folder. 2.2.1 Successful installation If you have successfully installed your R and your RStudio then you can do the following to check that it works. Find the RStudio shortcut on your Desktop, or find RStudio in your programs. Click on the program or shortcut icon to open up RStudio. The RStudio icon looks like this: Alternatively the R icon looks like this: We always use RStudio to start our work, we never need to open R or click on the R icon. Once you have opened RStudio it should look something like this: 2.3 Using RCloud RCloud RCloud is a web-based platform for Rstudio, which allows you to perform all visualization and analysis with your data, without downloading Rstudio on to your machine. We had considered using RCloud as a backup option for students if they had trouble downloading and installing R or RStudio. However, RCloud has changed a lot of its accessibility options recently and so we are no longer going to use this. 2.4 The RStudio Environment Your Rstudio environment is separated into 4 main panes. On your machine these panes by be organized differently. You can also change the order of these panes by going to your tools tab, then clicking Global preferences. First, in the top left, you have a source pane. This is where you will be writing most of your code. You will have to run the code, before seeing the output in the console. This pane is important because this is where you can write and save code for future use. Second, on the bottom left, there is the Console. All of the code and the codes output that you run from the source panel will show up in the console. You can also directly write code in the console. However, objects in the console will not be saved. Third, you have a Global Environment panel in the top right. When you first open RStudio this will be empty, but it fills up with things once you start working with the code. You will also see some tabs above the Global Environment. You should see the Environment and History tabs. You may also see other tabs up there including Git, Tutorial depending on the version of RStudio you have downloaded and what other programs you have on your machine. There are also some other buttons including the Import Dataset button above this panel. Here is an example of a Global Environment where you can see dataframes, matrices, values, and functions that are being worked with: If you click the play arrow next to the dataframe, you can view the entire dataframe. This is the save as running the function View(df) (see section 4.2). Another, important button is the broom icon, if you click this it will clear your global environment and you will have to rerun code to get any dataframes, function, etc. back. In the History tab, you can see all the code that was once ran through your console. This tab can be important, because information in the console is never saved and can be deleted easily. Nevertheless, you should get use to saving your code in R scripts, which well describe shortly, so you will probably not use this tab to often. The fourth pane, is the most compacted and has tabs for Files, Plots, Packages, Help and possibly Viewer depending on your version. The file tab, allows you access all files on your computer. The initial location for this is likely to be the folder that you saved R and Rstudio in - usually My Documents or something similar. You can navigate to other folders on your computer using the button with the three dots ... that looks like this: That will open up your computers folders and you can click to a new folder. The Plot tab, will be filled with plots generated from your code. There are buttons right under the tab to Zoom and Export plots. There is also a button which looks like a broom, this will clear the plots. The Packages tab, has a User Library which is just a list of all the packages you already have install on your machine. You can also install a package, by hitting the install button and then search for your wanted package. The update button will update all packages you already have installed. The Help tab, will give you information to help understand different functions and commands in R. You can also call for help with a function right from the console like this: ?t.test 2.5 Running Code 2.5.1 The Console In R, you can code run in the console and an Rscript file. In the console, the bottom left panel, you type code and run it by hitting Enter or Return on your keyboard. R will show the results in the console as well. You should write this code immediately after the command prompt which is a blue &gt; sign with a flashing cursor bar after it. Code written in the console looks like this: A downside of using the console is that all code and results will be forgotten when the R session is closed. This is why it is better to you use Rscripts. 2.5.2 RScript To open a new Rscript you can: Click on File -&gt; New File -&gt; R Script in the menu bar of RStudio. Click on the button in the toolbar in RStudio that looks like a white sheet with a green plus sign. This is located right under File in your Rstudio environment: Hit \"Ctrl\"+ \"Shift\" + \"N\" on your keyboard if you have Windows, or \"Command\"+ \"Shift\" + \"N\" if you have Mac. Once you have opened a new Rscript it will looks like this: You will see that it says Untitled1 which means this is a new script file and you havent saved it yet. Youll also see a gray 1 on the side. This means that that is line 1 of the code. At the moment there is no code. Here is a script that has a few lines of code in it: There are a few things to point out here. First, notice that the gray numbers now go to 11. This means we have 11 lines in the script. Some lines start with a # sign. These lines contain text (in green in our version) that is not code. After you put a # you can write notes to yourself or to others and R will ignore them - it will not run them as code. Anything else that is written, e.g. the x &lt;- 3 + 5 and the x + 1 is in black or blue text and can be run as code. 2.5.3 Saving an RScript To save your Rscript, hit the blue disk icon on the script toolbar and you can name your script. You can save it anywhere on your computer, but probably best to save it in an easy to remember folder. This allows you to come back later to finish working on some assignment even after you have closed the R session. RScript folders will be saved with an .R extension. If you open several scripts, they will arrange themselves as tabs and you can click between them. That could look like this: 2.5.4 Open an existing RScript If you already have an RScript that you wish to open, you can go to the folder that contains this .R file and double click on it to open. Alternatively, you could hit the folder icon on the RStudio toolbar and use that to locate the file you wish to open. If it is a file that you have viewed recently, then you can hit the down arrow to the side of the folder icon on the toolbar and it will list your most recent files. You can select the one you want from this list. It will look something like this: 2.5.5 Running Code in Scripts There are several ways to run the code in your Rscript file. You can highlight your code, then hit Run in the top right corner of the script file. You can click anywhere on the line that contains the code you wish to run, then hit Run You can highlight or click on the code you want to run, then use the keyboard shortcut Command + Return on Mac, or Control + Enter on Windows. You can click anywhere on the line that contains the code you wish to run, then use the keyboard shortcut Command + Return on Mac, or Control + Enter on Windows. 2.6 Packages When working with R, you will hear about different packages that we will utilize to complete a variety of tasks. A package is a collection of code, functions, and sometimes data developed by members of the R community with the purpose of freely sharing with other R users. We install packages to help us work more efficiently in R. You can think of them as similar to apps youd download from the app-store. There are two ways to download packages on to your computer. The first way is through the Packages tab in the right bottom panel in your Rstudio environment. In this tab you will see all the packages already installed on your machine. Here if you click on install, a box will pop up and your can enter the name of the package you wish to to have then hit install. The second way is through one line of code you can write in the console. For example, to just install the tidyverse package (which is actually a package that contains many packages within it) you would type install.package(\"tidyverse\") into your console like this: At the beginning of this class, you will be asked to install all the packages that you will need to run the various scripts used in this class. Youll only need to do this once. To do this, please copy and paste the following code in your console and hit enter: install.packages(c(\"car\", \"coin\", \"gridExtra\", \"lsr\", \"moments\", \"ppcor\", \"psych\", \"rcompanion\", \"tidyverse\")) Your Console should look something like this After you do that, your console should start spurting out lots of red, blue and black text. This may take a while (up to 10 minutes) to complete. You may get a prompt that shows up asking you if you want to download from source or some such statement. Its ok to hit Y in the console if asked there, or to hit OK or Yes on the dialog box that pops up. If this question doesnt arise, then thats fine. It should look something like this during the middle of the installation: If it has finished installing the packages successfully, then you should see something like this at the end: To test whether the main package that we will use tidyverse is downloaded correctly, then type the following in the console after the blue &gt; library(tidyverse) You should see something like the following appear - the red X under Conflicts are ok in this instance (they are just saying that the functions filter() and lag() are used by more than one package: If you have any issues with the installation process then please reach out to a TA or Professor Curley through the discussion page. We will get back to you and help. 2.7 Working with RStudio in PSY317L For this course, all the files that you will need to work with are contained in a single folder. This contains whats called a project folder in RStudio. To get this folder, you need to download the PSY317L_files.zip zip folder from Canvas. Once you have done that, double click the folder to extract all the files. The top level should just be a folder called PSY317L. If you click on that folder to open it, you should see the folders and files in the image below. Next, move that folder to somewhere on your computer. Ideally in your Documents folder would be the best place. Please check that if you click on that folder you see these folders and files. Importantly, you should see one file that is an RProject file called PSY317L. It will have a blue transparent cube icon with an R inside of it. For our course, the best way to access the R files is to hit the blue Rproject icon PSY317L. Once you click that, RStudio should automatically open and you should see something like this: Notice the red squares that weve added to the image. These all say PSY317L and indicate that youre working in the PSY317L RProject. You should also see the blue R cube in various (three) places on your Rstudio. Finally, you should see in your Files tab in the bottom right a list of all the files and folders that you will need in this course. To open any individual file, you just need to click on the appropriate folder. So, if we wanted to open the file paired_t_test.R, we would click on the Statistics folder and then click the paired_t_test.R file. Then our RStudio would look like this: Notice that the file has opened up in the top-right panel. To navigate back to the top folder that contains all our folders, you can hit the PSY317L label next to Home highlight in the red box. To close a file, just hit the x next to the file name in the top-right panel. 2.8 Quitting RStudio The easiest way to close working with RStudio is to hit the X in the top right corner of RStudio. If you have not done any work, this will just quite the program immediately. If you have been working with any RScript file and have made changes but have not saved it yet, then youll get the following prompt: It is asking you The document xxx.R has unsaved changes. Do you want to save these changes?. Most of the time youll want to hit Save and then it will quit. If you have saved all your RScript files, but you also ran code and perhaps have some things still present in the global environment (e.g. dataframes) or your have plots in your Plot window, you may get the following prompt: It asks you Save workspace image to ~/PSY317L/.RData?. You can safely hit Don't Save for this to just quit RStudio. "],["basic-syntax.html", "Chapter 3 Basic Syntax 3.1 Simple mathematical syntax 3.2 Assignment 3.3 Vectors 3.4 Characters 3.5 Naming of objects 3.6 Logical Operators 3.7 Some things that are useful to know. 3.8 Error Messages 3.9 Functions 3.10 Chaining Syntax", " Chapter 3 Basic Syntax Before we move forward it is important to know a little bit of R syntax. This chapter contains lots of information about how R is organized and works. Dont feel like you have to learn all of this now. This chapter is here for reference and for help. Also, these nuts and bolts of programming languages are things that you pick up over time - they are pretty hard to remember at first and you constantly do have to check and copy/paste things. Thats ok. Another very helpful resource on basic syntax in R is Chapter 3 of Danielle Navarros book which we highly recommend. 3.1 Simple mathematical syntax R is, first and foremost, a statistical programming language. So, unsurprisingly, you can do a bunch of mathematical operations fairly easily. Lets do some really tedious math: You can add with the + key: 4 + 6 ## [1] 10 You can see that the output returned 10 indicating that that is the answer to 4 + 6. However, we also see in the output a [1] before it. What does this mean? This is an index - which well talk about in more detail later- but essentially all it is telling us is that the number that comes directly after it is the first bit of the output. We only have one thing in our output here - the number 10, so its pretty pointless - but it does have relevance in the future. For now though, you can safely ignore it. Subtracting uses a hyphen or minus key -: 43 - 18 ## [1] 25 Multiplying uses the asterisk *: 5 * 3 ## [1] 15 Dividing uses this slash key /: 34 / 7 ## [1] 4.857143 You can square root like so by typing sqrt() and putting your number to square root in the brackets: sqrt(45) ## [1] 6.708204 Squaring a number requires us to use the cute ^ key followed by a number 2 to indicate that we are raising to the power 2 (which is squaring): 12^2 ## [1] 144 We can also get the logarithm of numbers: log(12) ## [1] 2.484907 Our first little warning Like all programming languages, R will only do what its told. One thing that catches people out is that it will read code in order from left to right. So, if youre doing several things at once, you may need to insert brackets to make sure that it does what you want it to. For instance, say you want to multiply 5 by the sum of 7 and 9. You might write that like this: 5 * 7 + 9 ## [1] 44 but 44 is not the answer youre looking for! What it has done is to take 5, then multiply it by 7 (makes 35) and then add 9 to make 44. OK, how about you write it like this instead: 7 + 9 * 5 ## [1] 52 Erm, this time it multiplied 9 by 5 to get 45 and then added 7 to make 52. What is happening is that some mathematical operations are taking precedence over others and R does things in certain order. To explicitly make sure that it does what you want, insert some brackets like this: 5 * (7 + 9) ## [1] 80 Now it knows that what you wanted to do was to add 7 and 9 to get 16 and have that number multiplied by 5 to get 80. This sort of thing is something to look out for, but as you get experienced youll have a sense for it. Well placed brackets can save you a headache. 3.2 Assignment e.g. what the assignment operator is. This symbol &lt;- which is just a less than sign followed by a hyphen is called an assignment operator. It basically is equivalent to saying you want to save something. You write what you want to be saved on the right hand side of it, and the name of your newly saved thing on the left of it. We call the thing that youve saved an object in programming speak. For instance, say you wanted to save the number 17, and you wanted to call that saved number an object called x. Youd do it like this: x &lt;- 17 Now, whenever you type, x, R thinks that it is the number 17. x ## [1] 17 We should probably fess up right now and tell you that there is another way that you can assign things in R. On the face of it, its a much easier way too, but were going to recommend you dont do it. The only reason that were bringing it up at this point is that if you look up help on the internet or in some books, youll see people doing it - so we should mention it. You can assign using the equal key = like this: y = 10 - 2 y ## [1] 8 So, as you can see, we created an object called y that was equal to 10 minus 2, that is 8. Using = seems so much easier than using &lt;- so what is the reason not to do it? Well, the equal sign gets used for a ton of other commands and sometimes it gets a little messy and confusing if too many equal signs fly around. Therefore, were going to politely ask that whenever you assign things, please use the &lt;- operator, even though its two bits of punctuation stuck together: hooray &lt;- 17 + 4 # thanks for using this sign hooray ## [1] 21 3.3 Vectors Of course, you can save even more complex things as the object. For example, if you wished to save the numbers 5, 6, 7, 8, 9, 10, you have two ways of doing that. Lets just see them in action, and then well explain the syntax: v1 &lt;- 5:10 v1 ## [1] 5 6 7 8 9 10 v2 &lt;- c(5,6,7,8,9,10) v2 ## [1] 5 6 7 8 9 10 As you can see both v1 and v2 are our newly saved objects and they both contain the numbers 5 through 10. For v1 we separated the numbers 5 and 10 with a colon :. In R, the : sign can be used to mean to when talking about a series of numbers. e.g. 5:10 ## [1] 5 6 7 8 9 10 101:103 ## [1] 101 102 103 The other way we did it with v2 was to use the c() function. This stands for concatenate which is a mouthful. Basically, its a way of sticking things together. You write c() and then put the stuff you want to stick together inside the brackets, but make sure you separate each thing by a comma ,. For example, c(1,10,100,1000) ## [1] 1 10 100 1000 c(4,6,8,10,8,6,1) ## [1] 4 6 8 10 8 6 1 Another bit of terminology might be worth mentioning right now. Our saved objects x, v1 and v2, as well as being called objects can also be called vectors. A vector is something in R that contains several items. These items are actually called elements. Importantly the items are indexed, which means that they are in order, not all jumbled up. That means that you can directly grab an element by its position. So, if you wished to get the first element (or item) of the object v1 youd type like this: v1[1] ## [1] 5 That returns 5 which was our first element we put into the vector. If we wish to get the 3rd element of v1 wed do this: v1[3] ## [1] 7 And, if we wished to get the first, second and third element of v1 wed do this: v1[1:3] ## [1] 5 6 7 We can also get non-consecutive elements of vectors. If we wanted to get the first, fourth and sixth elements of v1 wed do this: v1[c(1,4,6)] ## [1] 5 8 10 Notice here that you cant do this: v1[1,4,6] ## Error in v1[1, 4, 6]: incorrect number of dimensions You have to put the indexes of the elements you want inside c() and separate each with a comma, if they are non-consecutive. Be careful to make sure youre referring to elements that actually exist. The vector v1 has six elements in total, so if you ask it to give the seventh element, then you get the following: v1[7] ## [1] NA It just says NA. This is R for nothing or doesnt exist or missing. Youll also see NaN in some situations to represent missing data. Thats quite a lot about vectors, their elements, and how to index them. Hopefully the below image will help you remember this terminology: Working with Vectors Now, one of the great advantages of vectors is that you can do things to all of the numbers inside the vectors at once. For instance, say we wanted to square all the numbers inside of v1, then all we need to do is: v1^2 ## [1] 25 36 49 64 81 100 and notice how it squared the numbers 5 through 10 that are the contents of v1. You can also do things to the entire contents of a vector at once. For instance, if you wanted to add up all the numbers in v1 you could do this by using the function sum() like this: sum(v1) ## [1] 45 and it added 5+6+7+8+9+10 to give one answer! You can also use the names of the vectors together. For example, if you do v1 + v2 lets see what happens: v1 ## [1] 5 6 7 8 9 10 v2 ## [1] 5 6 7 8 9 10 v1 + v2 ## [1] 10 12 14 16 18 20 You can see that it added the first element of each vector to each other, then the second element of each vector to each other and so on. This only works though because v1 and v2 are the same length. If they had unequal number of elements, R would get mad and not do anything. 3.4 Characters The vectors that weve been looking at so far are all numeric meaning that they contain numbers. Another type of information that R uses is text - or characters. Sometimes called strings. Lets make a vector of characters: the_week &lt;- c(&quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, &quot;Friday&quot;, &quot;Saturday&quot;, &quot;Sunday&quot;) the_week ## [1] &quot;Monday&quot; &quot;Tuesday&quot; &quot;Wednesday&quot; &quot;Thursday&quot; &quot;Friday&quot; &quot;Saturday&quot; &quot;Sunday&quot; Here we have an object called the_week that has 7 elements. Each element is separated by a comma. This is pretty similar to when we enter numbers except for situation when we can use the : shortcut such as using 5:8 to mean 5, 6, 7, 8. We also stick everything together into a vector using c() as we did with numbers. The major difference is that to make sure R knows that each of these words is a character or text string, we have put quote marks around each word. If we didnt do that, it would be looking for objects called Monday, Tuesday etc. Actually, lets see an example of using words to create a vector, when the words are actually objects: aa &lt;- 5 bb &lt;- c(9, 11) cc &lt;- 10 dd &lt;- c(&quot;james&quot;, &quot;tyler&quot;) ee &lt;- &quot;hedgehog&quot; aa ## [1] 5 bb ## [1] 9 11 cc ## [1] 10 dd ## [1] &quot;james&quot; &quot;tyler&quot; ee ## [1] &quot;hedgehog&quot; example &lt;- c(aa, bb, cc, dd, ee) example ## [1] &quot;5&quot; &quot;9&quot; &quot;11&quot; &quot;10&quot; &quot;james&quot; &quot;tyler&quot; &quot;hedgehog&quot; Here, each of aa to ee is assigned to be an object and they contain information. When we write c(aa, bb, cc, dd, ee) we dont need to use quotes as were referring to the objects and not to words called aa, ee etc. Also notice that we were able to make vectors that mixed together numbers and characters. Also, some of our original vectors like bb and dd contained more than one element. When we put them all together with c(aa, bb, cc, dd, ee) to create the example object, R just stuck them all together in the order we wrote. Its pretty unusual though to mix numeric and character vectors together. Lets get back to talking about vectors that contain characters only. Some functions can work on both character vectors and numeric vectors. One example is length() which counts the number of elements in a vector: v1 ## [1] 5 6 7 8 9 10 the_week ## [1] &quot;Monday&quot; &quot;Tuesday&quot; &quot;Wednesday&quot; &quot;Thursday&quot; &quot;Friday&quot; &quot;Saturday&quot; &quot;Sunday&quot; length(v1) ## [1] 6 length(the_week) ## [1] 7 However, some functions only make sense when working with one type or the other. For instance sum() to add up the elements of a vector, only works with numeric data: sum(v1) ## [1] 45 If you try it with character data, then it will give you an error: sum(the_week) ## Error in sum(the_week): invalid &#39;type&#39; (character) of argument Others work with characters. For example, nchar() calculates the number of letters in a piece of character text. Applying it to a vector will tell you how many letters are in each of the days of the week: nchar(the_week) ## [1] 6 7 9 8 6 8 6 If you try this with numeric vectors, it gives you a funny output of how many characters are in each number! nchar(v1) ## [1] 1 1 1 1 1 2 Regardless of whether you are using numerical or character vectors, indexing works in the same way. e.g. To get the 3rd element of each vector: v1[3] ## [1] 7 the_week[3] ## [1] &quot;Wednesday&quot; e.g. to get the 5th and 5th elements of each vector: v1[5:6] ## [1] 9 10 the_week[5:6] ## [1] &quot;Friday&quot; &quot;Saturday&quot; e.g. to get the first, fourth, and sixth element of each vector: v1[c(1,4,6)] ## [1] 5 8 10 the_week[c(1,4,6)] ## [1] &quot;Monday&quot; &quot;Thursday&quot; &quot;Saturday&quot; 3.5 Naming of objects One of the hardest things in R is deciding what to name things. You run out of ideas very fast! Theres only so many times you can call things x or df or v1 before you get bored. The best names tend to be short and memorable. If you were saving a vector of color names for example, you might want to call it mycolors: mycolors &lt;- c(&quot;mistyrose&quot;, &quot;dodgerblue&quot;, &quot;pink&quot;) There are some important rules for the naming of objects. These are some DOs and some important DONTs: you can use characters or numbers in object names, or both do not use spaces try to use lower case ideally do not use any punctuation in names for objects, dataframes or column names except for the period . or underscore _. Everything else is forbidden. if you do use an underscore _ it cannot go at the beginning, e.g. _mynumber &lt;- 17 ## Error: &lt;text&gt;:1:1: unexpected input ## 1: _ ## ^ you can use a period . at the beginning of an object name, but please dont. periods . and underscores _ are best used in the middle of object names to help read the name, e.g. prime_numbers &lt;- c(1,3,5,7,11,13) prime_numbers ## [1] 1 3 5 7 11 13 but even better, use short object names! primes &lt;- c(1,3,5,7,11,13) primes ## [1] 1 3 5 7 11 13 you should avoid using certain words, as they have other meanings in R and calling something else by that name could confuse things. Examples of words to avoid: if, else, repeat, library, break, while, stop, function, for, in, next, TRUE, FALSE, NULL, Inf, NaN, NA, data, etc. That still leaves a lot of words you can use. Usually when you start to write one of these words in R, youll notice that they change color in the script (to which color depends upon your color settings) - this indicates that these words are special. for the same reason, there are several letters that I would avoid using, because they have special meaning: e.g. c, t, T, F. The last two are short for TRUE and FALSE. Weve already seen c() in action. t() means to transpose data. You might find that weve called some objects of numbers t in this course - sorry in advance if we broke this rule. 3.6 Logical Operators The R language contains several bits of punctuation that can be used for whats termed logical operators. These will lead to an output of either TRUE or FALSE being printed to the console. Its easiest to work out what they do by just seeing them in action: &quot;is equals to&quot; ## [1] &quot;is equals to&quot; 2+2 == 4 ## [1] TRUE 2+2 == 5 ## [1] FALSE The double equals sign == checks if the left hand side is equal to the right hands side. In the top example, 2+2 does equal 4 so we get a TRUE, while in the bottom example, 2+5 does not equal 5 so we get a FALSE. You dont have to check one thing at a time. You could check all elements of a vector at once: x &lt;- c(5, 10, 15, 10, 1, 11, 10) x == 10 ## [1] FALSE TRUE FALSE TRUE FALSE FALSE TRUE Here it is saying which elements of x are equal to 10 or not. If you want to ask whether something is not equal to something, then you use != like this: x &lt;- c(5, 10, 15, 10, 1, 11, 10) x != 10 ## [1] TRUE FALSE TRUE FALSE TRUE TRUE FALSE Other logical operators are &lt;, &lt;=, &gt; and &gt;= and can be used in the usual mathematical way: x &lt;- c(5, 10, 15, 10, 1, 11, 10) x &lt; 10 ## [1] TRUE FALSE FALSE FALSE TRUE FALSE FALSE x &lt;= 10 ## [1] TRUE TRUE FALSE TRUE TRUE FALSE TRUE x &gt; 10 ## [1] FALSE FALSE TRUE FALSE FALSE TRUE FALSE x &gt;= 10 ## [1] FALSE TRUE TRUE TRUE FALSE TRUE TRUE You can also add up the logical output. Say you wanted to know how many of your vector of x were equal to 10. You can use sum() to add up. With the TRUE AND FALSE output, sum() will count up the number of TRUEs you have. (It considers TRUE to be 1s and FALSE to be 0s) x &lt;- c(5, 10, 15, 10, 1, 11, 10) x == 10 ## [1] FALSE TRUE FALSE TRUE FALSE FALSE TRUE sum(x==10) ## [1] 3 Indexing Vectors with Logical Operators This is all a bit abstract and vague as to why we should care about logical operators. One common situation we use them is when we want to subset vectors. Remember, we use the square brackets [] to index a vector like this: x &lt;- c(5, 10, 15, 10, 1, 11, 10) x[5] ## [1] 1 x[1:2] ## [1] 5 10 Above we first got the 5th element of x and then we got the first and second elements of x. We can also grab things from vectors using logic. Effectively keeping those elements that give the output TRUE in response to the logic statement. Here are examples: x &lt;- c(5, 10, 15, 10, 1, 11, 10) x[x==10] ## [1] 10 10 10 x[x!=10] ## [1] 5 15 1 11 x[x&gt;10] ## [1] 15 11 Another funky piece of syntax that comes in useful from time to time is %in%. You can basically use it only keep those elements that match those that exist inside another element. stuff &lt;- c(1, 5, 15) x[x %in% stuff] ## [1] 5 15 1 3.7 Some things that are useful to know. whitespace R doesnt, on the whole, care that much about whitespace - this is space that is between code. It just ignores it. For instance, all of the following give the same result (whats happening here is that a and b are vectors that each have three elements. Telling a+b will make it add the first element of a to the first element of b and then the 2nd element of a to the 2nd element of b and so on: a &lt;- c(1,2,3) b &lt;- c(4,5,6) a + b ## [1] 5 7 9 a&lt;- c( 1, 2, 3) b &lt;- c(4,5,6 ) a + b ## [1] 5 7 9 a &lt;- c(1 ,2, 3) b &lt;- c( 4,5 ,6) a + b ## [1] 5 7 9 Hopefully, you get the point. You can also leave whitespace across lines and R will, for the most part, jump to the code ahead and not worry about the whitespace. This example looks ugly because the commas, numbers, and brackets are all on different lines, but R reads it ok: x &lt;- c( 4, 5, 3, 6 , 14 ) x ## [1] 4 5 3 6 14 Although this is possible. Please, please, dont do it! Its ugly to read, and whats more, there are situations in which it can cause you problems. Indeed, there are some important place where whitespace needs to be adhered to. Firstly, when using the assignment operator &lt;-. R cannot stand it if you put a space in between the &lt; and the -. e.g. eg &lt; - 5:10 ## Error in eval(expr, envir, enclos): object &#39;eg&#39; not found eg ## Error in eval(expr, envir, enclos): object &#39;eg&#39; not found It is stating here that object 'eg' not found because it thinks youre trying to ask it if eg is less than -5:10. If eg already existed, then you need to be extra careful here. We wont go into what could happen in that situation, but it could be bad! So, golden rule, no spaces with &lt;- ! Secondly, in future sections on carpentry and visualization, youll use some syntax that pipes or chains together a series of commands. These operators are + or %&gt;%. When using this syntax, you need to finish each line (expect the last line) with one of these pipe syntaxes. If you do not do that, youll get an error message, because R wont jump to the later lines when it is chaining things together. Although this might be a bit early in your R journey to mention - it is such a frequent error, that we just wanted to reference it here. Hopefully, this will be a useful reference aid if you see this error. Heres a simple example of what this looks like. x &lt;- c(3, 5, 6, 7, 4, 6, 5, 10, 4, 5, 7) x %&gt;% unique ## Error: &lt;text&gt;:4:1: unexpected SPECIAL ## 3: x ## 4: %&gt;% ## ^ Here we get an error message saying Error: unexpected SPECIAL in \"%&gt;%\". What has happened is that R read the lines where we assigned the numbers to x and then on the line that just had x it printed out the numbers. Then when it got to %&gt;% unique it didnt know what this was referring to. When using chain syntax such as %&gt;% you cannot start a new line with it. Instead you should write: x &lt;- c(3, 5, 6, 7, 4, 6, 5, 10, 4, 5, 7) x %&gt;% unique ## [1] 3 5 6 7 4 10 Now, it has processed the code appropriately and the output is what we want. Basically, we took our vector of x and extracted all the unique numbers. A golden rule should be - be neat with your code !!! and this will avoid a lot of errors and problems. Other common errors Close your brackets Missing commas or brackets might be the most common errors! You will get an error message though, and it usually will tell you what you did wrong. Also, when you start to type ( RStudio will automatically autocomplete to write () which helps you to not forget to close your bracket. a &lt;- c(4, 4, 2, 7 ## Error: &lt;text&gt;:2:0: unexpected end of input ## 1: a &lt;- c(4, 4, 2, 7 ## ^ When you forget to close your brackets, but you run your code anyway, youll notice that in your console you get a flashing bar and the cursor still has a plus sign like this : This is telling you that its waiting for something else to finish the code. It expected you to close your brackets! If you do this and you want to reset to get back to the &gt; cursor sign (which means its ready for action), then just click next to the plus on the flashing | and hit Esc key. This should reset it to the &gt; cursor, and look like this: Dont forget commas Commas need to be watched out for too. They are usually used to split separate items up, such as numbers in a vector or arguments in a function (see section 3.9). Missing them out can cause issues! For instance, here we are trying to make the vector b have the numbers 3, 6, 9 &amp; 10. But the way we write it, R might think we mean 3, 6 &amp; 910 because we forgot the comma. Remember, R often ignores spaces. Fortunately, R catches it because it knows numbers need to be paid attention to - and suggests that youve missed out a comma. Well, technically it tells you Error: unexpected numeric constant in .... which doesnt help much - but its because we forgot the comma. b &lt;- c(3, 6, 9 10) ## Error: &lt;text&gt;:1:16: unexpected numeric constant ## 1: b &lt;- c(3, 6, 9 10 ## ^ RStudio can help you as you write code. During most of this course, you will be following code already written by us. However, in the try for yourself examples, youll have to fill in blanks. Whenever there is an issue with code and it wont run, RStudio flags this for you as soon as it realizes. It will give you a big red circle with an X inside of it. These appear in your script. You can see an example below for our problems with missing brackets and commas. Another thing that often happens with commas, is that people forget to put something in between them. Heres this issue: x &lt;- c(5, 8, ) ## Error in c(5, 8, ): argument 3 is empty You get warned that one of your arguments are empty. If you scroll away from your code, youll get the X in the red circle telling you something is wrong. If your cursor is still by your code, you may get a yellow warning triangle telling you to beware, similar to this: Check your quote marks Quote marks \"hello\" or 'hello' are often used in R when we want to make sure something is a character (i.e. text). Most commonly used when were inputting data that include words, or when were stating a color, or when were adding a title or a label to a graph. An error that we often see is that quote marks are either missing or not closed. Up to this point in the book, we havent yet covered the situations in which this is most likely to occur - so dont worry about following this code just yet. Were actually trying to make a scatterplot, that we discuss more in section 5.3. Instead, just focus on the error message: library(tidyverse) ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point(color = purple) ## Error in layer(data = data, mapping = mapping, stat = stat, geom = GeomPoint, : object &#39;purple&#39; not found Note that it says object 'purple' not found. Thats because, even though we are trying to make the points of the scatterplot purple, because we didnt put it in quotes, its looking for the object called purple. It thinks there is something called purple that contains the information it needs. The actual code should include purple in quotes, to make sure it processes the word purple literally: library(tidyverse) ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point(color = &quot;purple&quot;) For the most part in R, you can choose whether you use double quotes \" or single quotes ' when using quotes. Just make sure you match them, and close them. For example, the following examples are correct: days &lt;- c(&quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;) days ## [1] &quot;Monday&quot; &quot;Tuesday&quot; &quot;Wednesday&quot; days &lt;- c(&#39;Monday&#39;, &#39;Tuesday&#39;, &#39;Wednesday&#39;) days ## [1] &quot;Monday&quot; &quot;Tuesday&quot; &quot;Wednesday&quot; You can also do this, although it looks a bit odd: days &lt;- c(&#39;Monday&#39;, &quot;Tuesday&quot;, &#39;Wednesday&#39;) days ## [1] &quot;Monday&quot; &quot;Tuesday&quot; &quot;Wednesday&quot; Here are some examples of what happens if you dont close your quotes: days &lt;- c(&quot;Monday, &quot;Tuesday&quot;, &quot;Wednesday&quot;) ## Error: &lt;text&gt;:1:21: unexpected symbol ## 1: days &lt;- c(&quot;Monday, &quot;Tuesday ## ^ Often youll get the error message Error: unexpected symbol... in such situations. This is usually a clue to look for a punctuation error. Another think youll notice when you make this error, is that the colors of your code dont look right. Notice that the colors of each day arent consistent: In general, wed recommend that when you need to use quotes, that the double quotes \"hello\" are the better option. There are two reasons for this. First, if you start to type those in R then the second quote mark magically appears. RStudio knows youre using quotes and already produces the closing quote. This doesnt happen (for some reason) when you use the single quote 'hello'. Secondly, often we use quote marks because were adding a title or a label to something. Some example code to add a title to a graph might be: p + ggtitle(&quot;Tyler&#39;s first graph&quot;) Notice in this title that one of the words requires an apostrophe '. If you had used single quotes to wrap around Tyler's first graph then this would have caused confusion as to where you wanted to end the text string. This only arises when using single quotes - so we say that double quotes are safer! Typos are bad! R can only run the code that you tell it to run.. (well this is not 100% true but wait until the end of the paragraph for that). If you dont precisely tell it what you want, it will get confused or angry. If you typo, then this can cause problems. Lets do an example. Say we want to make two vectors called apples and oranges and we want to add them together. This works: apples &lt;- c(4, 7, 10) oranges &lt;- c(3, 1, 2) apples + oranges ## [1] 7 8 12 but this will not work: apples &lt;- c(4, 7, 10) oranges &lt;- c(3, 1, 2) appless + oranges ## Error in eval(expr, envir, enclos): object &#39;appless&#39; not found Youll get the error saying object 'blah' not found. This is the most common error that we see. Basically, it cannot find an object (be it a vector or a dataframe) if its not spelled correctly. This also applies to column names of dataframes and so on. R is also case sensitive. The following doesnt work: apples &lt;- c(4, 7, 10) oranges &lt;- c(3, 1, 2) apples + ORANGES ## Error in eval(expr, envir, enclos): object &#39;ORANGES&#39; not found It will not automatically realize you meant oranges. However, there is a bit of help that RStudio will give you. Look what happens when we write ORANGES in our code script: It automatically popped up oranges as a suggestion. The little colored icon to the left of it indicates that it is in an object. So - RStudio can be your friend! Actually, when you first start to type the first letter of any object, you can hit the Tab key and various options will pop up for you to choose from. You can then either use your up and down arrows or your mouse to click on the thing you want. In the example below, you can see it includes our oranges object and ton of other stuff that we dont need right now that also beings with o. This is super helpful when youve forgotten what you called something: We said that its not 100% true that R cannot guess what you mean. There are a few exceptions. One example, is that sometimes R can guess the name of a column in a dataset if you only type half of it or the first few letters. However, many programmers think this is a terrible thing to be able to do and so perhaps we should just forget about it. We would highly recommend always writing everything precisely. 3.7.1 Tab is your friend As seen from the example above, the Tab key can be used to get help as youre writing code. This is a super useful little trick as it can be hard to remember the names of some of the commands (functions) in R. Earlier in this section we saw that it was possible to calculate the sum of a vector with a function called sum(). However, what if you forgot the name of this but you just had a vague recollection that it began with an s ? You could start to type s and then hit the Tab key. Scroll through the list that pops up by your code until you find the one that you think is correct! Youll also notice that you get a big yellow help box that appears to the side. This gives you a bit more information about what the thing youve highlighted does - and this may help jog your memory as to what you were looking for. After you have clicked on the thing you want- it will automatically include it in your script- and youre good to go: 3.8 Error Messages Well be honest here and say that often times when you get an error message, they arent massively helpful. Or, at least this was the case until recently. There still are many error messages that dont really help you too much in learning what you did wrong, but if you make errors when using the tidyverse (see section 4.5) such as when making visualization - these error messages are much more helpful now than they once were. Making errors when coding is part of the learning curve though. Its like learning to play the piano. There is almost no way you can do it just by watching somebody else. You have to practice and be ok with making small mistakes. At first they catch you out, but over time youll realize when youre about to mess up and youll avoid it. Or, youll mess up and be faster at working out how to get past it. If people are stuck on particular error messages during this course - please do let us know. Well try and collate an FAQ of error messages that we can provide answers to - that will hopefully make the learning process a lot less painful. 3.9 Functions Functions are operations that you can conduct in R. There are many built-in functions to R, and several that come in packages that we can install and load in. There is usually (although not always) an object that is the focus of the function. Functions have names and end in brackets. Weve already met some of these, e.g.: sum() length() c() sqrt() As we go through the course, well see lots more functions and it will become second nature to use them. There is one additional thing that well bring up at this point. The things that you put inside functions are called arguments. Sometimes youll see them called parameters - but that has other meanings in statistics, so lets use arguments. Some functions have one argument, others have more. Most functions have defaults for these arguments, so if you dont include them, it will still run using its predefined default. Lets illustrate this by introducing you to the function called round(). This function will round numbers to various decimal places. Lets use the number 17.10771947 as an example. What does round do to this number? round(17.10771947) ## [1] 17 Its default is to round it to 0 decimal places. Inside of round() the number is in the position of the 1st argument. round() can have two arguments - the second one relates to how many decimal places you would like to round to. So, if you want to round to 3 decimal places, you need to do this: round(17.10771047, 3) ## [1] 17.108 Now we have two arguments. In actual fact, those arguments have names, and you can most often work out what the names for the arguments should be from help guides. Technically, what we have just performed is the following: round(x = 17.10771047, digits = 3) ## [1] 17.108 But as you have already worked out, we didnt need to write the names of the arguments. R worked it out already. What if wed got the arguments, the wrong way round? round(3, 17.10771047) ## [1] 3 Now it defaulted to giving us the first argument to zero decimal places. Because the second argument isnt a whole number (integer) then it just ignored it. What if you keep these in this order, but give them their correct argument names? round(digits = 3, x = 17.10771047) ## [1] 17.108 Now it works as expected again. The moral of this story is that it is best practice to use the argument names! If you hit the Tab key as you type out a function name, then it will give you a list of possible arguments for that function: Arguments of functions arent always numeric. Sometimes you need to write in words, punctuation or the logical statements TRUE or FALSE to tell the function what you want to do. An example is the function paste which joins together two objects. Here we tell it to paste together three objects with an underscore: obj1 &lt;- &quot;hello&quot; obj2 &lt;- &quot;james&quot; obj3 &lt;- &quot;curley&quot; paste(obj1, obj2, obj3, sep = &quot;_&quot;) ## [1] &quot;hello_james_curley&quot; In functions, punctuation and characters need quote marks around them. TRUE and FALSE should not have punctuation around them. If you want to continue with R then its also possible to write custom functions that will do what we do whatever we ask them to do - but that is for the future. R works from in to out One thing to remember about R as a coding language is that it will apply functions from in to out. It will start with the innermost function in a statement, and then move outwards. So, if you have the following vector: x &lt;- c(1, 5, 3, 7, 10, 11, 13, 5, 6, 2, 5, 6, 3, 7) x ## [1] 1 5 3 7 10 11 13 5 6 2 5 6 3 7 and you want to get the unique numbers (i.e. get rid of the repeating numbers and only keep one of each of all the numbers), then you could use unique(): unique(x) ## [1] 1 5 3 7 10 11 13 6 2 If you wanted to take the unique numbers, and then square root each of them you could wrap unique(x) in sqrt() like this: sqrt(unique(x)) ## [1] 1.000000 2.236068 1.732051 2.645751 3.162278 3.316625 3.605551 2.449490 1.414214 That code is still just about readable. However, say you want to (for some reason), keep the unique numbers, then take the square root of each number, then add them all up, then round the answer to 2 decimal places. This is what it would like: round(sum(sqrt(unique(x))), digits = 2) ## [1] 21.56 It finds the object you are working with x then the first function outside of that is unique() then the next one outside of unique(x) is sqrt(), then the next one outside of sqrt(unique(x)) is sum() and the next one outside of sum(sqrt(unique(x))) is round(). Thats quite hard to follow! Another approach would be to do this in steps and assign objects at each step, like this: x ## [1] 1 5 3 7 10 11 13 5 6 2 5 6 3 7 x1 &lt;- unique(x) x1 ## [1] 1 5 3 7 10 11 13 6 2 x2 &lt;- sqrt(x1) x2 ## [1] 1.000000 2.236068 1.732051 2.645751 3.162278 3.316625 3.605551 2.449490 1.414214 x3 &lt;- sum(x2) x3 ## [1] 21.56203 x4 &lt;- round(x3, digits = 2) x4 ## [1] 21.56 Another alternative is to chain the syntax together (see next section). 3.10 Chaining Syntax The code in the previous section was pretty ugly! Here it is: round(sum(sqrt(unique(x))), digits = 2) ## [1] 21.56 We could rewrite it however by chaining together the outputs of each function using the pipe or chaining piece of syntax that looks like this %&gt;%. This is how it would look if we chained each function together: x %&gt;% unique %&gt;% sqrt %&gt;% sum %&gt;% round(2) ## [1] 21.56 Lets break down whats happening bit by bit. Essentially you can read the %&gt;% as and next do this. This first bit just keeps the numbers that are unique in the vector, removing duplicates: x %&gt;% unique ## [1] 1 5 3 7 10 11 13 6 2 Next we square root each of those unique numbers: x %&gt;% unique %&gt;% sqrt ## [1] 1.000000 2.236068 1.732051 2.645751 3.162278 3.316625 3.605551 2.449490 1.414214 Then the next step is to add up those six square rooted numbers: x %&gt;% unique %&gt;% sqrt %&gt;% sum ## [1] 21.56203 Then we round that value to two decimal places: x %&gt;% unique %&gt;% sqrt %&gt;% sum %&gt;% round(2) ## [1] 21.56 Its also worth pointing out that you dont have to start a new line after every %&gt;%, you can have these in the middle of rows. Just dont start rows with them (see errors above)! This also is fine: x %&gt;% unique %&gt;% sqrt %&gt;% sum %&gt;% round(2) ## [1] 21.56 "],["introduction-to-data-carpentry.html", "Chapter 4 Introduction to Data Carpentry 4.1 Data Types 4.2 Importing Data 4.3 Introduction to Dataframes 4.4 Manually creating a Dataframe 4.5 tidyverse 4.6 Wide versus Long Data 4.7 Joins", " Chapter 4 Introduction to Data Carpentry Data carpentry gives us the tools to work with large data sets. During this chapter you will learn basic concepts and skills to help you import, tidy and manage your data. 4.1 Data Types Before learning how to work with data in R, we should know about some of the different types of data there are. Here is a figure that lays out the hierarchical levels of data types. We use the term variable to refer here to data. 4.1.0.1 Qualitative vs. Quantitative Data As you can see at the highest level, there are two data types: quantitative and qualitative. The easiest way to distinguish these data types part is to think of qualitative data as data we that are types . In practical terms, you can think of quantitative data as measures such as values or counts that can be expressed as numbers and can be compared on a numeric scale - either whole numbers or numbers with decimal places. Qualitative data are measures or observations of qualities, types or characteristics. They are often cases not numbers but in text form (although they can sometimes be numbers). 4.1.1 Categorical Data Under qualitative data, we have two different data types: Nominal and Ordinal. Both of these data types are known as categorical data, which means they represent characteristics such as a persons gender, education level, language, ethnicity etc. Categorical data can also be represented in a dataset by numbers. For example, if we are studying smokers and non-smokers in a study, the variable smoking would be a categorical variable. We could represent that data in text format using smoker or non-smoker as our data entry, or we could represent smokers as a 0 and non-smokers as a 1. However, it is important to remember that these numbers dont have any mathematical meaning here - its just a convenient way of representing the data. 4.1.1.1 Nominal Data Nominal data, sometimes referred to as unordered data are values which represent discrete units and are used to label a variable that have no quantitative value. The key thing about nominal data is that they do not have any natural value or rank, therefore if you change the order of the data, the meaning of the data does not change. An example of nominal data would be the results to: What languages do you speak at home?. If in our study we got the answers: French, English, Spanish &amp; Hindi from different subjects, then the data type is unordered. There is no sense in that French, English, Spanish, or Hindi are higher in rank than any other - they are simply unordered categories. 4.1.1.2 Ordinal Data Like nominal data, ordinal data describes discrete units or categories. However, unlike nominal data, ordinal data does have a natural order or ranking to it. For example, in a study we may ask subjects, What is the highest level of education you achieved? They may be able to choose from the following options: 1- Elementary 2- High School 3- Undergraduate 4- Graduate Here, each level of the data is a category, but it is ordered. Graduate is higher than Undergraduate which is higher than High School which is higher than Elementary. Therefore this data variable (education) is ordinal data. 4.1.2 Numerical Data (Discrete vs. Continuous) Under quantitative data, there is numerical data consist of numbers with real mathematical meaning. There are two types of numerical data. Discrete data is data that is separated such that this data can only take on certain value. Most often, these are things that can only be counted in whole units. e.g. If you asked subjects in a study - how many unique people did you text this week?, then the answer has to be an integer (a whole number), e.g. 0,1,2,3,etc. You cannot text 2.5 people. Think about discrete data as count data since it can only be counted and not measured. Continuous data is the opposite of discrete data, it can only be measured not counted. Continuous quantitative data is most often measured using some instrument. For example, if we collected the body weight or height of each subject in a study, then we would get values that would be measured using a scale or a measuring tape and could include decimal places. e.g. you could be 170.5 lbs heavy and you can be 180.2 cm tall. Additionally, quantitative data can be described in terms of interval and ratio data. 4.1.2.1 Interval Data Interval data represents values that are ordered on a scale and the differences between any two values has meaning. In interval data there is no true zero, which means if you have zero in your data, then zero is just another number. For example, zero on the Fahrenheit temperature scale means it is zero degrees outside, but doesnt mean the scale stops - 0 actually refers to a temperature. We can also have negative numbers in interval data which are meaningful. 4.1.2.2 Ratio data For a variable to be considered ratio data, it holds all the properties of an interval variable - it relates to numbers (including those with decimal places) that can be ordered on a scale. However, ratio data also has a true meaning of zero. Whenever a ratio variable is zero, it means there is none of that variable. For example, in a study, if a subject reports that they texted zero people in the last week, then here 0 really means the absence of something - it relates to none. Another example of ratio scale data is temperature in Kelvin, where zero degrees Kelvin means there is no heat being emitted. 4.2 Importing Data There are different options for importing data. Its possible to import data of all different formats into RStudio. We will primarily use spreadsheet type files that have been saved with the .csv suffix. These are called comma separated values. Option 1. Import Tab You can click on the Import Dataset tab in the top right of RStudio - its located just above your global environment. Depending on your RStudio version, you will be asked to select from a dropdown menu. When importing .csv files, you want to select From CSV... if your menu looks like this: If your menu looks like the underneath, then youll want to select From Text (readr): Option 2. Writing code. This is the option that we will use in our scripts for this course. You may notice that all the datasets that we wish to use are in a folder called data. To read in any of these datasets, what we need to do is use the read_csv() function. This comes from a package contained with the tidyverse package, so we must have imported that library first. We then tell it which dataset to find within the data folder. We use the notation data/ to tell it to look inside the data folder. For instance, if we wished to load in the bmi.csv dataset, and assign it the name bmi, we would do it like this - make sure to put quotes around the whole file name and location: library(tidyverse) #load package bmi &lt;- read_csv(&quot;data/bmi.csv&quot;) # bring in data head(bmi) # first six rows ## # A tibble: 6 x 9 ## id age bmi chol insulin dbp sbp smoke educ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 18.3 15.4 47.2 2.78 29.5 49.7 1 D ## 2 2 19.4 14.5 52.1 3.47 31.3 49.0 2 C ## 3 3 21.1 16.0 52.2 4.06 32.4 51.7 1 B ## 4 4 21.3 19.5 53.2 4.48 32.0 NA 2 A ## 5 5 21.1 18.6 55.4 5.34 33.7 53.8 1 D ## 6 6 23.9 19.5 54.3 6.29 35.0 56.0 2 C tail(bmi) # last six rows ## # A tibble: 6 x 9 ## id age bmi chol insulin dbp sbp smoke educ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 15 34.8 30.4 63.6 15.3 42.7 64.8 1 B ## 2 16 33.6 29.9 65.5 NA 46.4 65.6 2 A ## 3 17 33.4 NA 65.2 18.2 46.2 64.6 1 D ## 4 18 35.1 32.5 68.6 19.7 45.8 66.1 2 C ## 5 19 35.0 33.4 NA 21.1 NA 68.4 1 B ## 6 20 37.5 34.1 68.0 22.1 49.3 68.6 2 A If you want to see more on what the data looks like the following functions can help. nrow(bmi) # how many rows in dataset ## [1] 20 ncol(bmi) # how many columns in dataset ## [1] 9 colnames(bmi) # column names ## [1] &quot;id&quot; &quot;age&quot; &quot;bmi&quot; &quot;chol&quot; &quot;insulin&quot; &quot;dbp&quot; &quot;sbp&quot; &quot;smoke&quot; &quot;educ&quot; If you just want to view your entire dataset, there is a function for that. View(bmi) 4.3 Introduction to Dataframes A dataframe, in R is like a spreadsheet that contains your data. Each column contains values or characters for one variable. In R, columns are called variables. Rows are called observations. In R dataframes need to contain the following: Columns should all be the same length, with unique column names. These column names should not start with numbers or punctuation, and have no spaces. The data stored in dataframes can hold many different data types. The most common are numbers. R describes columns with numbers as being numeric, although a column containing only whole numbers (e.g. 1, 5, 342, 1034) may be called integers. Columns containing any value with a decimal place (e.g. 0.01, 4.4, -7.39494) will be called double. Both double and integer are types of numeric data. The second most common data type for a column is character. This is the case if the data in the column have any text or punctuation. A related column type is factor. This occurs when the character type is grouped, e.g. if you had a column with different days of the week, you may wish for R to recognize that each is a separate category. Another column type that comes up semi-regularly, although not much in this course, is logical. This is when the column contains either TRUE or FALSE values. Lets dive into all the features of a dataframe. First import data, using read_csv(). df &lt;- read_csv(&quot;data/cheese.csv&quot;) 4.3.1 Dataframe basics Look at top few rows using head(). The default for head() is to look at the top 6 rows. However, if you put a comma, and then a number, you can get that number of rows instead. Here we get the default 6, and then we get 4 and 8 rows. head(df) ## # A tibble: 6 x 9 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 blue 18.7 0.8 7.78 21.4 2.34 75 0 353 ## 2 brick 18.8 0.784 8.60 23.2 2.79 94 0 371 ## 3 brie 17.4 0.826 8.01 20.8 0.45 100 0 334 ## 4 camembert 15.3 0.724 7.02 19.8 0.46 72 0 300 ## 5 caraway 18.6 0.83 8.28 25.2 3.06 93 0 376 ## 6 cheddar 21.1 0.942 9.39 24.9 1.28 105 0 403 head(df, 4) ## # A tibble: 4 x 9 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 blue 18.7 0.8 7.78 21.4 2.34 75 0 353 ## 2 brick 18.8 0.784 8.60 23.2 2.79 94 0 371 ## 3 brie 17.4 0.826 8.01 20.8 0.45 100 0 334 ## 4 camembert 15.3 0.724 7.02 19.8 0.46 72 0 300 head(df, 8) ## # A tibble: 8 x 9 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 blue 18.7 0.8 7.78 21.4 2.34 75 0 353 ## 2 brick 18.8 0.784 8.60 23.2 2.79 94 0 371 ## 3 brie 17.4 0.826 8.01 20.8 0.45 100 0 334 ## 4 camembert 15.3 0.724 7.02 19.8 0.46 72 0 300 ## 5 caraway 18.6 0.83 8.28 25.2 3.06 93 0 376 ## 6 cheddar 21.1 0.942 9.39 24.9 1.28 105 0 403 ## 7 cheshire 19.5 0.87 8.67 23.4 4.78 103 0 387 ## 8 colby 20.2 0.953 9.28 23.8 2.57 95 0 394 You can do the same thing with tail() to look at the bottom rows. The default again is 6, but you can tell it how many rows you wish to look at: tail(df) ## # A tibble: 6 x 9 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 past process,swiss,lofat 3.30 0.18 1.35 25.5 4.3 35 0 170 ## 2 cottage,lowfat,1% milkfat,w/veg 0.619 0.039 0.282 10.9 3 3 0 67 ## 3 past process,cheddar or american,lo na 19.7 0.988 8.93 22.2 1.6 94 0 375 ## 4 swiss,low sodium 17.7 0.968 7.26 28.4 3.4 92 0 376 ## 5 swiss,low fat 3.30 0.18 1.35 28.4 3.4 35 0 179 ## 6 mozzarella,lo na 10.9 0.509 4.84 27.5 3.1 54 0 280 tail(df, 10) ## # A tibble: 10 x 9 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 past process,american,lofat 4.41 0.222 2.00 24.6 3.5 35 0 180 ## 2 american cheddar,imitn 8.79 0.409 4.10 16.7 11.6 36 0 239 ## 3 parmesan,lo na 19.1 0.659 8.72 41.6 3.7 79 0 456 ## 4 cottage,lowfat,1% milkfat,no na 0.632 0.031 0.284 12.4 2.7 4 0 72 ## 5 past process,swiss,lofat 3.30 0.18 1.35 25.5 4.3 35 0 170 ## 6 cottage,lowfat,1% milkfat,w/veg 0.619 0.039 0.282 10.9 3 3 0 67 ## 7 past process,cheddar or american,lo na 19.7 0.988 8.93 22.2 1.6 94 0 375 ## 8 swiss,low sodium 17.7 0.968 7.26 28.4 3.4 92 0 376 ## 9 swiss,low fat 3.30 0.18 1.35 28.4 3.4 35 0 179 ## 10 mozzarella,lo na 10.9 0.509 4.84 27.5 3.1 54 0 280 We can also get various dimensions of a dataframe. nrow() gets the number of rows (also termed observations), ncol() gets the number of columns (also called variables), dim() returns both the number of rows and the number of columns, and length() is another way to get the number of columns. nrow(df) ## [1] 73 ncol(df) ## [1] 9 dim(df) ## [1] 73 9 length(df) ## [1] 9 As we showed earlier, colnames() is a very useful function for retrieving the names of columns in a dataframe. This is particularly useful when you cant fit all of the columns onto your screen which is common with big datasets. colnames(df) ## [1] &quot;type&quot; &quot;sat_fat&quot; &quot;polysat_fat&quot; &quot;monosat_fat&quot; &quot;protein&quot; &quot;carb&quot; &quot;chol&quot; ## [8] &quot;fiber&quot; &quot;kcal&quot; Sometimes you may wish to change column names. This is common when youve imported some messy data from elsewhere. Hopefully if it is your own data you named your columns well in the first place. The way to change column names is to assign the new name to the appropriate column, by indicating which column with square brackets []: colnames(df)[6] &lt;- &quot;carbo&quot; colnames(df) ## [1] &quot;type&quot; &quot;sat_fat&quot; &quot;polysat_fat&quot; &quot;monosat_fat&quot; &quot;protein&quot; &quot;carbo&quot; &quot;chol&quot; ## [8] &quot;fiber&quot; &quot;kcal&quot; In the above example, we changed the name of the 6th column to carbo. You can change all column names by doing the following: colnames(df) &lt;- c(&quot;type1&quot;, &quot;sat_fat1&quot;, &quot;polysat_fat1&quot;, &quot;monosat_fat1&quot;, &quot;protein1&quot;, &quot;carb1&quot;, &quot;chol1&quot;, &quot;fiber1&quot;, &quot;kcal1&quot;) head(df) ## # A tibble: 6 x 9 ## type1 sat_fat1 polysat_fat1 monosat_fat1 protein1 carb1 chol1 fiber1 kcal1 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 blue 18.7 0.8 7.78 21.4 2.34 75 0 353 ## 2 brick 18.8 0.784 8.60 23.2 2.79 94 0 371 ## 3 brie 17.4 0.826 8.01 20.8 0.45 100 0 334 ## 4 camembert 15.3 0.724 7.02 19.8 0.46 72 0 300 ## 5 caraway 18.6 0.83 8.28 25.2 3.06 93 0 376 ## 6 cheddar 21.1 0.942 9.39 24.9 1.28 105 0 403 Lets return them back to the original values: colnames(df) &lt;- c(&quot;type&quot;, &quot;sat_fat&quot;, &quot;polysat_fat&quot;, &quot;monosat_fat&quot;, &quot;protein&quot;, &quot;carb&quot;, &quot;chol&quot;, &quot;fiber&quot;, &quot;kcal&quot;) head(df) ## # A tibble: 6 x 9 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 blue 18.7 0.8 7.78 21.4 2.34 75 0 353 ## 2 brick 18.8 0.784 8.60 23.2 2.79 94 0 371 ## 3 brie 17.4 0.826 8.01 20.8 0.45 100 0 334 ## 4 camembert 15.3 0.724 7.02 19.8 0.46 72 0 300 ## 5 caraway 18.6 0.83 8.28 25.2 3.06 93 0 376 ## 6 cheddar 21.1 0.942 9.39 24.9 1.28 105 0 403 4.3.2 Indexing dataframes. There are two indexing methods we need to learn. One is the $ sign, which indicates we want to get data from a specific column. The other is the square brackets []. The $ indicates which column to call. For instance, if we wish to get all the data from the chol column of the dataset df, then we would type df$chol and it returns all the data in that column: df$chol ## [1] 75 94 100 72 93 105 103 95 17 13 7 10 4 110 89 89 116 94 114 110 90 89 79 89 64 ## [26] 54 96 74 88 68 123 69 51 31 104 90 92 102 94 94 85 72 94 85 105 79 46 105 105 105 ## [51] 21 100 12 88 55 62 65 11 4 63 18 14 54 35 36 79 4 35 3 94 92 35 54 What if you just want the first 10 rows of kcal? Then we could get all the data from that column with df$kcal but then use the square brackets to tell it to get the values in positions 1 to 10 with [1:10]: df$kcal[1:10] ## [1] 353 371 334 300 376 403 387 394 98 97 This works, because you can think of all the data values in each column as essentially its own vector. The square brackets work just as they would with any other vector indexing (see section 3.3). Square brackets can be used on the whole dataframe to call just certain rows and columns. Importantly row numbers need to be written before the comma, and columns after comma. If you leave anything blank, then it will assume that you mean all of the rows or columns. Technically, running df, and df[,] all return the whole dataframe. What youre saying is return all of the rows and all of the columns from df. df ## # A tibble: 73 x 9 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 blue 18.7 0.8 7.78 21.4 2.34 75 0 353 ## 2 brick 18.8 0.784 8.60 23.2 2.79 94 0 371 ## 3 brie 17.4 0.826 8.01 20.8 0.45 100 0 334 ## 4 camembert 15.3 0.724 7.02 19.8 0.46 72 0 300 ## 5 caraway 18.6 0.83 8.28 25.2 3.06 93 0 376 ## 6 cheddar 21.1 0.942 9.39 24.9 1.28 105 0 403 ## 7 cheshire 19.5 0.87 8.67 23.4 4.78 103 0 387 ## 8 colby 20.2 0.953 9.28 23.8 2.57 95 0 394 ## 9 cottage,crmd,lrg or sml curd 1.72 0.123 0.778 11.1 3.38 17 0 98 ## 10 cottage,crmd,w/fruit 2.31 0.124 1.04 10.7 4.61 13 0.200 97 ## # ... with 63 more rows df[,] #return all rows and all columns ## # A tibble: 73 x 9 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 blue 18.7 0.8 7.78 21.4 2.34 75 0 353 ## 2 brick 18.8 0.784 8.60 23.2 2.79 94 0 371 ## 3 brie 17.4 0.826 8.01 20.8 0.45 100 0 334 ## 4 camembert 15.3 0.724 7.02 19.8 0.46 72 0 300 ## 5 caraway 18.6 0.83 8.28 25.2 3.06 93 0 376 ## 6 cheddar 21.1 0.942 9.39 24.9 1.28 105 0 403 ## 7 cheshire 19.5 0.87 8.67 23.4 4.78 103 0 387 ## 8 colby 20.2 0.953 9.28 23.8 2.57 95 0 394 ## 9 cottage,crmd,lrg or sml curd 1.72 0.123 0.778 11.1 3.38 17 0 98 ## 10 cottage,crmd,w/fruit 2.31 0.124 1.04 10.7 4.61 13 0.200 97 ## # ... with 63 more rows To just get the 7th row, you put a 7 before the comma, and leave after the comma blank: df[7,] ## # A tibble: 1 x 9 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 cheshire 19.5 0.87 8.67 23.4 4.78 103 0 387 To get the 10th to 14th rows you can put 10:14 before the comma and leave after the comma blank: df[10:14,] ## # A tibble: 5 x 9 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 cottage,crmd,w/fruit 2.31 0.124 1.04 10.7 4.61 13 0.200 97 ## 2 cottage,nonfat,uncrmd,dry,lrg or sml curd 0.169 0.003 0.079 10.3 6.66 7 0 72 ## 3 cottage,lowfat,2% milkfat 0.979 0.07 0.443 11.8 3.66 10 0 86 ## 4 cottage,lowfat,1% milkfat 0.645 0.031 0.291 12.4 2.72 4 0 72 ## 5 cream 19.3 1.44 8.62 5.93 4.07 110 0 342 To get the 3rd column you can leave before the comma blank, and put a 3 after the comma: df[,3] ## # A tibble: 73 x 1 ## polysat_fat ## &lt;dbl&gt; ## 1 0.8 ## 2 0.784 ## 3 0.826 ## 4 0.724 ## 5 0.83 ## 6 0.942 ## 7 0.87 ## 8 0.953 ## 9 0.123 ## 10 0.124 ## # ... with 63 more rows To get the first and second columns, leave before the comma blank and put a 1:2 after the comma: df[,1:2] ## # A tibble: 73 x 2 ## type sat_fat ## &lt;chr&gt; &lt;dbl&gt; ## 1 blue 18.7 ## 2 brick 18.8 ## 3 brie 17.4 ## 4 camembert 15.3 ## 5 caraway 18.6 ## 6 cheddar 21.1 ## 7 cheshire 19.5 ## 8 colby 20.2 ## 9 cottage,crmd,lrg or sml curd 1.72 ## 10 cottage,crmd,w/fruit 2.31 ## # ... with 63 more rows If you want to get non-consecutive columns, you need to use c() with your numbers. For example, to get the 3rd, 5th, and 9th columns you leave before the comma blank and put c(3,5,9) after the comma: df[,c(3,5,9)] ## # A tibble: 73 x 3 ## polysat_fat protein kcal ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.8 21.4 353 ## 2 0.784 23.2 371 ## 3 0.826 20.8 334 ## 4 0.724 19.8 300 ## 5 0.83 25.2 376 ## 6 0.942 24.9 403 ## 7 0.87 23.4 387 ## 8 0.953 23.8 394 ## 9 0.123 11.1 98 ## 10 0.124 10.7 97 ## # ... with 63 more rows You can also combine these. So, to get the 20th to 22nd row, and the 1st, 5th and 9th column, you put 20:22 before the comma, and c(1,5,9) after the comma: df[20:22,c(1,5,9)] ## # A tibble: 3 x 3 ## type protein kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 gruyere 29.8 413 ## 2 limburger 20.0 327 ## 3 monterey 24.5 373 4.3.3 Adding and removing columns As we will see below in the tidyverse section, there is a way to creating new columns using the function mutate() which we recommend. However, there is also a quick way to create and delete new columns which is worth knowing about. To create a new column, you can simply type a new name after writing your dataframe name and the dollar sign, and then assign something to it. So, if you wished to create a column called food_type youd write df$food_type &lt;- and then put whatever you wanted to put into that column. For example, if you wanted it to contain the word cheese youd do the following: df$food_type &lt;- &quot;cheese&quot; head(df) ## # A tibble: 6 x 10 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal food_type ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 blue 18.7 0.8 7.78 21.4 2.34 75 0 353 cheese ## 2 brick 18.8 0.784 8.60 23.2 2.79 94 0 371 cheese ## 3 brie 17.4 0.826 8.01 20.8 0.45 100 0 334 cheese ## 4 camembert 15.3 0.724 7.02 19.8 0.46 72 0 300 cheese ## 5 caraway 18.6 0.83 8.28 25.2 3.06 93 0 376 cheese ## 6 cheddar 21.1 0.942 9.39 24.9 1.28 105 0 403 cheese Now every observation (row) has the entry cheese in the column food_type. If you wanted to put in different data for each observation, e.g. the country of origin of each cheese, then youd need a vector that was the same length as the number of rows of the dataset. See the section on manually creating dataframes just below for some examples of this. To delete a column from a dataframe, you just assign the word NULL, which is a special term in R, to that column, and then it will disappear: df$food_type &lt;- NULL head(df) ## # A tibble: 6 x 9 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 blue 18.7 0.8 7.78 21.4 2.34 75 0 353 ## 2 brick 18.8 0.784 8.60 23.2 2.79 94 0 371 ## 3 brie 17.4 0.826 8.01 20.8 0.45 100 0 334 ## 4 camembert 15.3 0.724 7.02 19.8 0.46 72 0 300 ## 5 caraway 18.6 0.83 8.28 25.2 3.06 93 0 376 ## 6 cheddar 21.1 0.942 9.39 24.9 1.28 105 0 403 4.3.4 Structure of Datasets This little subsection is a bit more about the inner workings of dataframes. It wont be coding that we use during this course, but if you ever do any extra R things by yourself, you may run into issues that this section could help you resolve. You can see the structure of your data, whether each variable is a number, character, factor, logical, etc. This can be useful when trying to graph and analysis different types of data. str(bmi) ## spec_tbl_df [20 x 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ id : num [1:20] 1 2 3 4 5 6 7 8 9 10 ... ## $ age : num [1:20] 18.3 19.4 21.1 21.3 21.1 ... ## $ bmi : num [1:20] 15.4 14.5 16 19.5 18.6 ... ## $ chol : num [1:20] 47.2 52.1 52.2 53.2 55.4 ... ## $ insulin: num [1:20] 2.78 3.47 4.06 4.48 5.34 ... ## $ dbp : num [1:20] 29.5 31.3 32.4 32 33.7 ... ## $ sbp : num [1:20] 49.7 49 51.7 NA 53.8 ... ## $ smoke : num [1:20] 1 2 1 2 1 2 1 2 1 2 ... ## $ educ : chr [1:20] &quot;D&quot; &quot;C&quot; &quot;B&quot; &quot;A&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. id = col_double(), ## .. age = col_double(), ## .. bmi = col_double(), ## .. chol = col_double(), ## .. insulin = col_double(), ## .. dbp = col_double(), ## .. sbp = col_double(), ## .. smoke = col_double(), ## .. educ = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; Here you can see that all variables are numeric, except for education with is a character. Another common variable type is factor. This is similar to a character but it has levels. This means, that R knows that there are groups in that variable. You might notice that the variable smoke is currently a numerical variable with values being either a 1 (non smoker) or 2 (smoker). If we wanted to make a graph and plot non-smoker or smoker on the x-axis, then this would appear as 1 or 2, which isnt helpful. In these situations, it can be helpful to convert our numeric variable into a factor. The way to do that is as follows: bmi$smoke &lt;- as.factor(bmi$smoke) The $ basically allows you to call certain columns in a dataframe. The code bmi$smoke is allowing us to only change that column, or variable to a factor without changing anything else in the dataframe. You can also change variables to characters with as.character() and to numbers with as.numeric(). 4.4 Manually creating a Dataframe Often in R, we dont want to import a dataset, but rather create a dataset of our own manually in the script. We can do that using the function data.frame(). Lets just do a small example. Say, we want to create a dataframe with four columns. We will have the names of 6 different pets in column 1 and call that column name. In the second column, we want the ages of those pets and well call that column age. In the third column we will have the type of animal that pet is, and well call that column animal. In the fourth column, well have whether its a male or female pet and well call that column sex. In the fifth column, well have the main color of the pet, and well call that column color. Here are our 6 pets: Steve, an orange male goldfish who is 5. Hannah, a female blue parrot who is 12. Colin, a brown male cat who is 15. Archibald, a grouchy green male terrapin who is 3. Missy, a female yellow labrador dog who is 2. Bob, a black male spider who is 10. We need to make sure we put each of the column information in the right order in our dataframe. This is how we manually enter the data: petdf &lt;- data.frame( name = c(&quot;Steve&quot;, &quot;Hannah&quot;, &quot;Colin&quot;, &quot;Archibald&quot;, &quot;Missy&quot;, &quot;Bob&quot;), age = c(5, 12, 15, 3, 2, 10), animal = c(&quot;goldfish&quot;, &quot;parrot&quot;, &quot;cat&quot;, &quot;terrapin&quot;, &quot;dog&quot;, &quot;spider&quot;), sex = c(&quot;M&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;M&quot;), color = c(&quot;orange&quot;, &quot;blue&quot;, &quot;brown&quot;, &quot;green&quot;, &quot;yellow&quot;, &quot;black&quot;) ) petdf ## name age animal sex color ## 1 Steve 5 goldfish M orange ## 2 Hannah 12 parrot F blue ## 3 Colin 15 cat M brown ## 4 Archibald 3 terrapin M green ## 5 Missy 2 dog F yellow ## 6 Bob 10 spider M black Inside dataframe() we write each column name. Then we put an = sign, and then write the vector of characters or numbers. After each columns data is entered, we write a comma , to indicate were going to the next column. The exception to this is after the last column is entered, color in this case, where we dont write a comma after were done entering all the color information. This indicates that were done. Also note that every column has the same number of pieces of information (6). If we had unequal bits of information, R would generally not allow us to make the dataframe. There is one gotcha here though - if you enter less than the number of rows of your new dataframe, e.g. if youd only entered two values in the color column say, R will repeat those two colors through the remaining empty rows. You need to be careful! If youre not sure what youre doing, the best is to ensure that you have the exact number of values as you want rows inside each vector. 4.5 tidyverse The package tidyverse is what we will be using for most of our data carpentry. tidyverse is a larger package which includes several packages useful for managing, exploring and visualizing data such as tidyr, dplyr, ggplot2, and more. In this section, we will learn the following functions and syntax: filter() - subsetting data select() - selecting columns arrange() - sorting a column mutate() - adding a new column count() - counts the number of values in a column %&gt;% means and next do this == means is equal to != means is not equal to | means or First, make sure that you have the tidyverse installed. If you run the following code and do not get an error message saying tidyverse not found, then you are good: library(tidyverse) If you did get the error message, then youll need to install the library. You only need to do this once, but you will have to load a library on every script you plan to use it. To install the package: install.package(tidyverse) #install To load the package to use each time you need it: library(tidyverse) From the tidyverse package readr we can read in our data. We are going to work with a dataset called bloodwork that we will shorten to bw. It contains health information on several subjects: library(tidyverse) #load bw &lt;- read_csv(&quot;data/bloodwork.csv&quot;) head(bw) ## # A tibble: 6 x 11 ## ids age sex state children smoker hrate bpsyst cellcount immuncount immuncount2 ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 19 female NJ 1 no 63 101 0.126 0.993 0.921 ## 2 GBH 20 male NY 1 yes 73 120 0.169 1.18 1.19 ## 3 EDH 21 female NJ 0 no 65 100 0.281 4.34 3.21 ## 4 AAA 21 female CT 3 no 66 109 0.244 2.56 4.01 ## 5 AJF 24 female NJ 0 no 67 108 0.092 6.45 9.13 ## 6 FJC 25 female NY 1 yes 80 118 0.014 3.97 2.85 tail(bw) ## # A tibble: 6 x 11 ## ids age sex state children smoker hrate bpsyst cellcount immuncount immuncount2 ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JIB 66 female NY 0 no 62 121 0.097 1.39 2.13 ## 2 HBB 67 female NJ 1 yes 74 147 0.288 2.27 1.73 ## 3 HDG 68 female NY 3 yes 65 129 0.11 2.65 2.59 ## 4 ECD 68 female NJ 2 yes 77 129 0.404 2.02 4.22 ## 5 HHJ 69 male CT 2 no 71 121 0.475 0.463 0.449 ## 6 CCG 70 male CT 0 yes 80 132 0.078 1.06 1.01 As you can see, we have variables such as ids, age, the state people live in, their heart rate, how many children they have etc. etc. 4.5.1 table() This function is not a tidyverse function but is a quick summary function that is useful to know - table(). It is good for quickly summarizing categorical or discrete numerical variables. For example, to look at the number of smokers and non-smokers, we can do the following: table(bw$smoker) ## ## no yes ## 15 15 We have 15 non-smokers and 15 smokers in the dataset. To look at the frequency count of how many subjects have 0, 1, 2, 3 children we can do: table(bw$children) ## ## 0 1 2 3 ## 13 10 5 2 We have thirteen people with 0 children ten with 1 child, five with 2 children and two with 3 children. You can also compare two categories at once. For instance, to look at the smokers and non-smokers that have different numbers of children, we can include both in the table() function and separate by a comma: table(bw$smoker, bw$children) ## ## 0 1 2 3 ## no 7 4 3 1 ## yes 6 6 2 1 The tidyverse version of table() is to use a function called count(). It does come in useful sometimes, but most of the time youll find using table() to be easier. However, here is an example of counting how many individual by each state there are in the data. You first take your dataset, then chain with the pipe %&gt;% the next command which is to count() the column state. bw %&gt;% count(state) ## # A tibble: 3 x 2 ## state n ## &lt;chr&gt; &lt;int&gt; ## 1 CT 12 ## 2 NJ 10 ## 3 NY 8 We can also get frequency counts for more than one column e.g. to see how many children non-smokers and smokers have, though again, the output from table() is nicer to look at: bw %&gt;% count(smoker, children) ## # A tibble: 8 x 3 ## smoker children n ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 no 0 7 ## 2 no 1 4 ## 3 no 2 3 ## 4 no 3 1 ## 5 yes 0 6 ## 6 yes 1 6 ## 7 yes 2 2 ## 8 yes 3 1 4.5.2 filter() - Subsetting Data filter() is a way to subset data. In other words, it is a way to only keep certain rows in your dataset. These are rows that must fulfill specific criteria. For example, lets say we wish to only keep rows that have values in the hrate column that are over 70. The first step is to take the dataframe bw() and then tell R that you are about to do something else with the %&gt;% syntax. Then use filter() with hrate &gt; 70 inside it. This mean, keep the rows with hrate&gt;70. See sections (chaining-syntax) and 4.5.6 for a bit more on chaining using %&gt;%. bw %&gt;% filter(hrate &gt; 70) ## # A tibble: 16 x 11 ## ids age sex state children smoker hrate bpsyst cellcount immuncount immuncount2 ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GBH 20 male NY 1 yes 73 120 0.169 1.18 1.19 ## 2 FJC 25 female NY 1 yes 80 118 0.014 3.97 2.85 ## 3 IEE 26 female NY 2 no 71 118 0.093 5.41 10.2 ## 4 IEA 29 female CT 0 yes 74 117 0.429 5.15 4.97 ## 5 FJG 40 female NJ 0 yes 80 109 0.253 5.63 4.11 ## 6 AGC 43 male NY 1 yes 77 108 0.072 1.44 0.775 ## 7 CAE 47 male CT 1 yes 73 122 0.176 0.127 0.107 ## 8 FGD 50 male NY 0 yes 77 147 0.428 0.037 0.066 ## 9 CGC 51 male CT 0 yes 71 119 0.045 1.73 5.40 ## 10 FHA 53 female CT 2 no 77 125 0.099 0.034 0.057 ## 11 JHC 55 female CT 0 no 73 121 0.093 1.32 1.17 ## 12 CFC 65 male CT 2 yes 77 129 0.151 2.40 1.89 ## 13 HBB 67 female NJ 1 yes 74 147 0.288 2.27 1.73 ## 14 ECD 68 female NJ 2 yes 77 129 0.404 2.02 4.22 ## 15 HHJ 69 male CT 2 no 71 121 0.475 0.463 0.449 ## 16 CCG 70 male CT 0 yes 80 132 0.078 1.06 1.01 To only keep values that are precisely equal to some value (be it a number or a character), we use filter() with == to mean is equal to: For example, to only keep rows where the state is equal to NJ. bw %&gt;% filter(state == &quot;NJ&quot;) ## # A tibble: 10 x 11 ## ids age sex state children smoker hrate bpsyst cellcount immuncount immuncount2 ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 19 female NJ 1 no 63 101 0.126 0.993 0.921 ## 2 EDH 21 female NJ 0 no 65 100 0.281 4.34 3.21 ## 3 AJF 24 female NJ 0 no 67 108 0.092 6.45 9.13 ## 4 BFB 28 female NJ 0 no 68 118 0.197 0.724 0.848 ## 5 ACC 33 male NJ 1 no 63 131 0.065 4.66 8.50 ## 6 CDC 38 female NJ 0 no 66 133 0.038 8.00 15.2 ## 7 EEB 39 male NJ 1 no 68 104 0.594 3.06 2.82 ## 8 FJG 40 female NJ 0 yes 80 109 0.253 5.63 4.11 ## 9 HBB 67 female NJ 1 yes 74 147 0.288 2.27 1.73 ## 10 ECD 68 female NJ 2 yes 77 129 0.404 2.02 4.22 If you want to include all subjects that had 3 children, youd do the following - note that the number does not go in quote marks: bw %&gt;% filter(children == 3) ## # A tibble: 2 x 11 ## ids age sex state children smoker hrate bpsyst cellcount immuncount immuncount2 ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AAA 21 female CT 3 no 66 109 0.244 2.56 4.01 ## 2 HDG 68 female NY 3 yes 65 129 0.11 2.65 2.59 If you want to include rows that are equal to one of two values, you can use | to mean OR. The following will include rows where the state column is equal to either NJ or NY in the bloodwork dataset. bw %&gt;% filter(state == &quot;NJ&quot; | state == &quot;NY&quot;) ## # A tibble: 18 x 11 ## ids age sex state children smoker hrate bpsyst cellcount immuncount immuncount2 ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 19 female NJ 1 no 63 101 0.126 0.993 0.921 ## 2 GBH 20 male NY 1 yes 73 120 0.169 1.18 1.19 ## 3 EDH 21 female NJ 0 no 65 100 0.281 4.34 3.21 ## 4 AJF 24 female NJ 0 no 67 108 0.092 6.45 9.13 ## 5 FJC 25 female NY 1 yes 80 118 0.014 3.97 2.85 ## 6 IEE 26 female NY 2 no 71 118 0.093 5.41 10.2 ## 7 BFB 28 female NJ 0 no 68 118 0.197 0.724 0.848 ## 8 ACC 33 male NJ 1 no 63 131 0.065 4.66 8.50 ## 9 CDC 38 female NJ 0 no 66 133 0.038 8.00 15.2 ## 10 EEB 39 male NJ 1 no 68 104 0.594 3.06 2.82 ## 11 FJG 40 female NJ 0 yes 80 109 0.253 5.63 4.11 ## 12 AGC 43 male NY 1 yes 77 108 0.072 1.44 0.775 ## 13 FGD 50 male NY 0 yes 77 147 0.428 0.037 0.066 ## 14 JCI 52 male NY 1 yes 61 115 0.131 0.233 0.17 ## 15 JIB 66 female NY 0 no 62 121 0.097 1.39 2.13 ## 16 HBB 67 female NJ 1 yes 74 147 0.288 2.27 1.73 ## 17 HDG 68 female NY 3 yes 65 129 0.11 2.65 2.59 ## 18 ECD 68 female NJ 2 yes 77 129 0.404 2.02 4.22 You can also keep rows that are not equal to some value using the syntax != which means not equal to. For example, to keep rows where the children column is not equal to 0: bw %&gt;% filter(children != 0) ## # A tibble: 17 x 11 ## ids age sex state children smoker hrate bpsyst cellcount immuncount immuncount2 ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 19 female NJ 1 no 63 101 0.126 0.993 0.921 ## 2 GBH 20 male NY 1 yes 73 120 0.169 1.18 1.19 ## 3 AAA 21 female CT 3 no 66 109 0.244 2.56 4.01 ## 4 FJC 25 female NY 1 yes 80 118 0.014 3.97 2.85 ## 5 IEE 26 female NY 2 no 71 118 0.093 5.41 10.2 ## 6 BHE 30 male CT 1 no 63 120 0.215 1.19 1.6 ## 7 ACC 33 male NJ 1 no 63 131 0.065 4.66 8.50 ## 8 EEB 39 male NJ 1 no 68 104 0.594 3.06 2.82 ## 9 AGC 43 male NY 1 yes 77 108 0.072 1.44 0.775 ## 10 CAE 47 male CT 1 yes 73 122 0.176 0.127 0.107 ## 11 JCI 52 male NY 1 yes 61 115 0.131 0.233 0.17 ## 12 FHA 53 female CT 2 no 77 125 0.099 0.034 0.057 ## 13 CFC 65 male CT 2 yes 77 129 0.151 2.40 1.89 ## 14 HBB 67 female NJ 1 yes 74 147 0.288 2.27 1.73 ## 15 HDG 68 female NY 3 yes 65 129 0.11 2.65 2.59 ## 16 ECD 68 female NJ 2 yes 77 129 0.404 2.02 4.22 ## 17 HHJ 69 male CT 2 no 71 121 0.475 0.463 0.449 You can also filter several variables at one time. Here, we are keeping all individuals from New York, with a heart rate of over 70, and who have more than 0 children: bw %&gt;% filter( state == &quot;NY&quot;, hrate &gt; 70, children != &quot;0&quot; ) ## # A tibble: 4 x 11 ## ids age sex state children smoker hrate bpsyst cellcount immuncount immuncount2 ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GBH 20 male NY 1 yes 73 120 0.169 1.18 1.19 ## 2 FJC 25 female NY 1 yes 80 118 0.014 3.97 2.85 ## 3 IEE 26 female NY 2 no 71 118 0.093 5.41 10.2 ## 4 AGC 43 male NY 1 yes 77 108 0.072 1.44 0.775 You can create new datasets from filtered data by creating a new object. Here, we create a new dataset called ny1 that is based on filtering out rows from our bw dataset. ny1 &lt;- bw %&gt;% filter( state == &quot;NY&quot;, hrate &gt; 70, children != &quot;0&quot; ) ny1 ## # A tibble: 4 x 11 ## ids age sex state children smoker hrate bpsyst cellcount immuncount immuncount2 ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GBH 20 male NY 1 yes 73 120 0.169 1.18 1.19 ## 2 FJC 25 female NY 1 yes 80 118 0.014 3.97 2.85 ## 3 IEE 26 female NY 2 no 71 118 0.093 5.41 10.2 ## 4 AGC 43 male NY 1 yes 77 108 0.072 1.44 0.775 4.5.3 select() - Selecting specific columns Sometimes datasets are too big and unwieldy to look at all at once. Often, we are just interested in certain columns and it makes more sense just to keep the ones we want to focus on. We can use select() to only keep certain columns: For example, to only keep the columns ids, smoker, hrate and children, wed do the following: bw %&gt;% select(ids, smoker, hrate, children) ## # A tibble: 30 x 4 ## ids smoker hrate children ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC no 63 1 ## 2 GBH yes 73 1 ## 3 EDH no 65 0 ## 4 AAA no 66 3 ## 5 AJF no 67 0 ## 6 FJC yes 80 1 ## 7 IEE no 71 2 ## 8 BED no 62 0 ## 9 BFB no 68 0 ## 10 IEA yes 74 0 ## # ... with 20 more rows Occasionally, you may just want to get rid of certain columns from your data. To get rid of one column you can use select(-column_name). To get rid of the children column, wed do the following: bw %&gt;% select(-children) ## # A tibble: 30 x 10 ## ids age sex state smoker hrate bpsyst cellcount immuncount immuncount2 ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 19 female NJ no 63 101 0.126 0.993 0.921 ## 2 GBH 20 male NY yes 73 120 0.169 1.18 1.19 ## 3 EDH 21 female NJ no 65 100 0.281 4.34 3.21 ## 4 AAA 21 female CT no 66 109 0.244 2.56 4.01 ## 5 AJF 24 female NJ no 67 108 0.092 6.45 9.13 ## 6 FJC 25 female NY yes 80 118 0.014 3.97 2.85 ## 7 IEE 26 female NY no 71 118 0.093 5.41 10.2 ## 8 BED 28 female CT no 62 104 0.082 1.18 0.788 ## 9 BFB 28 female NJ no 68 118 0.197 0.724 0.848 ## 10 IEA 29 female CT yes 74 117 0.429 5.15 4.97 ## # ... with 20 more rows To get rid of the children, bpsyst and smoker columns, wed do the following: bw %&gt;% select(-children, -bpsyst, -smoker) ## # A tibble: 30 x 8 ## ids age sex state hrate cellcount immuncount immuncount2 ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 19 female NJ 63 0.126 0.993 0.921 ## 2 GBH 20 male NY 73 0.169 1.18 1.19 ## 3 EDH 21 female NJ 65 0.281 4.34 3.21 ## 4 AAA 21 female CT 66 0.244 2.56 4.01 ## 5 AJF 24 female NJ 67 0.092 6.45 9.13 ## 6 FJC 25 female NY 80 0.014 3.97 2.85 ## 7 IEE 26 female NY 71 0.093 5.41 10.2 ## 8 BED 28 female CT 62 0.082 1.18 0.788 ## 9 BFB 28 female NJ 68 0.197 0.724 0.848 ## 10 IEA 29 female CT 74 0.429 5.15 4.97 ## # ... with 20 more rows You can also select and rename columns as you go. Here, we are selecting the columns ids, sex, smoker, hrate and children. We are renaming sex to be gender and ids to be subject. You rename by just typing the new name and then putting an = sign in front of the old name: bw %&gt;% select(subject = ids, gender = sex, smoker, hrate, children) ## # A tibble: 30 x 5 ## subject gender smoker hrate children ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC female no 63 1 ## 2 GBH male yes 73 1 ## 3 EDH female no 65 0 ## 4 AAA female no 66 3 ## 5 AJF female no 67 0 ## 6 FJC female yes 80 1 ## 7 IEE female no 71 2 ## 8 BED female no 62 0 ## 9 BFB female no 68 0 ## 10 IEA female yes 74 0 ## # ... with 20 more rows If you want these selections to be permanent then you need to rewrite selections in new dataframe. Here we call our new dataframe bw1. You can see the difference between bw and bw1: bw1 &lt;- bw %&gt;% select(subject = ids, gender = sex, smoker,hrate,children) head(bw) ## # A tibble: 6 x 11 ## ids age sex state children smoker hrate bpsyst cellcount immuncount immuncount2 ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 19 female NJ 1 no 63 101 0.126 0.993 0.921 ## 2 GBH 20 male NY 1 yes 73 120 0.169 1.18 1.19 ## 3 EDH 21 female NJ 0 no 65 100 0.281 4.34 3.21 ## 4 AAA 21 female CT 3 no 66 109 0.244 2.56 4.01 ## 5 AJF 24 female NJ 0 no 67 108 0.092 6.45 9.13 ## 6 FJC 25 female NY 1 yes 80 118 0.014 3.97 2.85 head(bw1) ## # A tibble: 6 x 5 ## subject gender smoker hrate children ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC female no 63 1 ## 2 GBH male yes 73 1 ## 3 EDH female no 65 0 ## 4 AAA female no 66 3 ## 5 AJF female no 67 0 ## 6 FJC female yes 80 1 Instead of typing out the column name each time you use select(), you can also use the column number. The code below selects for columns 1-2, 8, 10, but does not save the information as a new object. Notice that you dont need to put these numbers inside of c() for this to work: bw %&gt;% select(1:2,8,10) ## # A tibble: 30 x 4 ## ids age bpsyst immuncount ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 19 101 0.993 ## 2 GBH 20 120 1.18 ## 3 EDH 21 100 4.34 ## 4 AAA 21 109 2.56 ## 5 AJF 24 108 6.45 ## 6 FJC 25 118 3.97 ## 7 IEE 26 118 5.41 ## 8 BED 28 104 1.18 ## 9 BFB 28 118 0.724 ## 10 IEA 29 117 5.15 ## # ... with 20 more rows 4.5.4 mutate() - Creating new columns We use mutate() to add new columns to our dataset. Say we wanted to create a new column called totalimmune that is the sum of the two columns immuncount and immuncount2. Basically, we want to add each subjects immuncount and immuncount2 value together to create a new value called totalimmune that well put into the new column. The first thing you put inside mutate() is the name we want of the new column. immuncount + immuncount means to add these two columns. To make these easier to see, well just select three columns and call the new dataset bw2: bw2 &lt;- bw %&gt;% select(ids, immuncount, immuncount2) bw2 ## # A tibble: 30 x 3 ## ids immuncount immuncount2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 0.993 0.921 ## 2 GBH 1.18 1.19 ## 3 EDH 4.34 3.21 ## 4 AAA 2.56 4.01 ## 5 AJF 6.45 9.13 ## 6 FJC 3.97 2.85 ## 7 IEE 5.41 10.2 ## 8 BED 1.18 0.788 ## 9 BFB 0.724 0.848 ## 10 IEA 5.15 4.97 ## # ... with 20 more rows bw2 %&gt;% mutate(totalimmune = immuncount + immuncount2) ## # A tibble: 30 x 4 ## ids immuncount immuncount2 totalimmune ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 0.993 0.921 1.91 ## 2 GBH 1.18 1.19 2.37 ## 3 EDH 4.34 3.21 7.55 ## 4 AAA 2.56 4.01 6.57 ## 5 AJF 6.45 9.13 15.6 ## 6 FJC 3.97 2.85 6.82 ## 7 IEE 5.41 10.2 15.6 ## 8 BED 1.18 0.788 1.97 ## 9 BFB 0.724 0.848 1.57 ## 10 IEA 5.15 4.97 10.1 ## # ... with 20 more rows You can also create new columns in other ways. For example, if you want to create a new column called year and put 2020 into each row, wed do the following: bw2 %&gt;% mutate(year = 2020) ## # A tibble: 30 x 4 ## ids immuncount immuncount2 year ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 0.993 0.921 2020 ## 2 GBH 1.18 1.19 2020 ## 3 EDH 4.34 3.21 2020 ## 4 AAA 2.56 4.01 2020 ## 5 AJF 6.45 9.13 2020 ## 6 FJC 3.97 2.85 2020 ## 7 IEE 5.41 10.2 2020 ## 8 BED 1.18 0.788 2020 ## 9 BFB 0.724 0.848 2020 ## 10 IEA 5.15 4.97 2020 ## # ... with 20 more rows 4.5.5 arrange() - Sort Data Columns Often its easier to see data if the columns are sorted in ascending or descending order. We can do this using arrange(). Lets try another example! Well use the pga.csv dataset that has historical golf data summary statistics in it. Lets load in that dataset: pga &lt;- read_csv(&quot;data/pga.csv&quot;) We will select the columns name, year, total.holes, total.putts, and score.avg and save as pga1. pga1 &lt;- pga %&gt;% select(name, year, total.holes, total.putts, score.avg) head(pga1) ## # A tibble: 6 x 5 ## name year total.holes total.putts score.avg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Fred Funk 2004 1728 2774 70.4 ## 2 Scott Verplank 2004 1638 2619 69.9 ## 3 Craig Bowden 2004 1494 2401 71.5 ## 4 Joe Durant 2004 1530 2563 70.7 ## 5 Tom Byrum 2004 1494 2380 70.5 ## 6 Jose Coceres 2004 1152 1819 71.0 You can see that we have the total number of holes played, the total number of putts made, and the scoring average (lower is better in golf) made each year by various PGA golfers. Perhaps we want to know who has the highest or lowest scoring average. We can sort based on the column score.avg in ascending order with arrange() like this: pga1 %&gt;% arrange(score.avg) ## # A tibble: 2,269 x 5 ## name year total.holes total.putts score.avg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Tiger Woods 2007 1080 1736 67.8 ## 2 Tiger Woods 2009 1116 1763 68.1 ## 3 Tiger Woods 2006 936 1528 68.1 ## 4 Tiger Woods 2005 1332 2124 68.7 ## 5 Rory McIlroy 2014 1152 1830 68.8 ## 6 Vijay Singh 2004 1980 3216 68.8 ## 7 Luke Donald 2011 1206 1878 68.9 ## 8 Jim Furyk 2006 1584 2539 68.9 ## 9 Rory McIlroy 2012 972 1551 68.9 ## 10 Steve Stricker 2013 846 1348 68.9 ## # ... with 2,259 more rows So, the lowest average score was 67.794 by Tiger Woods in 2007. The second lowest was 68.052 by Tiger Woods in 2009. To sort data in descending order, we need to put a negative sign (hyphen) - in front of the column name: pga1 %&gt;% arrange(-score.avg) ## # A tibble: 2,269 x 5 ## name year total.holes total.putts score.avg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 David Gossett 2004 1026 1693 75.0 ## 2 Kevin Muncrief 2004 900 1490 73.5 ## 3 Chris Couch 2004 990 1635 73.5 ## 4 Hidemichi Tanaka 2006 1314 2136 73.5 ## 5 David Duval 2008 918 1497 73.2 ## 6 Brad Faxon 2010 972 1567 73.1 ## 7 Eric Axley 2009 1206 1916 73.1 ## 8 Hirofumi Miyase 2004 1134 1857 73.1 ## 9 Greg Kraft 2010 828 1370 73.1 ## 10 Stephen Gangluff 2012 918 1507 73.0 ## # ... with 2,259 more rows The worst average score on the PGA tour was by David Gossett in 2004 with an average of 75.013. arrange() can also sort data in ascending alphabetical order, if you put a column with character data into the function, such as the name column in our data: pga1 %&gt;% arrange(name) ## # A tibble: 2,269 x 5 ## name year total.holes total.putts score.avg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aaron Baddeley 2004 1530 2411 71.6 ## 2 Aaron Baddeley 2005 1476 2303 71.3 ## 3 Aaron Baddeley 2006 1368 2125 71.2 ## 4 Aaron Baddeley 2007 1440 2261 70.1 ## 5 Aaron Baddeley 2008 1314 2072 70.2 ## 6 Aaron Baddeley 2009 1170 1826 71.2 ## 7 Aaron Baddeley 2010 1692 2686 71.0 ## 8 Aaron Baddeley 2011 1350 2129 70.2 ## 9 Aaron Baddeley 2012 1296 2021 71.1 ## 10 Aaron Baddeley 2013 1188 1835 71.5 ## # ... with 2,259 more rows It turns out that Aaron Baddeley is the player that is closest to the beginning of the alphabet. You cannot sort in a descending way on character data. If you wish to sort over multiple columns, you just need to separate your column names with a comma inside arrange(). So, to first sort by year, and then by score.avg, wed do the following: pga1 %&gt;% arrange(year, score.avg) ## # A tibble: 2,269 x 5 ## name year total.holes total.putts score.avg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Vijay Singh 2004 1980 3216 68.8 ## 2 Ernie Els 2004 1044 1653 69.0 ## 3 Tiger Woods 2004 1296 2048 69.0 ## 4 Phil Mickelson 2004 1422 2287 69.2 ## 5 Retief Goosen 2004 990 1580 69.3 ## 6 Sergio Garcia 2004 1152 1919 69.8 ## 7 Stewart Cink 2004 1746 2731 69.8 ## 8 Stephen Ames 2004 1710 2756 69.9 ## 9 Scott Verplank 2004 1638 2619 69.9 ## 10 David Toms 2004 1332 2174 70.0 ## # ... with 2,259 more rows You can see that the lowest year in this dataset is 2004, and so it put all the rows with 2004 at the top, and then sorted each of these rows by score.avg. So, Vijay Singh had the lowest score with 68.839 in 2004. 4.5.6 Chaining together Just as an extra note, the real power of these tidyverse commands, is by chaining them all together. So, one thing we could do is select from our original pga dataset the columns above, then create a new column called putt.avg which is equal to the total number of putts total.putts divided by the total number of holes total.holes columns, and then arrange in ascending order by the new putt.avg column. Here it is in one bit of code: pga %&gt;% select(name, year, total.holes, total.putts, score.avg) %&gt;% mutate(putt.avg = total.putts / total.holes) %&gt;% arrange(putt.avg) ## # A tibble: 2,269 x 6 ## name year total.holes total.putts score.avg putt.avg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Brian Gay 2013 1422 2173 71.2 1.53 ## 2 Kevin Na 2011 1512 2331 70.4 1.54 ## 3 Greg Chalmers 2011 1548 2388 70.6 1.54 ## 4 Justin Leonard 2014 1350 2083 70.9 1.54 ## 5 Steve Stricker 2005 1116 1723 71.1 1.54 ## 6 Aaron Baddeley 2013 1188 1835 71.5 1.54 ## 7 Jordan Spieth 2015 1584 2448 68.9 1.55 ## 8 Jordan Spieth 2014 1764 2730 69.9 1.55 ## 9 Tim Clark 2007 1152 1784 69.9 1.55 ## 10 Jonas Blixt 2012 1260 1952 70.2 1.55 ## # ... with 2,259 more rows It turns out that Brian Gay in 2013 had the fewest putts per hole. 4.6 Wide versus Long Data Sometimes you will need to rearrange your data for some data analysis and more commonly for data visualization. Dataframes can come in two broad shapes. There is wide data where each row is a separate subject and each column has variables that may or may not contain the same type of data. The other is long data where values of the same type are only in one column, and the same subject may appear on several rows. This is easier to understand with a visual aid. The image below show data that is in wide format. Each row contains all the information for each subject - those being three NHL hockey teams. We see the division that each team belongs to, and the total goals they scored in the seasons beginning in 2016, 2017, 2018. These data are also untidy - as we prefer to have each row contain all the information for one unique value. To do that we need to turn the data into long format which looks like this: Here, each row contains information related to the team, division, the total number of goals scored, and the year that that piece of data came from. Because each row contains unique information, we call this type of data tidy. Essentially, you can see we took columns 3,4, and 5 from the wide data and made these longer. It can take a bit of practice to fully recognize what data are in wide or long format - sometimes its not so clear in really large datasets. This does become important though, because some visualization and analyses can only be done with data in wide format, and others with data in long format. Its therefore important to be able to switch our dataframes through these two formats. Fortunately, the tidyverse has built in functions that can turn your dataframe from long format to wide format and vice versa. pivot_longer makes the dataframes longer by increasing the number of rows by combing the number of columns. This is called long form data, which is needed to tidy data for graphing and some analysis. First, well make some small dataframe examples manually. The first dataframe is a wide dataframe. You could imagine this as showing five subjects and their reaction times at two different timepoints. We call this dataframe df.wide: # wide data has more than one score of each type per individual in rows df.wide &lt;- data.frame( name = c(&quot;James&quot;, &quot;Tyler&quot;, &quot;Stephen&quot;, &quot;Jennifer&quot;, &quot;Carmen&quot;), time1 = c(15, 17, 14, 13, 11), time2 = c(16, 19, 20, 21, 23) ) df.wide ## name time1 time2 ## 1 James 15 16 ## 2 Tyler 17 19 ## 3 Stephen 14 20 ## 4 Jennifer 13 21 ## 5 Carmen 11 23 The second dataframe well create manually contains the same data, but in a long tidy format: # long data (also called tidy data) has only one column for each type of score # Every row has the information to make each score unique df.long &lt;- data.frame( name = c(&quot;James&quot;, &quot;Tyler&quot;, &quot;Stephen&quot;, &quot;Jennifer&quot;, &quot;Carmen&quot;), score = c(15, 17, 14, 13, 11, 16, 19, 20, 21, 23), time = rep(c(&quot;time1&quot;, &quot;time2&quot;), each = 5) ) df.long ## name score time ## 1 James 15 time1 ## 2 Tyler 17 time1 ## 3 Stephen 14 time1 ## 4 Jennifer 13 time1 ## 5 Carmen 11 time1 ## 6 James 16 time2 ## 7 Tyler 19 time2 ## 8 Stephen 20 time2 ## 9 Jennifer 21 time2 ## 10 Carmen 23 time2 4.6.1 Wide to Long To go from wide to long format we can use pivot_longer. We need to tell R which columns contain the data that are currently in wide format and need to be turned into long format. Here, we put cols = 2:3 to indicate that the data are in columns 2 and 3. We also use names_to to tell it that we want the new column to be called time: # turn columns 2 and 3 into one long column: df.wide %&gt;% pivot_longer(cols = 2:3, names_to=&quot;time&quot;) ## # A tibble: 10 x 3 ## name time value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 James time1 15 ## 2 James time2 16 ## 3 Tyler time1 17 ## 4 Tyler time2 19 ## 5 Stephen time1 14 ## 6 Stephen time2 20 ## 7 Jennifer time1 13 ## 8 Jennifer time2 21 ## 9 Carmen time1 11 ## 10 Carmen time2 23 Our example dataset was very simple, only containing an identifier (name) column and the two columns with data. What if we had other columns in our dataset? Here we add a column called group that indicates if our subject are in a control or treatment group: df.wide$group &lt;- c(&quot;control&quot;, &quot;control&quot;, &quot;treatment&quot;, &quot;treatment&quot;, &quot;treatment&quot;) df.wide ## name time1 time2 group ## 1 James 15 16 control ## 2 Tyler 17 19 control ## 3 Stephen 14 20 treatment ## 4 Jennifer 13 21 treatment ## 5 Carmen 11 23 treatment To convert this dataframe to a long dataframe, we still use the same code indicating that the data to make long are contained in columns 2 and 3. Because each individual is always in the same group (e.g. James is always a control, Carmen is always in treatment), R will just make this a new column: df.wide %&gt;% pivot_longer(cols = 2:3, names_to=&quot;time&quot;) ## # A tibble: 10 x 4 ## name group time value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 James control time1 15 ## 2 James control time2 16 ## 3 Tyler control time1 17 ## 4 Tyler control time2 19 ## 5 Stephen treatment time1 14 ## 6 Stephen treatment time2 20 ## 7 Jennifer treatment time1 13 ## 8 Jennifer treatment time2 21 ## 9 Carmen treatment time1 11 ## 10 Carmen treatment time2 23 4.6.2 Long to Wide If we start with data in the long format, we can convert the dataframe to wide format using pivot_wider(). Here, we need to provide the name of the column that contains the data to become new wide columns. We do that below with values_from = score. We also need to state which column contains the new column headers. In this example they are in the column time, so we do that with names_from = time. head(df.long) ## name score time ## 1 James 15 time1 ## 2 Tyler 17 time1 ## 3 Stephen 14 time1 ## 4 Jennifer 13 time1 ## 5 Carmen 11 time1 ## 6 James 16 time2 df.long %&gt;% pivot_wider(values_from = score, names_from = time) ## # A tibble: 5 x 3 ## name time1 time2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 James 15 16 ## 2 Tyler 17 19 ## 3 Stephen 14 20 ## 4 Jennifer 13 21 ## 5 Carmen 11 23 Now our data are in a wide format. Again, it is more common that we have more columns in our dataset. Say we had rated the confidence of each participant on the tasks prior to testing them. We may have a column called confidence that might look like this: # if we had more columns in our dataset: df.long$confidence &lt;- c(3, 8, 9, 4, 10) # this repeats all the way down the dataset df.long ## name score time confidence ## 1 James 15 time1 3 ## 2 Tyler 17 time1 8 ## 3 Stephen 14 time1 9 ## 4 Jennifer 13 time1 4 ## 5 Carmen 11 time1 10 ## 6 James 16 time2 3 ## 7 Tyler 19 time2 8 ## 8 Stephen 20 time2 9 ## 9 Jennifer 21 time2 4 ## 10 Carmen 23 time2 10 Even when we have more columns, we still can make our data go wide in the same way: df.long %&gt;% pivot_wider(values_from = score, names_from = time) ## # A tibble: 5 x 4 ## name confidence time1 time2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 James 3 15 16 ## 2 Tyler 8 17 19 ## 3 Stephen 9 14 20 ## 4 Jennifer 4 13 21 ## 5 Carmen 10 11 23 Here, it just truncates the confidence column into one column to go along with name. 4.6.3 Real Data Example. It can be quite hard to work out which columns to select when converting data from long to wide. It mainly comes from practice and from trial-and-error. You can always check if you got it right, and correct yourself if you did not. In this section, well just show an example of converting dataframes between these two formats using some real data. The dataset well use is called wheels1.csv. Well read it in and assign the name wheel: wheel &lt;- read_csv(&quot;data/wheels1.csv&quot;) head(wheel) ## # A tibble: 6 x 14 ## strain id day1 day2 day3 day4 dob mother sex wheel startdate total age group ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 B6 692ao 12853 8156. 9028. 12516. 4/18/2005 633ax male 9 10/9/2005 42552. 174 inbred ## 2 B6 656aa 2644 4012. 5237 7404. 12/16/2004 593ar male 1 9/12/2005 19297 270 inbred ## 3 B6 675ag 4004. 3054. 3816. 3761 2/9/2005 593ad male 5 10/13/2005 14634. 246 inbred ## 4 B6 675ai 11754. 8863 11784 11684 2/9/2005 593ad male 4 10/13/2005 44086. 246 inbred ## 5 B6 656af 6906. 5322. 10424. 8468. 12/16/2004 593ar male 2 10/13/2005 31122 301 inbred ## 6 B6 656al 6517 4440 5334. 9291 12/19/2004 554aa male 8 10/9/2005 25582. 294 inbred Were mainly interested in the id, strain and columns with day in, so we shall use select() to select these columns: wheeldf &lt;- wheel %&gt;% select(id, strain, day1,day2,day3,day4) head(wheeldf) ## # A tibble: 6 x 6 ## id strain day1 day2 day3 day4 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 692ao B6 12853 8156. 9028. 12516. ## 2 656aa B6 2644 4012. 5237 7404. ## 3 675ag B6 4004. 3054. 3816. 3761 ## 4 675ai B6 11754. 8863 11784 11684 ## 5 656af B6 6906. 5322. 10424. 8468. ## 6 656al B6 6517 4440 5334. 9291 These data represent the number id of individual mice, the strain of that mouse, and the number of wheel revolutions made on day1, day2, day3 and day4. Its currently in wide format. Lets convert it to long format, by taking the data in each day column and forcing them to go long: #convert to long wheeldf.long &lt;- wheeldf %&gt;% pivot_longer(cols = 3:6, names_to=&quot;day&quot;) wheeldf.long ## # A tibble: 320 x 4 ## id strain day value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 692ao B6 day1 12853 ## 2 692ao B6 day2 8156. ## 3 692ao B6 day3 9028. ## 4 692ao B6 day4 12516. ## 5 656aa B6 day1 2644 ## 6 656aa B6 day2 4012. ## 7 656aa B6 day3 5237 ## 8 656aa B6 day4 7404. ## 9 675ag B6 day1 4004. ## 10 675ag B6 day2 3054. ## # ... with 310 more rows If we wished to make our new wheeldf.long dataset to go back to a wide format, we could do this: # to make it go wider again, we do: wheeldf.long %&gt;% pivot_wider( names_from = day, values_from = value ) ## # A tibble: 80 x 6 ## id strain day1 day2 day3 day4 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 692ao B6 12853 8156. 9028. 12516. ## 2 656aa B6 2644 4012. 5237 7404. ## 3 675ag B6 4004. 3054. 3816. 3761 ## 4 675ai B6 11754. 8863 11784 11684 ## 5 656af B6 6906. 5322. 10424. 8468. ## 6 656al B6 6517 4440 5334. 9291 ## 7 692an B6 11366. 6473 11790. 15724 ## 8 705ab B6 13794. 10868 14374. 12729 ## 9 692at B6 10486. 12650. 15851 15790 ## 10 692au B6 4592. 5819 9272 11379 ## # ... with 70 more rows 4.7 Joins A common requirement in data science is to join together two datasets. Usually these two datasets will have some bits of data in common. For instance, you may have one dataset that contains subject names and lots of demographic information. Another dataset might contain the subject names and their performance on various tests. Ideally, youd have all of this information in one dataframe. The tidyverse is excellent at joining dataframes quickly and automatically. It is far more foolproof than cutting, copying and pasting in excel which should always be a no-no. In this course, we wont actually ever ask you to do any joining. All the datasets that we provide are already cleaned up. However, we are including this small subsection in this guide to introduce you to the topic, should it be something you ever need to do on your own. Joining is surprisingly a really big topic by itself, so we just scratch the surface here. For much more information on this topic, we highly recommend Prof Jenny Bryans guides as a good starting point. To illustrate the power of joining, lets make up some data. The first dataframe, x contains ten subjects who are referred to by an id and their ages (age). x &lt;- data.frame(&quot;id&quot; = 1:10, &quot;age&quot; = c(21,25,17,34,25,33,22,27,29,24)) head(x) ## id age ## 1 1 21 ## 2 2 25 ## 3 3 17 ## 4 4 34 ## 5 5 25 ## 6 6 33 The second dataframe, y, contains the id of each individual again, but this time we also have data on their height, as well as how many hours they spent on an activity and some work. y &lt;- data.frame( &quot;id&quot; = 1:10, &quot;height_cm&quot; = c(156, 155, 154, 149, 153, 152, 151, 150, 147, 155), &quot;activity_hr&quot;= c(3,5,3,6,7,4,2,8,4,5), &quot;work_hr&quot; =c(40,35,38,46,50,42,40,46,41,40) ) head(y) ## id height_cm activity_hr work_hr ## 1 1 156 3 40 ## 2 2 155 5 35 ## 3 3 154 3 38 ## 4 4 149 6 46 ## 5 5 153 7 50 ## 6 6 152 4 42 Currently we have age in a separate dataframe. Perhaps we want to join it together with all the other information in our dataframe y. We can use full_join() to do this. full_join() joins two different dataframes based on one or more shared variable(s). The following will join based on id. x %&gt;% full_join(y) ## Joining, by = &quot;id&quot; ## id age height_cm activity_hr work_hr ## 1 1 21 156 3 40 ## 2 2 25 155 5 35 ## 3 3 17 154 3 38 ## 4 4 34 149 6 46 ## 5 5 25 153 7 50 ## 6 6 33 152 4 42 ## 7 7 22 151 2 40 ## 8 8 27 150 8 46 ## 9 9 29 147 4 41 ## 10 10 24 155 5 40 As you can see, R joined the two dataframes together and age is now included in the new dataframe. For this to work, you must have at least one column in each of your two dataframes that exactly matches by name. If you do not have this, R will not know how to join the dataframes together. If you have multiple matching columns, then R will match based on all columns that match - so be careful! "],["data-visualization.html", "Chapter 5 Data Visualization 5.1 Introduction to ggplot2 5.2 Histograms 5.3 Scatterplots 5.4 Line Graphs 5.5 Comparing Distributions across Groups 5.6 Bar Graphs 5.7 Small Multiples 5.8 Saving and Exporting ggplot2 graphs", " Chapter 5 Data Visualization In this chapter we shall discuss how we can make different types of data visualization in R. We will use the package ggplot2 to visualize data. This is part of the tidyverse package, meaning you should load tidyverse to ensure you have ggplot2 loaded. We shall also discuss a little bit about when to make different types of graphs, and what each type is best suited for. We will also give a few pieces of advice about how to make your visualizations as readable and interpretable as possible. For much more information on the theory of data visualization with excellent examples, please refer to the Fundamentals of Data Visualization book by Claus Wilke. To understand the power behind ggplot2 and for more data visualization examples, see the ggplot2: Elegant Graphics for Data Analysis by Hadley Wickham. 5.1 Introduction to ggplot2 The first thing to do when we want to make a visualization with ggplot2 is to load the tidyverse: library(tidyverse) Next, lets load in some data. Well pick the BlueJays.csv data: df &lt;- read_csv(&quot;data/BlueJays.csv&quot;) head(df) ## # A tibble: 6 x 9 ## BirdID KnownSex BillDepth BillWidth BillLength Head Mass Skull Sex ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0000-00000 M 8.26 9.21 25.9 56.6 73.3 30.7 1 ## 2 1142-05901 M 8.54 8.76 25.0 56.4 75.1 31.4 1 ## 3 1142-05905 M 8.39 8.78 26.1 57.3 70.2 31.2 1 ## 4 1142-05907 F 7.78 9.3 23.5 53.8 65.5 30.3 0 ## 5 1142-05909 M 8.71 9.84 25.5 57.3 74.9 31.8 1 ## 6 1142-05911 F 7.28 9.3 22.2 52.2 63.9 30 0 In the next few steps, well slowly build up a plot using ggplot2. This is not how you would typically write the code. However, it is worth going step by step, just to show you the logic behind the code. If we just run the function ggplot() notice that all we get is a blank gray canvas. R knows that we want to make a plot, but it has no idea what type of plot or with what data - so it just throws up the canvas: ggplot() Now, if we add the dataset to ggplot(), it still only gives us the blank canvas. It now knows we want to make a graph from the dataset called df but doesnt plot anything yet as we didnt tell it what to plot: ggplot(df) For R to know what you are trying to plot, you need to use aes(). You put that most of the time inside ggplot() after your dataframe name. (There are exceptions to this, but lets not worry about that yet). Inside the aes() well put what columns contain our data for the x and y axes. We may also refer to other columns inside aes() if we wish to modify the color or shape or something else about our data based on the values in some column. For our first example, lets make a scatterplot of body mass against head size of these Blue Jays. If you look at the original dataset, youll notice that both the Mass and Head columns contain continuous numeric data (i.e. they are numbers). In the code below, we are telling aes() to plot the Mass data on the x-axis and to plot the Head data on the y-axis. ggplot(df, aes(x=Mass, y=Head) ) Something did change this time. We get a plot with labels on the x- and y-axes. It recognizes that we wish to plot Mass and Head data. It even knows the range of our data on each axis. For instance, it knows that the Mass data lies somewhere between 55 and 85. However, we havent yet told it precisely what type of plot we want to make (it doesnt just assume that we wanted to make a scatterplot - it cant read our minds). So our next step is to tell it to make a scatterplot by adding points to the graph. We tell ggplot() what we are adding to the chart by using different geoms. For a scatterplot, the geom we require is geom_point() - that means add datapoints. It is hard to remember all the different geoms, but you can just look them up. Here is how we add datapoints to our graph with + geom_point(). ggplot(df, aes(x=Mass, y=Head) ) + geom_point() That is our first ggplot graph! It looks pretty good. The amazing thing about ggplot is almost everything you are looking at on that graph can be customized to your preferred design choice. Well discuss several of these customizations throughout this chapter. First, lets talk about changing the color of the datapoints. Inside of geom_point() we can change the color of all the points like this: ggplot(df, aes(x=Mass, y=Head) ) + geom_point(color=&quot;red&quot;) This made the points red. Just make sure you put a recognized color name (you can look them up here) or a recognized hex code. Notice though that color name must be put inside of quotes. What if we want to color the points based on another variable? For example, instead of having all of our data points be red, say we want them to be colored based on whether the birds or male or female? The column that has the information about whether the birds are male or female is KnownSex. Because we are basing the color on a column, we put that information inside of aes() with color = KnownSex. We dont put that inside geom_point(). This code looks like this: ggplot(df, aes(x=Mass, y=Head, color = KnownSex) ) + geom_point() 5.1.1 Assigning plots When we make plots, our code can start to get quite long as we make more and more additions or changes to the plot. One very useful thing to know is that we can assign our plot to be an object, just as we would with a vector or a dataframe. For instance, lets remake the plot above, but this time well assign it the name p. We do that using p &lt;-. p &lt;- ggplot(df, aes(x=Mass, y=Head, color = KnownSex) ) + geom_point() Now, whenever we type and run p we will get our plot. e.g. p 5.1.2 Titles and Axes Titles The advantage of assigning our plots to a short name, is that we can add things with less code. In R, if we wish to add a title to a plot, we do this with + ggtitle(). So, here is how we add a title to our above plot: p + ggtitle(&quot;Our first scatterplot&quot;) The above plot is basically the same as writing: ggplot(df, aes(x=Mass, y=Head, color = KnownSex) ) + geom_point() + ggtitle(&quot;Our First Scatterplot&quot;) Youll notice that we are chaining together commands with the +. This is similar to how we chain together commands with the %&gt;% when doing data carpentry. ggplot() instead chains with the +. Again, be careful not to start a row with a +, and you must end a row with a + unless its the very last row. To change the title of the x-axis or the y-axis, we use xlab and ylab respectively. We can do it like this: p + xlab(&quot;Body Mass (g)&quot;) + ylab(&quot;Head Size (mm)&quot;) 5.1.3 Colors, Shapes and Sizes R recognizes many default color names. These can be found at either of these places: Color names 1 Color names 2 Or, you can use a hex code Here we use the color dodgerblue to change all the points to that color: ggplot(df, aes(x=Mass, y=Head) ) + geom_point(color=&quot;dodgerblue&quot;) Here we change the points to the color #ababcc using a hexcode - note that hexcodes need to have # in front of them: ggplot(df, aes(x=Mass, y=Head) ) + geom_point(color=&quot;#ababcc&quot;) You can also change the shape of the points you plot with geom_point(pch = ). You need to insert the appropriate number according to this guide: For example, to have dodgerblue asterisks, we add pch = 8, separating the color and shape commands by a comma: ggplot(df, aes(x=Mass, y=Head) ) + geom_point(color=&quot;dodgerblue&quot;, pch = 8) Finally, we can change the size of our datapoints (or other shape we choose), using size =: ggplot(df, aes(x=Mass, y=Head) ) + geom_point(color=&quot;purple&quot;, size=2) 5.1.4 Themes Default Themes You may have noticed that every plot we have made so far has the same gray background with faint white gridlines. This is the default setting for the look of ggplot() graphs. There are several other themes that are available to us that change the overall appearance of our plots. Some of these are listed below: theme_bw() a variation on theme_grey() that uses a white background and thin grey grid lines. theme_linedraw() A theme with only black lines of various widths on white backgrounds, reminiscent of a line drawing. theme_light() similar to theme_linedraw() but with light grey lines and axes, to direct more attention towards the data. theme_dark() the dark cousin of theme_light(), with similar line sizes but a dark background. Useful to make thin colored lines pop out. theme_minimal() A minimalistic theme with no background annotations. theme_classic() A classic-looking theme, with x and y axis lines and no gridlines. theme_void() A completely empty theme Lets shows a couple of these different themes. The theme that we use the most in this course is theme_classic(). This is how you would apply this theme to your plot: ggplot(df, aes(x=Mass, y=Head) ) + geom_point() + theme_classic() It creates a very sleek simple graph. The downside to this type of graph is that it does get rid of the gridlines which can be helpful sometimes. Another theme that we use often is theme_minimal(). Here is how we would add this: ggplot(df, aes(x=Mass, y=Head) ) + geom_point() + theme_minimal() This theme is also simplistic, but has gridlines too. Custom themes Rather than changing many different aspects of the graph at once, we can change individual things one by one with theme(). We dont propose to cover this in more detail in this book - for more information about themes look here - however, here is one quick example. Lets say we wanted to make the panel background light blue instead of gray. We could do it like this: ggplot(df, aes(x=Mass, y=Head) ) + geom_point() + theme(panel.background = element_rect(fill = &quot;lightblue&quot;)) Again, this can get quite complicated - so stick with the default themes if you want to change your plots up a bit, or go to other help guides for more fine detail on customization. 5.2 Histograms Histograms are very common data visualizations. Histograms plot the frequency on the y-axis of a continuous variable on the x-axis. For instance, lets say we had the following data, that well call d1: d1 &lt;- data.frame(vals = c(1, 3, 4, 3, 6, 7, 2, 9, 3, 2, 2, 3, 1, 5, 4, 4)) d1 ## vals ## 1 1 ## 2 3 ## 3 4 ## 4 3 ## 5 6 ## 6 7 ## 7 2 ## 8 9 ## 9 3 ## 10 2 ## 11 2 ## 12 3 ## 13 1 ## 14 5 ## 15 4 ## 16 4 If we wanted to know how many of each number in the column vals we have, we could use table(): table(d1$vals) ## ## 1 2 3 4 5 6 7 9 ## 2 3 4 3 1 1 1 1 The table above represents the frequency table or frequency count of the data. We can plot these data like this: In this histogram, the height of each bar represents the total amount of the number on the x-axis. So, the height of the bar at x=9 is one. This mean we have 1 of this value in our data distribution. The height of the bar at x=3 is four, therefore we have four in our distribution for the value 3. In the example above, the width of the bars is precisely 1. We could change the width to say two. This is illustrated below: Here, the first bar is at height 9. It spans the values of x between 1-3. The second bar is at height 4, this include values between 3.01-5, and so on. What we did here was to adjust the binwidth. When we have large distributions, adjusting the binwidth helps us to interpret the data more easily. 5.2.1 Histograms with ggplot2 To describe how to make histograms with the ggplot() function, lets look at the films.csv dataset. film &lt;- read_csv(&quot;data/films.csv&quot;) head(film) ## # A tibble: 6 x 5 ## film year rottentomatoes imdb metacritic ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Avengers: Age of Ultron 2015 74 7.8 66 ## 2 Cinderella 2015 85 7.1 67 ## 3 Ant-Man 2015 80 7.8 64 ## 4 Do You Believe? 2015 18 5.4 22 ## 5 Hot Tub Time Machine 2 2015 14 5.1 29 ## 6 The Water Diviner 2015 63 7.2 50 This dataset contains 146 rows of data. Each row has a unique film, with the final three columns giving three different ratings measures of how good the film was. These are their respective rottentomatoes, imdb and metacritic scores. If we wished to plot the distribution of imdb scores, we need to put x=imdb inside the aes() part of the ggplot code. That is to tell it to plot these scores on the x-axis. We do not need to put a y= inside this, as we are not plotting anything from our dataset on the y-axis. Instead, ggplot2 will count up the frequency of our scores between regular intervals of imdb scores. We then add + geom_histogram() to tell it to make a histogram. All together it looks like this: ggplot(film, aes(x=imdb)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Now, this doesnt look great and we have several problems with it. The two major problems that we get with our first histograms are. 1) The binwidth is almost never appropriate. We need to tell ggplot exactly what we want the binwidth on the x-axis to be. That is, what interval do we want our scores to be counted over. Looking at the graph, our scores range from just below 4 to about 8.6. Perhaps a better interval would be 0.2, so we count how many films had scores between 3.6-3.8, 3.8-4.0, 4.0-4.2, 4.2-4.4, .. 8.4-8.6, 8.6-8.8 etc. 2) Having black bars makes it really hard to distinguish the bars when they are close in heights. We need to fix the color scheme. OK, lets make the bars dodgerblue and border them white. Inside geom_histogram() we use color=\"white\" to represent the outside lines of the bars. We use fill=\"dodgerblue to indicate the color inside the bars should be dodgerblue. ggplot(film, aes(x=imdb)) + geom_histogram(color=&quot;white&quot;, fill=&quot;dodgerblue&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Now lets fix that binwidth. To resolve this, inside geom_histogram() we write binwidth = 0.2. ggplot(film, aes(x = imdb)) + geom_histogram(binwidth = 0.2, color=&quot;white&quot;, fill=&quot;dodgerblue&quot;) This looks a lot better. Now we can see that the majority of films have ratings in the 6.2-7.8 range, with relatively few above 8 and below 5. Its not always easy to know what size interval to choose for the x-axis in histograms. Its worth just playing around with that number and seeing how it looks. When we set the interval to be some value - here, we chose 0.2 - R doesnt automatically make that between easy to interpret numbers such as 4.0-4.2, 4.2-4.4 etc. It could just as easily have chosen 3.874-4.074, 4.074-4.274. Obviously, the latter is hard for us to interpret when looking at the axes. You can see in the above plot, that the vertical lines of the histogram bars dont neatly fall on top of whole numbers. To fix, this you can adjust the boundaries by picking a value to center your interval on. So, if we pick boundary=4, then that will be a boundary marker, and the interval will go 4.0-4.2, 4.2-4.4 etc. ggplot(film, aes(x = imdb)) + geom_histogram(binwidth = 0.2, color=&quot;white&quot;, fill=&quot;dodgerblue&quot;,boundary=4) Just be careful with using the boundaries that it does not crop your histogram incorrectly. Changing histograms too much can lead to misrepresenting the data. We would recommend that you dont use the boundary feature unless you have a real need to do so - just be careful! Like with all ggplot figures, you can add as much customization as you wish. Here, we add a new theme, title and x- and y-axis labels: ggplot(film, aes(x = imdb)) + geom_histogram(binwidth = 0.2, color=&quot;white&quot;, fill=&quot;dodgerblue&quot;) + theme_classic() + ggtitle(&quot;Histogram of IMDB Ratings&quot;) + xlab(&quot;Rating&quot;) + ylab(&quot;Frequency&quot;) This looks really nice ! 5.2.2 Density Curves Instead of plotting the frequency or counts of values on the y-axis, we can instead plot density. Here, we essentially convert the histogram to a solid line that estimates the overall shape of the distribution. We call this line a density curve. You can make this plot using ggplot() using + geom_density() instead of + geom_histogram(). In the code below we do this for the imdb ratings, and we make the line color navy, and the fill of the density curve dodgerblue: ggplot(film, aes(x = imdb)) + geom_density(color = &quot;navy&quot;, fill = &quot;dodgerblue&quot;) Usually the fill of these plots is too much, so its nice to add some transparency. You can do that by picking a number between 0 and 1 to provide to the alpha argument. Here we choose alpha = .4: ggplot(film, aes(x = imdb)) + geom_density(color = &quot;navy&quot;, fill = &quot;dodgerblue&quot;, alpha=.4) The useful thing about density plots is that they give you a quick visual aid as to the overall shape of the distribution. You can easily see where the bulk of the data lie (here between 6 and 8 ratings score), and whether the data is symmetrical or not. 5.2.3 Comparing Distributions Instead of just plotting one histogram or one density curve, we often are interested in comparing two or more distributions. This means we are interested in comparing two or more histograms or density curves. To do this, we first need to ensure that our data are all measured in the same units. Overlaid Histograms To illustrate this, lets use the lifeexp.csv data which contains life expectancy data for many countries. life &lt;- read_csv(&quot;data/lifeexp.csv&quot;) head(life) ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia year_1952 28.8 8425333 779. ## 2 Afghanistan Asia year_2007 43.8 31889923 975. ## 3 Albania Europe year_1952 55.2 1282697 1601. ## 4 Albania Europe year_2007 76.4 3600523 5937. ## 5 Algeria Africa year_1952 43.1 9279525 2449. ## 6 Algeria Africa year_2007 72.3 33333216 6223. You can see that one of the columns is called lifeExp which is the life expectancy of each country in either 1952 or 2007. The year is shown in the year column, and the country is shown in the country column. Youll notice that these data are in long format (see section 4.6). Perhaps we are interested in the distribution of life expectancies across all countries in the year 1952 compared to the distribution of life expectancies in the year 2007. We have a few options to do this. The first option does not look good for this example (although it may work in other situations). This is an overlaid histogram. To do this, inside aes() as well as saying which column our distribution data is in x=lifeExp, we also tell it to make separate histograms based on the year column with fill=year. This will ensure it uses different fill colors for the two different years. Although not necessary, putting position=\"identity\" inside geom_histogram() helps make the plot a little nicer. Putting color=\"black\" and alpha=.7 inside geom_histogram() also helps distinguish the two histograms. ggplot(life, aes(x=lifeExp, fill=year)) + geom_histogram(binwidth=2, position=&quot;identity&quot;, color=&quot;black&quot;, alpha=.7) + theme_minimal() This plot is still pretty bad though. This method of plotting is better when the histograms are quite distinctive from one another and there isnt much overlap in the distributions. Choosing two colors that contrast more strongly than the default colors can help. Here, we are using hexcodes to pick a gray and a mustard yellow color. We manually define our fill colors using + scale_fill_manual(values = c(\"#999999\", \"#E69F00\")). To change the colors, just change the hexcodes to different ones or the names of colors youd like. Just make sure that you have the same number of colors as groups in your data. Here, we have two groups (1952 and 2007) so we need two colors. Also, notice that it says scale_fill_manual and not scale_color_manual. Because we are dealing with the inside color - this is considered to be a fill in ggplot2 terms. We used fill=year inside aes() so we need to match that with fill when manually choosing colors. ggplot(life, aes(x=lifeExp, fill=year)) + geom_histogram( binwidth=2, position=&quot;identity&quot;, color=&quot;black&quot;, alpha=.7) + theme_minimal() + scale_fill_manual(values = c(&quot;#999999&quot;, &quot;#E69F00&quot;)) Overlaid Density Plots Comparing distributions can also be done with geom_density. This is usually simpler to compare than overlaid histograms. The default plot for this would be to include fill=year inside the aes() code, as the year column contains the data that we wish to make separate plots for. ggplot(life, aes(x=lifeExp, fill=year)) + geom_density(alpha = 0.4) We can add a custom fill colors with + scale_fill_manual(values = c(\"#999999\", \"#E69F00\")) and a custom theme with + theme_classic(). ggplot(life, aes(x=lifeExp, fill=year)) + geom_density(aes(fill = year), alpha = 0.4) + scale_fill_manual(values = c(&quot;#999999&quot;, &quot;#E69F00&quot;)) + theme_classic() This plot is now very easy to interpret. Its clear that in 2007 most countries had life expectancies of over 70, with a tail towards younger life expectancies. In 1952, the opposite pattern is found with most countries having life expectancies around 40 with the tail going towards older countries. 5.2.4 Stem-and-Leaf Plots Stem-and-leaf plots are a simplistic version of histograms. Before the advent of computers, this kind of plot would sometimes be easier to make than a histogram. Their heyday was quite a few decades ago! In fact, nowadays, these types of plots are almost never made by researchers or data scientists in the real world. They are pretty much exclusive to introductory statistics courses. This is a bit of a shame because we think they are pretty cute. Here is an example. Imagine we have the following numbers in a distribution. They may represent temperatures: 20, 20, 23, 28, 29, 31, 32, 39, 40, 41, 42, 44, 44, 45, 48, 49, 55, 55, 56, 58, 59, 61, 62, 65, 66, 67, 70, 71, 75, 82, 86 We can represent these in a stem-and-leaf plot as below. The first column represents the tens and the second column represents the ones. So the 6 in the last row in the second column represents a temperature of 86. We put the second column data in ascending order. The heights of these bars represent a kind of histogram of sorts. The columns do not have to be tens and ones. For instance, if our data had been seconds, and the distribution was 2.0, 2.0, 2.3, 2.8....... 7.5, 8.2, 8.6 we could have done the same stem-and-leaf plot. There isnt a simple ggplot way of making stem-and-leaf plots, but there is a built-in function called stem() that can make them. For an example, if we return to our imdb ratings: head(film) ## # A tibble: 6 x 5 ## film year rottentomatoes imdb metacritic ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Avengers: Age of Ultron 2015 74 7.8 66 ## 2 Cinderella 2015 85 7.1 67 ## 3 Ant-Man 2015 80 7.8 64 ## 4 Do You Believe? 2015 18 5.4 22 ## 5 Hot Tub Time Machine 2 2015 14 5.1 29 ## 6 The Water Diviner 2015 63 7.2 50 We can make a stem-and-leaf plot of the imdb column like this. The scale=0.6 parameter dictates how long the stem-and-leaf plot should be. You can adjust it to your liking. Lower numbers make the plot shorter: stem(film$imdb, scale=0.6) ## ## The decimal point is at the | ## ## 4 | 0234 ## 4 | 6699 ## 5 | 01224444 ## 5 | 555556678999 ## 6 | 0011112333333333444444 ## 6 | 5555666666666777777789999999 ## 7 | 0000111111122222222223333344444444 ## 7 | 555555666777788888888899 ## 8 | 012222344 ## 8 | 6 Here, the lowest rating we have is 4.0, and the highest is 8.6. 5.3 Scatterplots In the introduction to ggplot2, we already demonstrated how to make a scatterplot. Here we will show a few extra features of these plots. Scatterplots plot continuous variables on the x- and y-axes, and can be very useful to examine the association between the two continuous variables. We use them a lot when plotting data related to correlation (see section 12) or regression (see section 13). As we showed earlier, geom_point is used to add datapoints to scatter plots. Well do this for the cheese.csv dataset, that contains nutritional information about various cheeses: cheese &lt;- read_csv(&quot;data/cheese.csv&quot;) head(cheese) ## # A tibble: 6 x 9 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 blue 18.7 0.8 7.78 21.4 2.34 75 0 353 ## 2 brick 18.8 0.784 8.60 23.2 2.79 94 0 371 ## 3 brie 17.4 0.826 8.01 20.8 0.45 100 0 334 ## 4 camembert 15.3 0.724 7.02 19.8 0.46 72 0 300 ## 5 caraway 18.6 0.83 8.28 25.2 3.06 93 0 376 ## 6 cheddar 21.1 0.942 9.39 24.9 1.28 105 0 403 Well start with a simple scatterplot looking at the association between saturated fat on the x-axis and cholesterol on the y-axis intake. ggplot(cheese, aes(x=sat_fat, y=chol) ) + geom_point() We can change the color of the points by adding a color inside of geom_point - making sure that the color name is in quotes: ggplot(cheese, aes(x=sat_fat, y=chol) ) + geom_point(color = &quot;purple&quot;) To add a straight trendline through the data we use + stat_smooth(method = \"lm\"). The stat_smooth bit tells it to add a trendline, and the method=\"lm\" bit in the middle is telling it to make the straight line: ggplot(cheese, aes(x=sat_fat, y=chol) ) + geom_point(color = &quot;purple&quot;) + stat_smooth(method = &quot;lm&quot;) Here you can see it automatically puts a shaded area around your trendline, which represents a confidence interval around the trendline. There is a way to remove it by adding se = FALSE or se = F inside of stat_smooth(): ggplot(cheese, aes(x=sat_fat, y=chol) ) + geom_point(color = &quot;purple&quot;) + stat_smooth(method = &quot;lm&quot;, se = FALSE) You can also change the color of the trendline, by adding to stat_smooth ggplot(cheese, aes(x=sat_fat, y=chol) ) + geom_point(color = &quot;purple&quot;) + stat_smooth(method = &quot;lm&quot;, se= F, color = &quot;black&quot;) As with all ggplot2 graphs, you can customize the plot. For example changing the theme, adding a title and axes titles: ggplot(cheese, aes(x=sat_fat, y=chol) ) + geom_point(color = &quot;purple&quot;) + stat_smooth(method = &quot;lm&quot;, se= F, color = &quot;black&quot;) + xlab(&quot; Saturated Fat&quot;) + ylab(&quot;Cholesterol&quot;) + ggtitle(&quot;Saturated Fat vs Cholesterol&quot;) + theme_minimal() If you wish to change the color of the points based on a grouping variable, then we need to put our color= into the aes(). You then need to provide the column that has the color grouping variable. For example, to change the color of points in our plot of body mass against head size in Blue Jays based on the sex of birds: head(df) ## # A tibble: 6 x 9 ## BirdID KnownSex BillDepth BillWidth BillLength Head Mass Skull Sex ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0000-00000 M 8.26 9.21 25.9 56.6 73.3 30.7 1 ## 2 1142-05901 M 8.54 8.76 25.0 56.4 75.1 31.4 1 ## 3 1142-05905 M 8.39 8.78 26.1 57.3 70.2 31.2 1 ## 4 1142-05907 F 7.78 9.3 23.5 53.8 65.5 30.3 0 ## 5 1142-05909 M 8.71 9.84 25.5 57.3 74.9 31.8 1 ## 6 1142-05911 F 7.28 9.3 22.2 52.2 63.9 30 0 ggplot(df, aes(x=Mass, y=Head, color = KnownSex) ) + geom_point() If you wish to customize the colors of your datapoints, then you need to add scale_color_manual() like this: ggplot(df, aes(x=Mass, y=Head, color = KnownSex) ) + geom_point() + scale_color_manual(values = c(&quot;darkorange&quot;, &quot;steelblue2&quot;)) + theme_classic() If you have a lot of points on your scatterplot, it can get quite hard to see all the datapoints. One way to deal with this is to change the transparency of the points. You can do this by adjusting the alpha level inside of geom_point(). alpha= ranges from 0 to 1, with 0 being fully transparent and 1 being fully solid. ggplot(df, aes(x=Mass, y=Head, color = KnownSex) ) + geom_point(alpha=.4) + scale_color_manual(values = c(&quot;darkorange&quot;, &quot;steelblue2&quot;)) + theme_classic() 5.3.0.1 Multiple Groups on a Scatterplot We can add multiple trendlines to each group of datapoints plotted on a scatterplot. Lets look at the following data of the chemical components of different olive oils produced in Italy. This is what the data look like: olives &lt;- read_csv(&quot;data/olives.csv&quot;) head(olives) ## # A tibble: 6 x 10 ## macro.area region palmitic palmitoleic stearic oleic linoleic linolenic arachidic eicosenoic ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 South Apulia.north 1075 75 226 7823 672 36 60 29 ## 2 South Apulia.north 1088 73 224 7709 781 31 61 29 ## 3 South Apulia.north 911 54 246 8113 549 31 63 29 ## 4 South Apulia.north 966 57 240 7952 619 50 78 35 ## 5 South Apulia.north 1051 67 259 7771 672 50 80 46 ## 6 South Apulia.north 911 49 268 7924 678 51 70 44 If we use table(), we can see how many different regions are represented in the data. There are three unique Italian areas where the olives come from: table(olives$macro.area) ## ## Centre.North Sardinia South ## 151 98 323 Say we are interested in looking at how oleic and linoleic acid contents are related to each other by macro.area: ggplot(olives, aes(x=oleic, y=linoleic, color=macro.area)) + geom_point() + theme_classic() If we wanted to add a trendline for each area, all we need to do is add our stat_smooth(method=\"lm) line to the code. It already knows to plot these as separate trendlines for each group because inside aes() we have color=macro.area. As long as there is a group= or color= inside aes() then it knows to do things like adding trendlines separately for each group: ggplot(olives, aes(x=oleic, y=linoleic, color=macro.area)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=F) + theme_classic() 5.3.1 Bubble Charts Bubble Charts are an extension to scatterplots. In scatterplots we plot two continuous variables against each other. With a bubble chart we add a third continuous variable and vary the size of our datapoints according to this variable. For example, say we wish to also plot skull size on our Blue Jay scatterplot. We could increase the size of the points for individuals with larger skull sizes. We do this by adding size=Skull into our aes() part: ggplot(df, aes(x=Mass, y=Head, color = KnownSex, size = Skull) ) + geom_point(alpha=.4) + scale_color_manual(values = c(&quot;darkorange&quot;, &quot;steelblue2&quot;)) + theme_classic() The issue with bubble charts is that they can start to look very cluttered, making it hard to actually see any patterns. They should probably be used sparingly. One trick you can employ to make them a little easier to see is to add scale_size() to the plot. Here, you enter two numbers to tell it what size points to scale to. In our example below, we used scale_size(range = c(.1, 4)) which makes our points range between sizes 0.1 and 4. This makes the plot a little less busy: ggplot(df, aes(x=Mass, y=Head, color = KnownSex, size = Skull) ) + geom_point(alpha=.4) + scale_color_manual(values = c(&quot;darkorange&quot;, &quot;steelblue2&quot;)) + theme_classic() + scale_size(range = c(.1, 4)) 5.4 Line Graphs Line graphs connect continuous values on the y-axis over time on the x-axis. They are very useful for show patterns of change over time. Lets look at the jennifer.csv dataset: jennifer &lt;- read_csv(&quot;data/jennifer.csv&quot;) head(jennifer) ## # A tibble: 6 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1916 Female Jennifer 5 0.00000461 ## 2 1919 Female Jennifer 6 0.00000511 ## 3 1920 Female Jennifer 7 0.00000563 ## 4 1921 Female Jennifer 5 0.00000391 ## 5 1922 Female Jennifer 7 0.00000561 ## 6 1923 Female Jennifer 9 0.00000719 This dataset shows the number n of children born each year (year) in the United States with the name Jennifer. In 1916 there were five children born with the name Jennifer. In 1917 there were 0. In 1923 there were 9. This dataset goes up to 2017 where there were 1052 children born with the name Jennifer: tail(jennifer) ## # A tibble: 6 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2012 Female Jennifer 1923 0.000993 ## 2 2013 Female Jennifer 1689 0.000878 ## 3 2014 Female Jennifer 1521 0.000779 ## 4 2015 Female Jennifer 1283 0.000660 ## 5 2016 Female Jennifer 1159 0.000601 ## 6 2017 Female Jennifer 1042 0.000556 Therefore, we have a continuous variable (n) and a time variable (year). We can plot these as we would plot a scatterplot by supplying year to our x-axis and n to our y-axis. We could then add datapoints with geom_point() essentially making a scatterplot: ggplot(jennifer, aes(x=year, y=n) ) + geom_point() But, we arent dealing with just a scatterplot. These datapoints can be connected to each other as they are ordered in time. Instead of using geom_point() we can use geom_line() to draw a line instead: ggplot(jennifer, aes(x=year, y=n) ) + geom_line() If you so desired, you could plot both the points and lines together: ggplot(jennifer, aes(x=year, y=n) ) + geom_point() + geom_line() You can adjust the colors of the lines and the points independently by supplying color= inside of each geom: e.g. Changing the color of the line, but not the points: ggplot(jennifer, aes(x=year, y=n) ) + geom_point() + geom_line(color = &quot;purple&quot;) Changing the color of both the points and the line: ggplot(jennifer, aes(x=year, y=n) ) + geom_point(color = &quot;violet&quot;) + geom_line(color = &quot;purple&quot;) You can also change the width of lines by adding lwd= to geom_line(): ggplot(jennifer, aes(x=year, y=n) ) + geom_line(color = &quot;purple&quot;, lwd=2) There are also several different styles of lines. You can change these by adjusting the number you provide to lty= inside of geom_line(). Here are a few examples: ggplot(jennifer, aes(x=year, y=n) ) + geom_line(lty=2) ggplot(jennifer, aes(x=year, y=n) ) + geom_line(lty=3) This illustration shows some of the linetype options: Just a quick reminder: Please only connect datapoints into a line if it is meaningful to do so! This is almost always when your x-axis is some measure of time. 5.4.1 Multiple Line Graphs Often we wish to compare the patterns over time of different groups. We can do that by plotting multiple lines on the same graph. Lets look at this example dataset. jenlinda &lt;- read_csv(&quot;data/jenlinda.csv&quot;) tail(jenlinda) ## # A tibble: 6 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2015 Female Jennifer 1283 0.000660 ## 2 2015 Female Linda 425 0.000218 ## 3 2016 Female Jennifer 1159 0.000601 ## 4 2016 Female Linda 436 0.000226 ## 5 2017 Female Jennifer 1042 0.000556 ## 6 2017 Female Linda 404 0.000215 Here, we have data in long format. We still have our continuous outcome variable of n in one column. We also have year in another column. So we can plot these two against each other. Importantly, we can split our lines based on our grouping variable, which is the name column. In that column we have two different groups - Jennifer and Linda. To plot separate lines based on the name column, we need to add group=name to our aes(). Weve also added some custom labels, titles and a theme. ggplot(jenlinda, aes(x=year, y=n, group=name)) + geom_line()+ xlab(&quot;Year&quot;) + ylab(&quot;Number of Children Born&quot;) + ggtitle(&quot;Popularity of Names Jennifer &amp; Linda in USA&quot;) + theme_minimal() You may notice that both lines are the same color! To make the lines have different colors, we insert color=name into the aes() instead of group=name: ggplot(jenlinda, aes(x=year, y=n, color=name)) + geom_line()+ xlab(&quot;Year&quot;) + ylab(&quot;Number of Children Born&quot;) + ggtitle(&quot;Popularity of Names Jennifer &amp; Linda in USA&quot;) + theme_minimal() Again, we could customize these colors if we did not like them with scale_color_manual() like this: ggplot(jenlinda, aes(x=year, y=n, color=name)) + geom_line()+ xlab(&quot;Year&quot;) + ylab(&quot;Number of Children Born&quot;) + ggtitle(&quot;Popularity of Names Jennifer &amp; Linda in USA&quot;) + theme_classic() + scale_color_manual(values=c(&quot;#ffadf3&quot;, &quot;#800f4f&quot;)) Just insert your favorite colors, and make sure you provide the same number of colors as you have separate groups/lines. 5.5 Comparing Distributions across Groups One of the most important data visualizations that we make is to compare the distribution of data across groups. Here we have a categorical variable on the x-axis, and a continuous variable on the y-axis. For some reason, the most common way to represent these data in most of the scientific literature is to plot bar graphs with error bars - so-called dynamite plots. However, in our very strong opinion these plots are dreadful and you should never use them. Fortunately others agree. Instead, please choose from strip plots, boxplots or violin plots, or a combination, depending upon your data. In this section well use the wheels1.csv dataset. These data show the number of revolutions of a running wheel made by mice over a four day period. The mice vary by their strain (type). Here we just select the id, strain and day4 columns for this example: wheels &lt;- read_csv(&quot;data/wheels1.csv&quot;) wheels1 &lt;- wheels %&gt;% select(id, strain, day4) head(wheels1) ## # A tibble: 6 x 3 ## id strain day4 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 692ao B6 12516. ## 2 656aa B6 7404. ## 3 675ag B6 3761 ## 4 675ai B6 11684 ## 5 656af B6 8468. ## 6 656al B6 9291 The day4 column represents how many wheel revolutions the mice made on their fourth day running in the wheel. Some mice really like running in the wheel, others arent as bothered! Lets have a look at how many datapoints we have in each strain: table(wheels$strain) ## ## B6 F1-129B6 F1-B6129 S129 Swiss ## 14 22 15 16 13 We have 80 mice in five different strains. 5.5.1 Strip Plots Strip plots essentially just plot the raw data. Its like plotting a scatterplot, except we plot a categorical variable on the x-axis. So in our example, inside aes() well put strain on the x-axis with x=strain, and on the y-axis we put our outcome variable with y=day4. Well add datapoints with + geom_point(): ggplot(wheels1, aes(x = strain, y = day4)) + geom_point() + theme_classic() The major issue with this plot is that all the points are in a very straight line, and it can be difficult to distinguish between different points. To change this, instead of using geom_point() we use geom_jitter() which bounces the points around a bit: ggplot(wheels1, aes(x = strain, y = day4)) + geom_jitter() + theme_classic() Whoops! The points exploded. Now its not possible to know which points belong to which group. To constrain this, we can set width= inside of geom_jitter() which tells the points how far they are allowed to bounce around: ggplot(wheels1, aes(x = strain, y = day4)) + geom_jitter(width = .15) + theme_classic() This looks a lot better!. 5.5.2 Boxplots Boxplots are a very useful way of summarizing the distribution of data. The image below summarizes what each line in the boxplot represents. For more details on all of these descriptive measures see section 6: The middle horizontal line is at 6. This represents the median of the distribution which is the middle value. 50% of the distribution lies above this value and 50% below it. The higher horizontal line at the top of the box represents the upper quartile. This is approximately the median of the upper 50% of the data, so is approximately the 75% percentile. The lower horizontal line at the bottom of the box represents the lower quartile. This is approximately the median of the lower 50% of the data, so is approximately the 25% percentile of the data. Therefore, the middle 50% of the data (from the 25% percentile to the 75% percentile) lies inside the box. The long vertical lines represent the range of the data. The top of that line is the maximum value in the data, and the bottom of that line is the minimum value in the distribution. The above is a basic boxplot. However, ggplot2 does things a little bit differently. It turns out there is more than one way to calculate the lower and upper quartiles (see section 6.4.2). Also, R doesnt necessarily extend the vertical lines (whiskers) all the way to the minimum and maximum values. If there are datapoints that are too far away from the upper or lower quartile, then it truncates the whisker and shows datapoints outside of this range as dots. Here is an illustration of a ggplot boxplot: OK, lets have a look at some boxplots using ggplot(). We provide the same x=strain and y=day4 values as we do with strip plots. Instead of geom_jitter() we use geom_boxplot(): ggplot(wheels1, aes(x = strain, y = day4)) + geom_boxplot() + theme_classic() You can see in this example, that the strain F1-129B6 and the strain S129 both have two datapoints that are shown as outliers beyond the whiskers. To change the colors of the boxplots, you can change color= and fill= inside geom_boxplot(). Remember that color refers to the color of the lines, and fill refers to the filled in color of the shape: ggplot(wheels1, aes(x = strain, y = day4)) + geom_boxplot(color=&quot;navy&quot;, fill=&quot;lightsteelblue1&quot;) + theme_classic() You can change the size, color and shape of the outliers. For instance, to remove them completely, we do outlier.shape=NA inside geom_boxplot() ggplot(wheels1, aes(x = strain, y = day4)) + geom_boxplot(color=&quot;navy&quot;, fill=&quot;lightsteelblue1&quot;, outlier.shape = NA) + theme_classic() To change the size and color, you can use outlier.size and outlier.color respectively inside geom_boxplot() ggplot(wheels1, aes(x = strain, y = day4)) + geom_boxplot(color=&quot;navy&quot;, fill=&quot;lightsteelblue1&quot;, outlier.size = .5, outlier.color = &quot;gray66&quot;) + theme_classic() 5.5.2.1 Overlaying points It can often be helpful to overlay your raw datapoints over the top of boxplots, providing that you dont have too much data. To do this, just add your points with either geom_point() or preferably geom_jitter(). But one warning - make sure you remove any outliers with outlier.shape=NA otherwise those datapoints will show up twice: ggplot(wheels1, aes(x = strain, y = day4)) + geom_boxplot(color=&quot;navy&quot;, fill=&quot;lightsteelblue1&quot;, outlier.shape = NA) + geom_jitter(width=0.15, color=&quot;navy&quot;) + theme_classic() Sometimes this can look a bit too busy. One way to contrast things is to set either the points or the boxplots themselves to have some transparency with alpha=. Here we make the points a bit transparent: ggplot(wheels1, aes(x = strain, y = day4)) + geom_boxplot(color=&quot;navy&quot;, fill=&quot;lightsteelblue1&quot;, outlier.shape = NA) + geom_jitter(width=0.15, color=&quot;navy&quot;, alpha = .3) + theme_classic() Here we leave the points solid, but make the boxplot fill transparent: ggplot(wheels1, aes(x = strain, y = day4)) + geom_boxplot(color=&quot;navy&quot;, fill=&quot;lightsteelblue1&quot;, outlier.shape = NA, alpha = .3) + geom_jitter(width=0.15, color=&quot;navy&quot;) + theme_classic() And finally, making both a bit transparent, adding some custom titles and labels. You can just choose what you think looks best! ggplot(wheels1, aes(x = strain, y = day4)) + geom_boxplot(color=&quot;navy&quot;, fill=&quot;lightsteelblue1&quot;, outlier.shape = NA, alpha = .3) + geom_jitter(width=0.15, color=&quot;navy&quot;, alpha=.3) + theme_classic() + xlab(&quot;Mouse Strain&quot;) + ylab(&quot;Total Revolutions&quot;) + ggtitle(&quot;Mouse Wheel Running&quot;) 5.5.2.2 Reordering categorical x-axes The boxplot that we made looks ok, but one thing is visually annoying. The boxes are plotted in alphabetical order on the x-axis (B6, F1-129B6.. Swiss). There is no reason why they should be in this order. A more visually appealing way would be to order the boxplots from the group with the highest median to the lowest median. To do this, instead of putting x=strain inside of aes() we put x = reorder(strain, -day4, median) inside instead. This is a bit of a mouthful. To break it down, its saying plot strain on the x-axis, but reorder the groups based on the median of the strain column (thats the -day4 in the code). ggplot(wheels1, aes(x = reorder(strain, -day4, median), y = day4)) + geom_boxplot(color=&quot;navy&quot;, fill=&quot;lightsteelblue1&quot;, alpha = .3) + theme_classic() The output looks pretty good - it is now really easy to notice that one of the groups has a much lower distribution than the others. The major issue is that the label of the x-axis is terrible. So lets fix that: ggplot(wheels1, aes(x = reorder(strain, -day4, median), y = day4)) + geom_boxplot(color=&quot;navy&quot;, fill=&quot;lightsteelblue1&quot;, alpha = .3) + theme_classic() + xlab(&quot;Mouse Strain&quot;) + ylab(&quot;Total Revolutions&quot;) + ggtitle(&quot;Mouse Wheel Running&quot;) Much nicer! 5.5.2.3 Flipping Axes Often boxplots look perfectly fine with the categorical grouping variable on the x-axis and the continuous variable on the y-axis. If you start to have many groups, then sometimes the boxplots looks too cluttered when placed on the x-axis. In this situation, it might look better to flip the axes, and have the boxplots stacked vertically. To do this, you write your plot code exactly as you would normally, but you just add + coord_flip() to the end of the code. Lets add this to the reordered boxplots we just made in the previous section: ggplot(wheels1, aes(x = reorder(strain, -day4, median), y = day4)) + geom_boxplot(color=&quot;navy&quot;, fill=&quot;lightsteelblue1&quot;, alpha = .3) + theme_classic() + xlab(&quot;Mouse Strain&quot;) + ylab(&quot;Total Revolutions&quot;) + ggtitle(&quot;Mouse Wheel Running&quot;) + coord_flip() This is OK, but it would be nicer if the highest values were at the top. There may well be a more straightforward way of doing this, but a quick solution is to wrap reorder(strain, strain, median) with fct_rev, so you now have x = fct_rev(reorder(strain, strain, median)). Its a whole lot of code, but it does make the graph really pretty, so its worth it: ggplot(wheels1, aes(x = fct_rev(reorder(strain, -day4, median)), y = day4)) + geom_boxplot(color=&quot;navy&quot;, fill=&quot;lightsteelblue1&quot;, alpha = .3) + theme_classic() + xlab(&quot;Mouse Strain&quot;) + ylab(&quot;Total Revolutions&quot;) + ggtitle(&quot;Mouse Wheel Running&quot;) + coord_flip() 5.5.3 Violin Plots A disadvantage of boxplots, especially when you have large distributions, is that the box does not tell you much about the overall shape of the distribution. An alternative are violin plots, where the width of the shape reflects the shape of the distribution. To make these plots, instead of using geom_boxplot() we use geom_violin(). ggplot(wheels1, aes(x = strain, y = day4)) + geom_violin() + theme_classic() You can do all the customizing, reordering, coloring, transparency-ing, etc that you do with boxplots: ggplot(wheels1, aes(x = fct_rev(reorder(strain, strain, median)), y = day4)) + geom_violin(color=&quot;navy&quot;, fill=&quot;lightsteelblue1&quot;, alpha = .3) + theme_classic() + xlab(&quot;Mouse Strain&quot;) + ylab(&quot;Total Revolutions&quot;) + ggtitle(&quot;Mouse Wheel Running&quot;) + coord_flip() 5.5.4 Stacked Boxplots Sometimes, we want to compare distributions for the same group side by side. For instance, we may not just want to plot the day4 wheel running data, but also plot the day1 data. Below, we have data in wide format. We have ids, strain, day1 running and day4 running. wheels2 &lt;- wheels %&gt;% select(id, strain, day1, day4) head(wheels2) ## # A tibble: 6 x 4 ## id strain day1 day4 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 692ao B6 12853 12516. ## 2 656aa B6 2644 7404. ## 3 675ag B6 4004. 3761 ## 4 675ai B6 11754. 11684 ## 5 656af B6 6906. 8468. ## 6 656al B6 6517 9291 We need to turn this to long data to be able to make the stacked boxplot graph. See section 4.6 for more on how to switch between wide and long data formats: wheels2.long &lt;- wheels2 %&gt;% pivot_longer(cols = 3:4, names_to = &quot;day&quot;) wheels2.long ## # A tibble: 160 x 4 ## id strain day value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 692ao B6 day1 12853 ## 2 692ao B6 day4 12516. ## 3 656aa B6 day1 2644 ## 4 656aa B6 day4 7404. ## 5 675ag B6 day1 4004. ## 6 675ag B6 day4 3761 ## 7 675ai B6 day1 11754. ## 8 675ai B6 day4 11684 ## 9 656af B6 day1 6906. ## 10 656af B6 day4 8468. ## # ... with 150 more rows Now the wheel running data is in its own column - value. So we use y=value. The grouping variable is in the day column, so we use fill=day to make separate boxplots based on the day. This will also make them different fill colors: ggplot(wheels2.long, aes(x = strain, y = value, fill=day)) + geom_boxplot() + theme_classic() This looks ok, but the colors are yucky. Lets add custom titles, labels, and well customize the fill colors using scale_fill_manual. We have two groups (day1 and day4) so we need to provide two colors: ggplot(wheels2.long, aes(x = strain, y = value, fill=day)) + geom_boxplot() + theme_classic() + scale_fill_manual(values = c(&quot;#9381e3&quot;, &quot;#faff75&quot;)) + xlab(&quot;Mouse Strain&quot;) + ylab(&quot;Total Revolutions&quot;) + ggtitle(&quot;Mouse Wheel Running&quot;) It turns out all the strains increase their overall running in wheels from day 1 to day 4, except the S129 strain who get bored with wheel running - probably similar to how youre bored of seeing graphs about wheel running. 5.5.5 Ridgeline Plots Another useful way of displaying distributions of data across groups is using ridgeline plots. These are essentially density histogram plots for each categorical group plotted side by side. To do this we need to use a package called ggridges. This can be installed by going to the Packages tab, selecting Install and typing in ggridges in the box. Lets go back to the olives data. Say we are interested in displaying the distribution of oleic acid content by macro.area. We plot the categorical group of interest (here macro.area) on the y-axis, and the continuous variable whose distribution we are interested in (oleic) on the x-axis. We then use stat_density_ridges() to plot the ridgeline plots. library(ggridges) ggplot(olives, aes(x = oleic, y = macro.area)) + stat_density_ridges() + theme_classic() You can add color by adding in a fill= to the aes(). ggplot(olives, aes(x = oleic, y = macro.area, fill = macro.area)) + stat_density_ridges() + theme_classic()  and perhaps we can manually override the default color scheme - here Im using hex codes to pick a very purpley color scheme: ggplot(olives, aes(x = oleic, y = macro.area, fill = macro.area)) + stat_density_ridges() + theme_classic() + scale_fill_manual(values=c(&quot;#D1B8D0&quot;, &quot;#F78EF2&quot;, &quot;#AC33FF&quot;)) A nice thing about these ridgeline plots is that we can easily add on lines that represent the lower quartile, median and upper quartile by adding in the argument quantile_lines = TRUE like this: ggplot(olives, aes(x = oleic, y = macro.area, fill = macro.area)) + stat_density_ridges(quantile_lines = TRUE) + theme_classic() + scale_fill_manual(values=c(&quot;#D1B8D0&quot;, &quot;#F78EF2&quot;, &quot;#AC33FF&quot;)) The final ridgeline plot below plots the distributions of oleic acid by region. There are 9 regions. Its best in these plots to try and plot the categories from highest to lowest median, as it looks nicer. The following code is a bit tricky, and if youre not interested - then you can safely ignore. However, just in case it is of interest to anyone: to do that you need to make sure ggplot recognizes the categorical variable region in this case to be a factor (a grouped variable) and that they are in the right order. It can be done using using this line: fct_reorder(region, -oleic, .fun = median). Essentially this says, make the region variable a factor, and reorder it to be from highest median of oleic acid to lowest. One final thing - you have to do this for both the y axis category, and the fill - otherwise your colors wont match your y-axis categories. In the below code, I also added quantiles, x-axis and y-axis titles, a title and I removed the legend as it didnt add any extra information that isnt already on the plot. ggplot(olives, aes(x = oleic, y = fct_reorder(region, -oleic, .fun = median), fill = fct_reorder(region, -oleic, .fun = median) )) + stat_density_ridges(quantile_lines = TRUE) + theme_classic() + scale_fill_manual(values=c(&quot;#0000FF&quot;, &quot;#2000DF&quot;, &quot;#4000BF&quot;, &quot;#60009F&quot;, &quot;#800080&quot;, &quot;#9F0060&quot;, &quot;#BF0040&quot;, &quot;#DF0020&quot;, &quot;#FF0000&quot;)) + theme(legend.position = &quot;none&quot;) + ylab(&quot;Region&quot;) + xlab(&quot;Oleic Acid Content&quot;) + ggtitle(&quot;Oleic Acid Content of Italian Olives by Region&quot;) For more information about these plots, you can look up the help documentation for this package here. 5.6 Bar Graphs A common form of data that we wish to show are the amounts of different categories. Often these data could be presented in a table format. For instance, the table below shows the total number of number 1 hits by six different artists in the UK. A table is a completely legitimate way to present data. A graphical way of presenting these same data would be to make a bar graph. In these plots we have a categorical grouping variable on the x-axis and a numerical value (either continuous or discrete) on the y-axis. An advantage of bar graphs over tables is that it is often easier to visualize the proportional differences between categories in their values when looking at a bar graph compared to a table. Bar graphs are therefore especially useful when the differences between groups are larger. If we wish to make a bar graph using ggplot() our data may come in two different ways. First, we may already have the totals that we wish to plot - that is our dataset already contains the values that the bar heights will be at. Second, we may not have these counts but need R to calculate them for us. These two different initial data setups require different geoms to create bar graphs. geom_col() Lets first describe the situation when you have a dataset where you have already counted the number that applies to each group. We will use the number1s.csv data which contains the same data as the table above. df1 &lt;- read_csv(&quot;data/number1s.csv&quot;) head(df1) ## # A tibble: 6 x 2 ## name total ## &lt;chr&gt; &lt;dbl&gt; ## 1 Elvis 21 ## 2 The Beatles 17 ## 3 Cliff Richard 14 ## 4 Westlife 14 ## 5 Madonna 13 ## 6 Take That 12 When data look like this and you have one column that is the category (x = name) and one column containing the numerical data y = total, you can use geom_col(). ggplot(df1, aes(x = name, y = total) ) + geom_col() + theme_classic() Notice that the default order is alphabetical. You can reorder by putting reorder around the x-axis column. If you put reorder(name, total) this is telling it to reorder the name variable by their respective increasing values of total: ggplot(df1, aes(x = reorder(name, total), y = total) ) + geom_col() + theme_classic() Alternatively, if you put reorder(name, -total), with the - sign in front of total, this is telling it to reorder the name variable by their respective decreasing values of total: ggplot(df1, aes(x = reorder(name, -total), y = total) ) + geom_col() + theme_classic() To change the color of the bars, you need to put fill= inside geom_col() as we are dealing with filling in a shape: When changing color use fill here because its a shape. ggplot(df1, aes(x = reorder(name, -total), y = total) ) + geom_col(fill = &quot;#32b5ed&quot;) + theme_classic() If you wish to add a different color border around the bars, then you can add color= inside the geom_col(): ggplot(df1, aes(x = reorder(name, -total), y = total) ) + geom_col(fill = &quot;#32b5ed&quot;, color=&quot;#193642&quot;) + theme_classic() And, as per usual, all other customizations are acceptable, including rotating the chart using coord_flip(): ggplot(df1, aes(x = reorder(name, total), y = total) ) + geom_col(fill = &quot;#32b5ed&quot;, color=&quot;#193642&quot;) + xlab(&quot;&quot;) + ylab(&quot;Total Number 1&#39;s&quot;) + ggtitle(&quot;Number 1 hits in UK&quot;) + theme_classic() + coord_flip() In the code for this flipped bar graph, notice that we removed the - from next to -total when reordering. If wed left it in, it would have plotted the bars in the opposite order. If you are unsure with your own data whether to use it or not - just see what happens with and without it. Bar graphs look better when the highest value is at the top. One other key thing about bar graphs, is that they should technically start at 0. As you are visualizing amounts, it would be misleading to start the graph at e.g. 10 in the above example. That would distort the relationship of the length of the bars to each other. Some people extend this rule to all graphs, but this is a misconception. Often we dont need to know where 0 is for boxplots for instance. However, it is generally important to know where 0 is for bar graphs if we wish to compare bars between groups. geom_bar() Often we want to make bar graphs to visualize how many we have of each group, but we dont yet know how many we have! For example, take the following dataset which is found in pets.csv. pets &lt;- read_csv(&quot;data/pets.csv&quot;) head(pets) ## # A tibble: 6 x 2 ## name pet ## &lt;chr&gt; &lt;chr&gt; ## 1 Leon Cat ## 2 Lesley Dog ## 3 Devon Dog ## 4 Timothy Dog ## 5 Paul None ## 6 Jody Cat These data show different individuals in a class in the name column and what their favorite pet is in the pet column. Perhaps we want to visualize which pets are the most popular. Wed like to get the total number of people who put cat as their favorite, the total number of people that put dog down and so on. One quick way to visually inspect how many we have of each pet in the pet column is to use the function table(): table(pets$pet) ## ## Bird Cat Dog None ## 2 6 11 6 To make the bar graph of these data using ggplot(), we need to use geom_bar(). Fortunately, geom_bar() counts how many we have of each for us. We do not need to supply a y column. We just need to supply x=pet to indicate that that column will be our grouping variable. ggplot(pets, aes(x = pet)) + geom_bar() + theme_classic() Once we have the basic plot down, all the other bits and pieces can be done: Then just customize. ggplot(pets, aes(x = pet)) + geom_bar(color=&quot;black&quot;, fill=&quot;plum3&quot;) + theme_classic()+ xlab(&quot;Pet&quot;)+ ylab(&quot;Total&quot;)+ ggtitle(&quot;Popularity of Pets in a Class&quot;) You can also reorder your factor. With geom_bar() we reorder in a similar way to how we did with geom_boxplot(). We use x = reorder(pet, pet, table) to tell it to reorder the pet category according to the frequency count of each as calculated by the table() function. Using coord_flip() makes it easier to read and compare bars. ggplot(pets, aes(x = reorder(pet, pet, table))) + geom_bar(color=&quot;black&quot;, fill=&quot;plum3&quot;) + theme_classic()+ xlab(&quot;Pet&quot;)+ ylab(&quot;Total&quot;)+ ggtitle(&quot;Popularity of Pets in a Class&quot;) + coord_flip() 5.7 Small Multiples Often we want to compare graphs across multiple categories. One good strategy to do this is to make small multiples, which is essentially replicating the same graph for each group several times in different panels. This is probably best explained by doing an example. Scatterplot small multiple Here, we load in the penguins.csv dataset. This data shows the size of various penguins culmen (the beak) and flippers: penguins &lt;- read_csv(&quot;data/penguins.csv&quot;) head(penguins) ## # A tibble: 6 x 7 ## species island culmen_length_mm culmen_depth_mm flipper_length_mm body_mass_g sex ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 MALE ## 2 Adelie Torgersen 39.5 17.4 186 3800 FEMALE ## 3 Adelie Torgersen 40.3 18 195 3250 FEMALE ## 4 Adelie Torgersen 36.7 19.3 193 3450 FEMALE ## 5 Adelie Torgersen 39.3 20.6 190 3650 MALE ## 6 Adelie Torgersen 38.9 17.8 181 3625 FEMALE The dataset contains three different species: table(penguins$species) ## ## Adelie Chinstrap Gentoo ## 146 68 119 We might be interested in examining how body mass is associated with flipper length across species and across sex. Here, we have two different columns containing categorical variables. We have sex and species. If we wanted to show all of this on just one scatterplot, we could change the color of the points to represent species, and the shape of the points to represent sex. We change the shape by a column using shape= inside of aes(): ggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm, color = species, shape = sex)) + geom_point() + theme_classic() The problem with this sort of graph is that it is far too cluttered. Using shape to distinguish categories isnt that useful or helpful. You really have to squint at the graph to work out what is a circle and what is a triangle. An alternative approach is to make small multiples. We create a separate scatterplot for each species. Here, we color our points by sex with color=sex inside aes(). We add to our code the line facet_wrap(~species) to tell ggplot() to make separate scatterplots for each species. Please note the ~ that comes before the column name that you wish to make separate plots for: ggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm, color = sex)) + geom_point() + theme_minimal() + facet_wrap(~ species) You may notice that all the scatterplots have the same range of values on the x-axis. Technically, this is the most appropriate approach as it enables you to make comparisons across groups more easily. However, if you want to fit the data on each scatterplot to cover the whole canvas, you can make the axes unfixed by adding scales=\"free\" to your facet_wrap() command: ggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm, color = sex)) + geom_point() + theme_minimal() + facet_wrap(~ species, scales = &quot;free&quot;) Line graph small multiple We can also make small multiples for line graphs. Lets illustrate this using the lifeexp_all.csv dataset. le &lt;- read_csv(&quot;data/lifeexp_all.csv&quot;) head(le) ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. In this dataset we have a column giving the life expectancy (lifeExp) of various countries that are in the country column. We also have a year column that goes from 1952 to 2007 at five year intervals. Consequently, we could plot a line graph of year on the x-axis and life expectancy on the y-axis. We could make separate lines for each country. As there are far too many countries to plot, it is not worth making each one a separate color. Because of this, rather than putting color=country into aes() to indicate to make separate lines for each country, well put group=country. This will make separate lines for each country, but make them all the same color. If we make them a light color and a bit transparent, it will look best: ggplot(le, aes(x = year, y = lifeExp, group = country)) + geom_line(color=&quot;cornflowerblue&quot;, alpha=0.2) + theme_minimal() This gives us a sense of the overall pattern of life expectancies from 1952 to 2007. The trend for most countries is generally upwards, though there are some countries that have big crashes. We also have another categorical variable in our dataset. There is a column called continent. We could replot this line graph, but separate the plots based on which continent the lines/countries belong to. We do that again using facet_wrap(~continent). ggplot(le, aes(x = year, y = lifeExp, group = country)) + geom_line(color=&quot;cornflowerblue&quot;, alpha=0.5) + theme_minimal() + facet_wrap(~continent) Because there are fewer lines on each graph, we upped the alpha to 0.5 to make the lines a bit darker on this plot. If you wish to make the lines belonging to each panel different colors from each other, you can add color=continent to your aes(). You have to remove the color from geom_line() to make this work: ggplot(le, aes(x = year, y = lifeExp, group = country, color = continent)) + geom_line( alpha=0.5) + theme_minimal() + facet_wrap(~continent)+ xlab(&quot;Year&quot;) + ylab(&quot;Life Expectancy&quot;) 5.8 Saving and Exporting ggplot2 graphs How do we save the nice plots that we have made using ggplot2? There are some quite advanced ways of saving high resolution images. Here, well just run through some quick and easy options. First, you could just hit zoom in RStudio to make your plot bigger. Resize the window to your preferred graph size and then take a screenshot. Paste your screenshot into a program such as paint and crop away. This is a very crude method - but its fast and reliable if you just want to have an image to insert into some other program. A second option is after you have made your plot, you can hit the export tab on the plot viewer. Choose either Save as Image or Save as PDF and then choose how and where you want to save the image. A more premium option is to use a function from ggplot called ggsave(). The first step you should do is to assign your plot to an object name. In the code below, we are making a scatterplot that we save to the object plot1: plot1 &lt;- ggplot(cheese, aes(x = chol, y = kcal)) + geom_point(color=&#39;purple&#39;, size=2) + theme_classic() + xlab(&quot;Cholesterol&quot;) + ylab(&quot;Calories in kcal&quot;) + ggtitle(&quot;Cheese&quot;) plot1 Next, run a line of code that will save your plot. You type ggsave(). The first thing you put inside this is the location where you want your plot to be stored. You need to write a location on your computer. If you are using an Rproject such as with this course, you could put your plot in a folder called img. Remember to type the file extension .png or .pdf after the name of your new plot. The second thing you need to write is the name of the graph object you wish to save. Here our graph is called plot1. ggsave(&quot;img/cheese_plot.png&quot;, plot1) # save as a png ggsave(&quot;img/cheese_plot.pdf&quot;, plot1) # save as a pdf You can also play around with the width and height of your saved image. You probably need to trial and error this a few times to get the proportions that you really like. Here we are making an image that is 10 inches wide and 8 inches high. ggsave(&quot;img/cheese_plot2.png&quot;, plot1, width = 10, height = 8) #(in inches, though can be in cm) "],["descriptives.html", "Chapter 6 Descriptives 6.1 Sample vs Population 6.2 Sample and Population Size 6.3 Central Tendency 6.4 Variation 6.5 Descriptive Statistics in R 6.6 Descriptives for Datasets", " Chapter 6 Descriptives Descriptive statistics describe basic features of the data in simple summaries. Examples include reporting measures of central tendency such as the mean, median, and mode. These are values that represent the most typical or most central point of a data distribution. Another class of descriptives are measures of variability or variation such as variance, standard deviation, ranges or interquartile ranges. These measures describe the spread of data. As well as being useful summaries in their own right, descriptive statistics are also used in data visualization to summarize distributions. There are several functions in R and other packages that help to get descriptive statistics from data. Before we go into detail about how to use R to get descriptives, well describe these measures of central tendency and variation in a bit more detail. 6.1 Sample vs Population The first thing we would like to discuss is the difference between samples and populations. We can calculate descriptive measures such as means and standard deviations for both samples and populations, but we use different notation to describe these. A population is all subjects that we could possibly collect data from. For example, if we were interested in the IQ scores of eighth graders in Texas, then our population of interest is all eighth graders in Texas. If we wished to study maze learning in juvenile rats, then our population of interest would be all juvenile rats. If we were studying leaf growth in sunflowers, then our population of interest is all sunflowers. If we were able to measure the size of leaves on all sunflowers in existence, or measure the maze learning of all juvenile rats in the world, or the IQ of all eighth graders in Texas, then we would have data for the whole population. We would then be able to say something about the mean or median or some other descriptive about the population. Clearly, it is not always possible to measure every subject in a population. Of our three examples, it may just about be possible to measure the IQ of all Texas eighth graders although it would be a lot of work. It seems unlikely to be possible to measure the leaf growth of all sunflowers or the maze learning of all juvenile rats. Instead, what we typically do is to collect data on a subset of subjects. We call this subset a sample. For instance, if we picked 10 sunflowers then we would collect data on just those sunflowers. We may be able to calculate the average leaf size of these 10 sunflowers and use that to estimate what the true leaf size is of all sunflowers. We call the descriptive measures of samples estimates or statistics, whereas the descriptive measures of populations are called parameters. Lets now discuss different descriptive measures in turn. 6.2 Sample and Population Size This isnt strictly a descriptive measure - but it is worth pointing out that the notation for the size of your data is different depending upon whether you are talking about a sample of a population. If you are taking about a sample, then we use the lower case \\(n\\) to refer to the sample size. So, if you see \\(n=10\\) this means that the sample size is 10. e.g. you picked 10 sunflowers to collect data on. If you see the upper case \\(N\\) this refers to the population size. So if you see that the population size is \\(N=1200000\\) this refers to a population size of 1.2 million. 6.3 Central Tendency The first class of descriptives we will explore are measures of central tendency. These can be thought of as values that describe what is the most common, most typical or most average value in a distribution. Also, here we are using the term distribution to refer to a group of numbers or our data. Well use the following dataset as an example. Lets imagine that this is a sample of data: x &lt;- c(1, 14, 12, 5, 3, 6, 11, 15, 9, 5, 4, 2, 7, 5, 3, 8, 11) x ## [1] 1 14 12 5 3 6 11 15 9 5 4 2 7 5 3 8 11 We can calculate the sample size using length(): length(x) ## [1] 17 We can see that the sample size is \\(n=17\\). 6.3.1 Mode The mode or modal value of a distribution is the most frequent or most common value in a distribution. The number that appears the most times in our sample of data above is 5. The mode is therefore 5. In our example, its possible to manually check all the values, but a quicker way to summarize the frequency count of each value in a vector in R is to use table() like this: table(x) ## x ## 1 2 3 4 5 6 7 8 9 11 12 14 15 ## 1 1 2 1 3 1 1 1 1 2 1 1 1 We can see that there are 3 instances of the number 5 making it the mode. There are two instances of 11 and 3, and every other number in the distribution has only 1 instance. Is 5 really the middle of this distribution though? The mode has some serious deficiencies as a measure of central tendency in that although it picks up on the most frequent value, that value isnt necessarily the most central measure. 6.3.2 Median The median value is the middle value of the distribution. It represents the value at which 50% of the data lies above the median, and 50% lies below the data. One way to look at this is to visualize our distribution as a dot plot: We have 17 datapoints, which is an odd number. In this case, we want the number/dot at which half the remaining datapoints (8) are below the median, and half (the other 8) are above the median. You can see in the image, that the median value is therefore 6. This leaves 8 dots below it and 8 dots above it. To do this by hand, we would first order the data, and then work from the outside to the inside of the distribution, crossing off one from each end at a time. The image below shows how were doing that using different colors to show the crossing out: In R, we have a quick shortcut for calculating the median, and its to use the function called median(): median(x) ## [1] 6 If we have an even number of values in our distribution, then we take the average of the middle two values. For example, look at the image below. It has 12 numbers in the distribution, so we take the average of the 6th and 7th number: Once weve crossed out each number going from outside to in, were left with the 6th and 7th numbers being 10 and 15. The average of these numbers is 12.5, so the median is 12.5. We can see that with median(): y &lt;- c(5,7,7,9,9,10,15,16,16,21,21,22) median(y) ## [1] 12.5 6.3.3 Mean The mean, or arithmetic mean is the measure that most people think about when they think of the average value in a dataset or distribution. There are actually various different ways of calculating means, so the one that we will focus on is called the arithmetic mean. This is calculated by adding up all the numbers in a distribution and then dividing by the number of datapoints. You can write this as a formula. For a sample, it looks like this: \\(\\overline{x} = \\frac{\\Sigma{x}}{n}\\) And for a population it looks like this: \\(\\mu = \\frac{\\Sigma{x}}{N}\\) Notice that we use \\(\\overline{x}\\) to denote the mean of a sample, and \\(\\mu\\) to denote the mean of a population. Despite these notation differences, the formula is essentially exactly the same. Lets calculate the arithmetic mean of our distribution x: sum_of_x &lt;- 1 + 14 + 12 + 5 + 3 + 6 + 11 + 15 + 9 + 5 + 4 + 2 + 7 + 5 + 3 + 8 + 11 sum_of_x ## [1] 121 So, here \\(\\Sigma{x}=121\\). That makes the mean: sum_of_x / 17 ## [1] 7.117647 This makes the mean \\(\\overline{x}=7.12\\). The shortcut way of doing this in R, is to use the function mean(): mean(x) ## [1] 7.117647 Well talk more about the pros and cons of the mean and median in future chapters. 6.4 Variation As well as describing the central tendency of data distributions, the other key way in which we should describe a distribution is to summarize the variation in data. This family of measures look at how much spread there is in the data. Another way of thinking about this is that these measure give us a sense of how clumped or how spread out the data are. 6.4.1 Range The simplest measure of spread is the range. This simply is the difference between the minimum and maximum value in a dataset. Looking at our distribution x, the minimum value is 1, and the maximum value is 15 - therefore the range is \\(15-1 = 14\\). The problem with range as a measure can be illustrated by just adjusting our data distribution slightly. Say, instead of having a datapoint at 15, we had a value at 25. Now the range is 24 instead of 14. This suggests that the data is much more spread out, but in reality it is just one datapoint that is forcing the range to be much higher - the rest of the data is no more clumped or spread out. This is the major drawback of the range - it can be easily influenced by outliers - as illustrated below. In R, we can calculate the minimum, maximum and range of a distribution using the functions min(), max() and range(). Although range() just gives us the minimum and maximum values, we have to do the rest: min(x) ## [1] 1 max(x) ## [1] 15 range(x) ## [1] 1 15 6.4.2 Interquartile Range The interquartile range or IQR is another measure of spread. It is roughly equivalent to the range of the middle 50% of the data. One way to think about this is to consider how the median splits the data into a bottom half and a top half. Then, calculate the median of the lower half of the data and the median of the upper half of the data. These values can be considered to be the lower quartile and upper quartile respectively. The interquartile range is the difference between these values. A visualization of this is below: The median of the bottom half of our data is 3.5 (the average of 3 and 4). The median of the top half is 11 (the average of 11 and 11). This makes the IQR equal to \\(11-3.5 = 7.5\\). If we start with an even number of numbers in our distribution, then we include each of the middle numbers in their respective lower and upper halves. The image below represents this: With this distribution, we calculated the median to be 12.5 as the numbers 10 and 15th were the middle two values. Because of this, we include 10 in the bottom half and 15 in the top half. When we work out to in on each of these halves, we find that the median of the bottom half is 8 (the average of 7 and 9) and the median of the upper half is 18.5 (the average of 16 and 21). Therefore, the lower quartile is 8 and the upper quartile is 18.5, making the IQR equal to \\(18.5-8=10.5\\). The above explanation of how to calculate the IQR is actually just one way of trying to estimate the middle 50% of the data. With this way of doing it, the lower quartile represents the 25% percentile of the data (25% of values being lower than it and 75% of values being higher). The upper quartile represents the 75% percentile of the data (75% of values being lower than it and 25% being higher). Unfortunately, there are several ways of calculating the lower and upper quartiles and estimating where these 25% and 75% percentiles are. When we calculate them in R, the default method it uses is actually different to our by hand method. To calculate quartiles, we use the function quantile() (note - not quartile!) but we have to put a second argument to say if we want the lower quartile or upper quartile. quantile(x, 0.25) #0.25 means lower quartile ## 25% ## 4 quantile(x, 0.75) #0.75 means upper quartile ## 75% ## 11 You can see these values are slightly different to our by hand method. The upper quartile of x agrees with our method being 11. By hand we got the lower quartile to be 3.5, but R gives it as 4. This would make the IQR equal to \\(11-4 =7\\). The quick way of getting that in R is to use IQR(): IQR(x) ## [1] 7 We recommend using the R functions to calculate quartiles and interquartile ranges - it is a slightly stronger method than our by-hand method. You can actually do the by-hand method in R by adding type=6 to the functions. There are actually nine different ways of calculating these in R - which is ridiculous! quantile(x, 0.25, type = 6) ## 25% ## 3.5 quantile(x, 0.75, type = 6) ## 75% ## 11 IQR(x, type = 6) ## [1] 7.5 6.4.3 Average Deviation An alternative way of looking at spread is to ask how far from the center of the data distribution (i.e. the mean) is each datapoint on average. Distributions that are highly clumped will have most datapoints very close to the distributions mean. Distributions that are spread out will have several datapoints that are far away from the mean. Look at the two distributions below. Both of them have means of 10. The top distribution (A) however is much more clumped than the bottom distribution (B) which is more spread out. Lets look at these in more detail. Well start with distribution A. We can calculate the difference of each datapoint from the mean (10) like this: A &lt;- c(5,8,8,9,9,10,10,11,12,12,12,14) A - mean(A) ## [1] -5 -2 -2 -1 -1 0 0 1 2 2 2 4 If we add up all of those differences from the mean, then they will equal 0. We can show this like this: sum(A - mean(A)) ## [1] 0 A way to count up all the differences from the mean and to make sure that they count is to make each number positive regardless of its sign. We can do this using abs() in R: abs(A - mean(A)) ## [1] 5 2 2 1 1 0 0 1 2 2 2 4 When we sum all of these values up, we get the total of all the differences from the mean of each datapoint: sum(abs(A - mean(A))) ## [1] 22 We see that this total is 22. In formula notation, we find that \\(\\Sigma|(x - \\overline{\\mu})|\\). Here \\(x\\) represents each datapoint. \\(\\overline{\\mu}\\) represents the population mean. \\(| |\\) represents take the absolute values of, and \\(\\Sigma\\) means sum up. To get the average deviation we simply divide our sum of difference scores by the number of datapoints, which is 12 in this case. The formula for average deviation is: \\(AD = \\frac{\\Sigma|(x - \\overline{\\mu})|}{N}\\) 22/12 ## [1] 1.833333 Our average deviation is therefore 1.83. This can be interpreted as each datapoint being on average 1.83 units away from the mean of 10. Another way to have got the \\(N\\) would have been to use length() which counts the number of datapoints: sum(abs(A - mean(A))) / length(A) ## [1] 1.833333 We could do the same for distribution B. Calculate the sum of all the difference scores, and then divide by the \\(N\\): B &lt;- c(2,3,4,6,8,10,11,12,13,15,17,19) #difference scores B - mean(B) ## [1] -8 -7 -6 -4 -2 0 1 2 3 5 7 9 # absolute difference scores abs(B - mean(B)) ## [1] 8 7 6 4 2 0 1 2 3 5 7 9 # sum of absolute difference scores sum(abs(B - mean(B))) ## [1] 54 # average deviation sum(abs(B - mean(B))) / length(B) ## [1] 4.5 Here, the total sum of differences from the mean is 54. The average deviation is 4.5. This value being higher than 1.83, shows that distribution B is more spread out than distribution A, which makes sense just looking at the dotplots of the data. 6.4.4 Standard Deviation An alternative, and much more common method of calculating the deviation from the mean of the average datapoint is the standard deviation. This is very similar to the absolute deviation, but the method of making the difference scores positive is different. In average deviation, we just ignore the sign of the difference scores and we make everything positive (this is called taking the absolute value). In standard deviation the method used to make these difference scores positive is to square them. Lets look at how this one works for our two distributions. Well start with distribution A again. First step, is again to get the difference scores, by taking each datapoint away from the mean of the distribution: A - mean(A) ## [1] -5 -2 -2 -1 -1 0 0 1 2 2 2 4 Next, we square these difference scores to get positive values: (A - mean(A))^2 ## [1] 25 4 4 1 1 0 0 1 4 4 4 16 Notice that the datapoints that are furthest from the mean get proportionally larger than values that are close to the mean. Squaring has this effect. We need to sum these squared differences to get a measure of how much deviation there is in total - this figure can also be called the Sum of Squares or Sum of Squared Differences: sum((A - mean(A))^2) ## [1] 64 The total of the squared differences is 64. The notation for this is: \\(\\Sigma(x-\\mu)^2\\) To get a sense of the average squared difference, we then divide the total of the squared differences by our \\(N\\): sum((A - mean(A))^2) / 12 ## [1] 5.333333 The average squared difference is 5.33. The notation for this is \\(\\frac{\\Sigma(x-\\mu)^2}{N}\\). This is a useful measure of deviation, but unfortunately it is still in the units of squared differences. To get it back to the original units of the distribution we just square root it and we call this the standard deviation: sqrt(5.333333) ## [1] 2.309401 The standard deviation \\(\\sigma=2.309\\). The notation for this is: \\(\\sigma = \\sqrt{\\frac{\\Sigma(x-\\mu)^2}{N}}\\) We are using \\(\\sigma\\) to represent the population standard deviation. We can do the same thing for our population B. Lets calculate the difference scores, then square them, then add them up, then divide by \\(N\\), and finally square root: # difference scores B - mean(B) ## [1] -8 -7 -6 -4 -2 0 1 2 3 5 7 9 # squared difference scores (B - mean(B))^2 ## [1] 64 49 36 16 4 0 1 4 9 25 49 81 # Sum of squared differences sum((B - mean(B))^2) ## [1] 338 # Average squared difference sum((B - mean(B))^2) / 12 ## [1] 28.16667 # Standard Deviation sqrt(sum((B - mean(B))^2) / 12) ## [1] 5.307228 The population standard deviation \\(\\sigma\\) for population B is 5.31. Again, as this value is higher than 2.31 this suggests that population B is more spread out than population A, because its datapoints are on average further from the mean. 6.4.5 Variance Variance is related to standard deviation. In fact, it is just standard deviation squared. It can be calculated by the formula: \\(\\sigma^2 = \\sigma^2\\) which is a bit ridiculous. Variance is denoted by \\(\\sigma^2\\) and is calculated by squaring the standard deviation \\(\\sigma\\). Its actually the value you get before you do the square root step when calculating standard deviation. Therefore, we can actually say: \\(\\sigma^2 = \\frac{\\Sigma(x-\\mu)^2}{N}\\) 6.4.6 Average versus Standard Deviation So why do we have two methods for calculating the deviation from the mean. We have the average deviation and the standard deviation. One thing you should notice is that the standard deviation is larger than the average deviation. Distribution A had an average deviation of 4.5 and a standard deviation of 5.3. Distribution B had an average deviation of 1.83 and a standard deviation of 2.3. The reason for this is that squaring difference scores leads to larger values than just taking absolute values. So why do we do the squaring thing? The main reason is that it emphasizes datapoints that are further away from the mean, and this can be an important aspect of spread that we need to take account for. Because of that, it is favored to use the standard deviation above the average deviation. 6.4.7 Sample Standard Deviation Something that is often confusing in introductory statistics is that there are two different formulas for calculating the standard deviation. The one we have already introduced above is called the population standard deviation and its formula is: \\(\\sigma = \\sqrt{\\frac{\\Sigma(x-\\mu)^2}{N}}\\) But, we use a different formula when we are calculating the standard deviation for a sample. This is called the sample standard deviation: \\(s = \\sqrt{\\frac{\\Sigma(x-\\mu)^2}{n-1}}\\) Notice two things. First, we use the notation \\(s\\) to indicate a sample standard deviation. Second, instead of dividing by \\(N\\) in the formula, we divide by \\(n-1\\). So, for our example data distribution of A, this is how we would calculate \\(s\\): First, we get the difference scores, by subtracting the mean of the distribution from each score: #difference scores A - mean(A) ## [1] -5 -2 -2 -1 -1 0 0 1 2 2 2 4 Second, we square these difference scores to make them positive and to emphasize larger difference scores: #square the difference scores (A - mean(A))^2 ## [1] 25 4 4 1 1 0 0 1 4 4 4 16 Third, we sum up all the squared difference scores: #sum the squared difference scores sum((A - mean(A))^2) ## [1] 64 Fourth, we divide this sum by \\(n-1\\), which technically gives us the variance: #divide by n-1 to get the variance # the &#39;n&#39; is 12 here (sum((A - mean(A))^2))/(12-1) ## [1] 5.818182 Finally, we square root this value to get the sample standard deviation - a measure of the typical deviation of each datapoint from the mean: #square root to get the SD sqrt((sum((A - mean(A))^2))/(12-1)) ## [1] 2.412091 Here we have manually calculated the sample standard deviation \\(s=2.412\\). Earlier in this chapter we calculated the population standard deviation of this same distribution to be \\(\\sigma=2.309\\). Notice that the sample standard deviation \\(s\\) is larger than the population standard deviation \\(\\sigma\\). This is because \\(n-1\\) will always be smaller than \\(N\\), inflating the final result. So far, we havent shown you the shortcut for calculating the standard deviation in R. Its actually just the function sd(): sd(A) ## [1] 2.412091 Hopefully you notice that the output of sd() is the sample standard deviation and not the population standard deviation. There is actually no built in function for calculating the population standard deviation \\(\\sigma\\) in R. The below code is a custom function to calculate it that we made. Its called pop.sd(). # this is a custom function to calculate # the population standard deviation pop.sd &lt;- function(s) { sqrt(sum((s - mean(s))^2)/length(s)) } When we look at the population standard deviation of A we can see that it matches what we worked out by hand earlier: pop.sd(A) ## [1] 2.309401 Lets look at distribution B for its sample and population standard deviations: sd(B) ## [1] 5.543219 pop.sd(B) ## [1] 5.307228 Again you can see that \\(s=5.543\\) is greater than \\(\\sigma=5.307\\). Both these values are higher than the standard deviations for distribution A, indicating that distribution B is more spread out and less clumped than distribution A. 6.4.8 Sample versus Population Standard Deviation So why are there two formulas, and why do we divide by \\(n-1\\) in the sample standard deviation? The short answer is that whenever we determine the standard deviation for a sample, our goal is technically not to calculate the standard deviation just for that sample. The bigger goal is that we are trying to estimate the population standard deviation \\(\\sigma\\). However, when we use the population SD formula on samples, we consistently underestimate the real population standard deviation. Why is this? Basically its for two reasons. First, within any one sample we typically have much less variation than we do in our population, so we tend to underestimate the true variation. Secondly, when we have our sample we use our sample mean \\(\\overline{x}\\) as an estimate of the population mean \\(\\mu\\). Because \\(\\overline{x}\\) will be usually slightly different from \\(\\mu\\) we will be usually underestimating the true deviation from the mean in the population. The bottom line is this: using the population SD formula for a sample generally gives an underestimate of the true population standard deviation \\(\\sigma\\). The solution is to use a fudge-factor of dividing by \\(n-1\\) which bumps up the standard deviation. This is what we do in the sample standard deviation formula. In the sections below, we are going to visually demonstrate this. Hopefully this helps to show you that dividing by \\(n-1\\) works. Dont worry too much about any code here, the aim isnt for you to learn how to run simulations such as these, but we want you to be able to visually see whats going on. 6.4.8.1 Comparing population and sample means Before we get to why we need a separate formula for the sample standard deviation, lets show you why we dont need a separate formula for the sample mean compared to the population mean. Both of these formulas are essentially the same: Sample mean: \\(\\Large\\overline{x} = \\frac{\\Sigma{x}}{n}\\) Population mean: \\(\\Large \\mu = \\frac{\\Sigma{x}}{N}\\) Lets assume the following data distribution is our population, well call it pop. The following code creates a population of 10000 numbers drawn from a random normal distribution (see section 7.0.3) with a population mean of 8 and population standard deviation of 2. Because were randomly drawing numbers to make our population, the final population wont have a mean and standard deviation that are precisely 8 and 2, but we can calculate what they turn out to be: set.seed(1) # just so we all get the same results pop &lt;- rnorm(10000, mean = 8, sd = 2) #100 random numbers with mean of 8, popSD of 2. We now have our population of size \\(N=10000\\). We can precisely calculate the population mean \\(\\mu\\) and population standard deviation \\(\\sigma\\) of our 10000 numbers using mean() and pop.sd(): mean(pop) ## [1] 7.986926 pop.sd(pop) ## [1] 2.024612 So our population has a mean \\(\\mu=7.99\\) and population standard deviation \\(\\sigma=2.02\\). Lets now start taking samples. Well just choose samples of size \\(n=10\\). We can get samples using sample() in R. Lets look at the sample mean of each sample: #first sample samp1 &lt;- sample(pop, size = 10, replace = T) samp1 ## [1] 3.722781 8.785220 6.045566 11.705993 7.297500 7.399121 11.976971 8.401657 6.306361 7.152555 mean(samp1) ## [1] 7.879373 Here our sample mean \\(\\overline{x}=8.62\\) which is close-ish, but a fair bit above \\(\\mu=7.99\\). Lets do it again: #second sample samp2 &lt;- sample(pop, size = 10, replace = T) samp2 ## [1] 6.142884 7.614448 9.575279 9.072075 4.108463 10.599279 8.224608 6.735289 7.004465 7.791237 mean(samp2) ## [1] 7.686803 Again our value of \\(\\overline{x}=8.10\\) is above \\(\\mu=7.99\\), but this time much closer. Lets do a third sample: #third sample samp3 &lt;- sample(pop, size = 10, replace = T) samp3 ## [1] 8.352818 12.802444 10.304643 6.061141 8.633719 8.028558 7.350359 5.904344 6.875784 11.512439 mean(samp3) ## [1] 8.582625 This time our value of \\(\\overline{x}=7.86\\) is a bit below \\(\\mu=7.99\\). What if we did this thousands and thousands of times? Would our sample mean be more often lower or higher than the population mean \\(\\mu=7.99\\)? This is what the code below is doing - its effectively grabbing a sample of size 10 and then calculating the sample mean, but its doing this 20,000 times. Its storing all the sample means in an object called results.means. Note: you dont need to know how this code works! though do reach out if you are interested. results.means&lt;- vector(&#39;list&#39;,20000) for(i in 1:20000){ samp &lt;- sample(pop, size = 10, replace = T) results.means[[i]] &lt;- mean(samp) } Lets look at 10 of these sample means we just collected: unlist(results.means)[1:10] ## [1] 7.457507 7.804544 8.099342 7.886644 7.642569 8.228794 8.581173 7.417380 7.098994 7.458659 Some are above and some are below \\(\\mu=7.99\\). Lets calculate the mean of all the 20,000 sample means: mean(unlist(results.means)) ## [1] 7.990952 It turns out that the average sample mean that we collect using the formula \\(\\Large \\overline{x} = \\frac{\\Sigma{x}}{n}\\) is 7.99 which is the same as the population mean \\(\\mu\\). What this means is that this formula is perfectly fine to use to estimate the population mean. It is what we call an unbiased estimator. Over the long run, it gives us a very good estimate of the population mean \\(\\mu\\). Here is a histogram of our sample means from our 20,000 samples: The vertical solid black line represents \\(\\mu=7.99\\). This histogram is centered on this value, showing that our sample mean formula is unbiased in estimating the population mean - overall, it isnt under- or over-estimating the population mean. As a side note - what we just did in the exercise above was to calculate a sampling distribution of sample means - something well discuss much more in section 7.2. 6.4.8.2 Sample standard deviation as an unbiased estimator Lets do something similar with our two formulas for calculating standard deviation. Well take samples of size \\(n=10\\) and use the sample standard deviation \\(s\\) and the population standard deviation \\(\\sigma\\) formulas to estimate the true \\(\\sigma=2.02\\). #first sample sd(samp1) ## [1] 2.510835 pop.sd(samp1) ## [1] 2.381987 With the first sample, both estimates are lower than \\(\\sigma=2.02\\), although the sample standard deviation is a bit closer to \\(\\sigma=2.02\\). #second sample sd(samp2) ## [1] 1.850897 pop.sd(samp2) ## [1] 1.755915 With the second sample of 10, both estimates are even lower than \\(\\sigma=2.02\\). Again, the sample standard deviation formula produces a result that is closer to 2.02 than does the population deviation formula. What if we did this for 20,000 samples of size 10? Well save the estimates using the sample SD formula in the object results.samp.sd and the estimates using the population SD formula in results.pop.sd. Again, dont worry about the code here - just focus on the output: results.samp.sd&lt;- vector(&#39;list&#39;,20000) results.pop.sd&lt;- vector(&#39;list&#39;,20000) for(i in 1:20000){ samp &lt;- sample(pop, size = 10, replace = T) results.samp.sd[[i]] &lt;- sd(samp) results.pop.sd[[i]] &lt;- pop.sd(samp) } We can work out the average estimate of the standard deviation across all 20,000 samples: mean(unlist(results.samp.sd)) ## [1] 1.967842 mean(unlist(results.pop.sd)) ## [1] 1.866859 So, over 20,000 samples both formulas actually overall underestimate the true population standard deviation of \\(\\sigma=2.02\\), however, the sample standard deviation formula is closer with its average being 1.97 compared to the population standard deviations formula being at 1.87. We can graph this like this: This visualization shows us a few things. First, over all 20,000 samples, some of our estimates of the true standard deviation \\(\\sigma\\) are higher and some are lower regardless of which formula we use. However, when we use the population formula (dividing by \\(N\\)), we have far more samples with estimates of the standard deviation \\(\\sigma\\) which are too low. The distribution is clearly not symmetrical. If we consider the right histogram, when we use the sample SD formula (dividing by \\(n-1\\)), we correct this by and large. This histogram is closer to symmetrical, and we are not underestimating the true population standard deviation nearly as much. In this way, we called the sample standard deviation \\(s\\) an unbiased estimator. If we were to take larger sample sizes, then our estimates of the population standard deviation \\(\\sigma\\), would get better and better when using the sample standard deviation formula. 6.5 Descriptive Statistics in R The above sections interweaved some theory with how to get descriptive information using R. In this section well summarize how to get descriptive summaries from real data in R. The dataset that well use is a years worth of temperature data from Austin, TX. atx &lt;- read_csv(&quot;data/austin_weather.csv&quot;) head(atx) # first 6 rows ## # A tibble: 6 x 4 ## month day year temp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 2019 43.3 ## 2 1 2 2019 39.4 ## 3 1 3 2019 41.2 ## 4 1 4 2019 44.1 ## 5 1 5 2019 48.6 ## 6 1 6 2019 48.8 The temp column shows the average temperature for that day of the year in 2019. Here is a histogram showing the distribution. It is often hot in Texas. ggplot(atx, aes(x= temp)) + geom_histogram(color=&quot;black&quot;, fill=&quot;lightseagreen&quot;, binwidth = 2)+ theme_classic()+ xlab(&quot;Average temperature&quot;) Basic Descriptives Here is a list of some the basic descriptive commands such as calculating the \\(n\\), the minimum, maximum and range. We apply each function to the whole column of data atx$temp, i.e. all the numbers of the distribution: length(atx$temp) # length this tells you the &#39;n&#39; ## [1] 365 range(atx$temp) # range ## [1] 34.5 89.2 min(atx$temp) # minimum ## [1] 34.5 max(atx$temp) # maximum ## [1] 89.2 Mean, Median, and Mode These mean and median are straightforward in R: mean(atx$temp) # mean ## [1] 68.78767 median(atx$temp) # median ## [1] 70.8 For some descriptives, like mode, there is not a function already built into R. One option is to use table() to get frequencies - but this isnt useful when you have relatively large datasets. The output is too large. Another option is to use tidyverse methods. Here, we use group_by() to get each temperature, then we use count() to count how many of each temperature we have, and then arrange() to determine which is most frequent: atx %&gt;% group_by(temp) %&gt;% count() %&gt;% arrange(-n) ## # A tibble: 248 x 2 ## # Groups: temp [248] ## temp n ## &lt;dbl&gt; &lt;int&gt; ## 1 84.8 5 ## 2 51.6 4 ## 3 75.5 4 ## 4 83.7 4 ## 5 84.2 4 ## 6 84.3 4 ## 7 84.9 4 ## 8 86.1 4 ## 9 42.3 3 ## 10 52 3 ## # ... with 238 more rows This shows us that the modal value is 84.8F. In reality however, the mode is never something that you will calculate outside of an introductory stats class. Variation The default standard variation measure in R is the sample standard deviation sd(), and is the one you should pretty much always use: sd(atx$temp) # sample standard deviation ## [1] 14.90662 Variance can also be calculated using var() - remember this is the standard deviation squared. When you calculate this using the sample standard deviation \\(s\\) the formula notation for the variance is \\(s^2\\): var(atx$temp) # variance ## [1] 222.2072 The lower quartile, upper quartile and inter-quartile range can be calculated like this: quantile(atx$temp, .25) # this is the lower quartile ## 25% ## 56.5 quantile(atx$temp, .75) # this is the upper quartile ## 75% ## 83.3 IQR(atx$temp) # this is the inter-quartile range. ## [1] 26.8 Remember there are several ways of calculating the quartiles (see above). 6.5.1 Dealing with Missing Data Often in datasets we have missing data. In R, missing data in our dataframes or vectors is represented by NA or sometimes NaN. A slightly annoying feature of many of the descriptive summary functions is that they do not work if there is missing data. Heres an illustration. Weve created a vector of data called q that has some numbers but also a missing piece of data: q &lt;- c(5, 10, 8, 3, NA, 7, 1, 2) q ## [1] 5 10 8 3 NA 7 1 2 If we try and calculate some descriptives, R will not like it: mean(q) ## [1] NA sd(q) ## [1] NA range(q) ## [1] NA NA median(q) ## [1] NA What we have to do in these situations is to override the missing data. We need to tell it that we really do want to get these values and it should remove the missing data before doing that. We do that by adding the argument na.rm=T to the end of each function: mean(q, na.rm=T) ## [1] 5.142857 sd(q, na.rm=T) ## [1] 3.338092 range(q, na.rm=T) ## [1] 1 10 median(q, na.rm=T) ## [1] 5 Now R is happy to do what we want. The only gotcha that you need to watch out for is length() which we sometimes use to calculate the \\(n\\) of a vector. If we do this for q, well get 8, which includes our missing value: length(q) ## [1] 8 This is a way of getting around that - it looks odd, so weve just put it here for reference. Its not necessary for you to remember this. Its essentially asking what is the length of q when you dont include the NA: length(q[!is.na(q)]) ## [1] 7 6.6 Descriptives for Datasets Often in studies, we are interested in many different outcome variables at once. We are also interested in how groups differ in various descriptive statistics. The following code will show you how to get descriptive statistics for several columns. In the next section well discuss getting descriptives for different groups from data. First read in these data that are looking at various sales of different video games. vg &lt;- read_csv(&quot;data/videogames.csv&quot;) head(vg) ## # A tibble: 6 x 12 ## name platform year genre publisher NA_sales EU_sales JP_sales global_sales critic user rating ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Wii Sports Wii 2006 Sports Nintendo 41.4 29.0 3.77 82.5 76 8 E ## 2 Mario Kar~ Wii 2008 Racing Nintendo 15.7 12.8 3.79 35.6 82 8.3 E ## 3 Wii Sport~ Wii 2009 Sports Nintendo 15.6 11.0 3.28 32.8 80 8 E ## 4 Wii Fit Wii 2007 Sports Nintendo 8.92 8.03 3.6 22.7 80 7.7 E ## 5 Wii Fit P~ Wii 2009 Sports Nintendo 9.01 8.49 2.53 21.8 80 7.4 E ## 6 Grand The~ PS3 2013 Action Take-Two In~ 7.02 9.14 0.98 21.1 97 8.2 M One way to get quick summary information is to use the R function summary() like this: summary(vg) ## name platform year genre publisher ## Length:2502 Length:2502 Min. :1992 Length:2502 Length:2502 ## Class :character Class :character 1st Qu.:2005 Class :character Class :character ## Mode :character Mode :character Median :2008 Mode :character Mode :character ## Mean :2008 ## 3rd Qu.:2011 ## Max. :2016 ## NA_sales EU_sales JP_sales global_sales critic user ## Min. : 0.0000 Min. : 0.0000 Min. :0.00000 Min. : 0.0100 Min. :13.00 Min. :0.700 ## 1st Qu.: 0.0700 1st Qu.: 0.0200 1st Qu.:0.00000 1st Qu.: 0.1500 1st Qu.:61.00 1st Qu.:6.200 ## Median : 0.1800 Median : 0.1000 Median :0.00000 Median : 0.3800 Median :72.00 Median :7.400 ## Mean : 0.4852 Mean : 0.3012 Mean :0.04023 Mean : 0.9463 Mean :69.98 Mean :7.027 ## 3rd Qu.: 0.4675 3rd Qu.: 0.2800 3rd Qu.:0.01000 3rd Qu.: 0.9275 3rd Qu.:81.00 3rd Qu.:8.100 ## Max. :41.3600 Max. :28.9600 Max. :3.79000 Max. :82.5400 Max. :98.00 Max. :9.300 ## rating ## Length:2502 ## Class :character ## Mode :character ## ## ## Youll notice here that it just gives you some summary information for different columns, even those that have no numerical data in them. Its also not broken down by groups. However, summary() can be a quick way to get some summary information. A slightly better function is Describe() in the psych package. Remember to install the psych package before using it. Also, here we are telling it only to provide summaries of the relevant numeric columns (which are the 6th through 11th columns): library(psych) describe(vg[c(6:11)]) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## NA_sales 1 2502 0.49 1.29 0.18 0.27 0.21 0.00 41.36 41.36 15.79 422.23 0.03 ## EU_sales 2 2502 0.30 0.90 0.10 0.15 0.13 0.00 28.96 28.96 16.63 444.50 0.02 ## JP_sales 3 2502 0.04 0.20 0.00 0.01 0.00 0.00 3.79 3.79 12.05 191.56 0.00 ## global_sales 4 2502 0.95 2.56 0.38 0.53 0.42 0.01 82.54 82.53 16.52 441.74 0.05 ## critic 5 2502 69.98 14.34 72.00 71.03 14.83 13.00 98.00 85.00 -0.67 0.07 0.29 ## user 6 2502 7.03 1.44 7.40 7.19 1.33 0.70 9.30 8.60 -1.06 1.06 0.03 This function also includes some descriptives that we dont necessarily need to worry about right now, but it does contain most of the ones we are concerned with. 6.6.1 Descriptives for Groups There are a few ways of getting descriptives for different groups. In our videogame dataset vg, we have a column called genre. We can use the function table() to get the \\(n\\) for all groups. table(vg$genre) ## ## Action Racing Shooter Sports ## 997 349 583 573 We have four different groups of genres, and we might want to get descriptives for each. We can use the function describeBy() from the psych package to get a very quick and easy, but a bit annoying, look at group summaries. It also ignores missing data which is helpful. We dictate which group to get summaries by using the group = \"genre\" argument: describeBy(vg[c(4,6:11)], group = &quot;genre&quot;) ## ## Descriptive statistics by group ## genre: Action ## vars n mean sd median trimmed mad min max range skew kurtosis se ## genre* 1 997 1.00 0.00 1.00 1.00 0.00 1.00 1.00 0.00 NaN NaN 0.00 ## NA_sales 2 997 0.41 0.82 0.17 0.24 0.19 0.00 9.66 9.66 6.19 52.60 0.03 ## EU_sales 3 997 0.27 0.58 0.10 0.15 0.13 0.00 9.14 9.14 7.14 77.32 0.02 ## JP_sales 4 997 0.05 0.14 0.00 0.01 0.00 0.00 1.13 1.13 4.44 22.38 0.00 ## global_sales 5 997 0.83 1.67 0.36 0.50 0.39 0.01 21.12 21.11 6.66 61.52 0.05 ## critic 6 997 68.02 14.21 70.00 68.63 14.83 20.00 98.00 78.00 -0.38 -0.31 0.45 ## user 7 997 7.12 1.34 7.40 7.26 1.19 1.70 9.30 7.60 -1.04 1.05 0.04 ## -------------------------------------------------------------------------------- ## genre: Racing ## vars n mean sd median trimmed mad min max range skew kurtosis se ## genre* 1 349 1.00 0.00 1.00 1.00 0.00 1.00 1.00 0.00 NaN NaN 0.00 ## NA_sales 2 349 0.40 1.03 0.13 0.22 0.18 0.00 15.68 15.68 10.36 140.24 0.06 ## EU_sales 3 349 0.31 0.85 0.10 0.17 0.13 0.00 12.80 12.80 10.23 135.03 0.05 ## JP_sales 4 349 0.03 0.24 0.00 0.00 0.00 0.00 3.79 3.79 12.65 180.01 0.01 ## global_sales 5 349 0.86 2.36 0.30 0.48 0.36 0.01 35.57 35.56 10.34 136.42 0.13 ## critic 6 349 69.84 14.01 72.00 71.18 14.83 13.00 95.00 82.00 -0.92 0.87 0.75 ## user 7 349 6.99 1.50 7.30 7.16 1.48 1.00 9.20 8.20 -1.05 1.06 0.08 ## -------------------------------------------------------------------------------- ## genre: Shooter ## vars n mean sd median trimmed mad min max range skew kurtosis se ## genre* 1 583 1.00 0.00 1.00 1.00 0.00 1.00 1.00 0.00 NaN NaN 0.00 ## NA_sales 2 583 0.56 1.22 0.16 0.27 0.21 0.00 9.73 9.73 4.41 22.41 0.05 ## EU_sales 3 583 0.33 0.67 0.10 0.18 0.13 0.00 5.73 5.73 4.46 24.55 0.03 ## JP_sales 4 583 0.02 0.07 0.00 0.01 0.00 0.00 0.88 0.88 6.43 52.31 0.00 ## global_sales 5 583 1.02 2.09 0.35 0.54 0.43 0.01 14.77 14.76 4.20 19.81 0.09 ## critic 6 583 70.49 15.12 73.00 71.71 14.83 27.00 96.00 69.00 -0.68 -0.18 0.63 ## user 7 583 6.95 1.54 7.30 7.14 1.33 1.20 9.30 8.10 -1.14 1.08 0.06 ## -------------------------------------------------------------------------------- ## genre: Sports ## vars n mean sd median trimmed mad min max range skew kurtosis se ## genre* 1 573 1.00 0.00 1.00 1.00 0.00 1.00 1.00 0.00 NaN NaN 0.00 ## NA_sales 2 573 0.60 1.99 0.27 0.35 0.28 0.00 41.36 41.36 16.15 312.92 0.08 ## EU_sales 3 573 0.33 1.44 0.08 0.13 0.12 0.00 28.96 28.96 15.26 282.22 0.06 ## JP_sales 4 573 0.05 0.30 0.00 0.00 0.00 0.00 3.77 3.77 9.54 100.35 0.01 ## global_sales 5 573 1.11 4.00 0.50 0.64 0.50 0.01 82.54 82.53 16.17 307.46 0.17 ## critic 6 573 72.95 13.44 76.00 74.42 10.38 19.00 97.00 78.00 -1.10 1.26 0.56 ## user 7 573 6.97 1.46 7.30 7.12 1.33 0.70 9.20 8.50 -0.92 0.68 0.06 The above is a quick and dirty way of getting summary information by group. But it is messy. We suggest an alternative method which is to write code using the tidyverse package. This can give us descriptive statistics in a more organized way. For instance, if we wanted to get the mean of the column NA_sales by genre we would use group_by() and summarise() in this way: vg %&gt;% group_by(genre) %&gt;% summarise(meanNA = mean(NA_sales)) ## # A tibble: 4 x 2 ## genre meanNA ## &lt;chr&gt; &lt;dbl&gt; ## 1 Action 0.407 ## 2 Racing 0.397 ## 3 Shooter 0.555 ## 4 Sports 0.603 The above code can be read as taking the dataset vg, and then grouping it by the column genre, and then summarizing the data to get the mean of the NA_sales column by group/genre. Please not the British spelling of summarise(). The tidyverse was originally written using British spelling, and although R is usually fine with British or US spelling, this is one situation in which it is usually helpful to stick with the British spelling for boring reasons. If you had missing data, youd do it like this. vg %&gt;% group_by(genre) %&gt;% summarise(meanNA = mean(NA_sales, na.rm = T)) ## # A tibble: 4 x 2 ## genre meanNA ## &lt;chr&gt; &lt;dbl&gt; ## 1 Action 0.407 ## 2 Racing 0.397 ## 3 Shooter 0.555 ## 4 Sports 0.603 You can do several summaries at once like this. Here we are getting the means and sample standard deviations of the NA_sales and EU_sales columns by genre: vg %&gt;% group_by(genre) %&gt;% summarise(meanNA = mean(NA_sales), sd_NA = sd(NA_sales), meanEU = mean(EU_sales), sd_EU = sd(EU_sales)) ## # A tibble: 4 x 5 ## genre meanNA sd_NA meanEU sd_EU ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Action 0.407 0.819 0.267 0.577 ## 2 Racing 0.397 1.03 0.309 0.852 ## 3 Shooter 0.555 1.22 0.331 0.667 ## 4 Sports 0.603 1.99 0.326 1.44 To save time, you can tell it to just get the summary of all numeric columns by using summarise_if(). vg$year &lt;- as.factor(vg$year) # just need to make year non-numeric first so doesn&#39;t get included in the numeric columns vg %&gt;% group_by(genre) %&gt;% summarise_if(is.numeric, mean, na.rm = T) %&gt;% as.data.frame() ## genre NA_sales EU_sales JP_sales global_sales critic user ## 1 Action 0.4071013 0.2668004 0.04857573 0.8349649 68.01605 7.117954 ## 2 Racing 0.3967908 0.3085100 0.03246418 0.8630946 69.84241 6.991691 ## 3 Shooter 0.5554717 0.3310292 0.02288165 1.0245969 70.48714 6.948885 ## 4 Sports 0.6034031 0.3260733 0.04808028 1.1108028 72.94764 6.970157 vg %&gt;% group_by(genre) %&gt;% summarise_if(is.numeric, sd, na.rm = TRUE) %&gt;% as.data.frame() ## genre NA_sales EU_sales JP_sales global_sales critic user ## 1 Action 0.8189867 0.5772930 0.14425469 1.670552 14.20560 1.339989 ## 2 Racing 1.0315188 0.8522769 0.24017403 2.363348 14.00640 1.497225 ## 3 Shooter 1.2165479 0.6674269 0.07343275 2.087551 15.11614 1.540249 ## 4 Sports 1.9868151 1.4352370 0.30421425 3.996394 13.44039 1.463734 6.6.2 Counts by Group Another common use of group_by() is to get counts of how many we have of each categorical variable. For instance, lets look more at the videogames dataset vg. We have previously seen that we can use table() to count simple frequencies. For instance, the following: table(vg$genre) ## ## Action Racing Shooter Sports ## 997 349 583 573 shows us how many observations of each genre we have. We have 997 Action games, 349 Racing games, 583 shooter games and 573 sports games. We can look at how these breakdown by platform by adding one more argument into our table() function which relates to our second column of interest: table(vg$genre, vg$platform) ## ## PC PS2 PS3 Wii X360 ## Action 144 247 237 127 242 ## Racing 48 135 63 30 73 ## Shooter 144 128 123 30 158 ## Sports 32 203 116 79 143 We can see here that we have 32 Sports games on the PC, 135 racing games on the PS2 and so on. This is a nice and straightforward way of doing this. Its also possible to do it using the tidyverse() which can come in handy sometimes in some circumstances. To do it this way, we make use of group_by() and count(). We tell it the two columns we wish to group our data by (in this case it is the genre and the platform columns), and then tell it to count how many observations we have: vg %&gt;% group_by(genre, platform) %&gt;% count() ## # A tibble: 20 x 3 ## # Groups: genre, platform [20] ## genre platform n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Action PC 144 ## 2 Action PS2 247 ## 3 Action PS3 237 ## 4 Action Wii 127 ## 5 Action X360 242 ## 6 Racing PC 48 ## 7 Racing PS2 135 ## 8 Racing PS3 63 ## 9 Racing Wii 30 ## 10 Racing X360 73 ## 11 Shooter PC 144 ## 12 Shooter PS2 128 ## 13 Shooter PS3 123 ## 14 Shooter Wii 30 ## 15 Shooter X360 158 ## 16 Sports PC 32 ## 17 Sports PS2 203 ## 18 Sports PS3 116 ## 19 Sports Wii 79 ## 20 Sports X360 143 These data are presented in a slightly different way. The count of each combination is shown in the new n column. The nice thing about this tidy approach is that we can further manipulate the data. This is better illustrated with an even busier dataset, the catcolor.csv dataset: cats &lt;- read_csv(&quot;data/catcolor.csv&quot;) head(cats) ## # A tibble: 6 x 7 ## animal_id monthyear name color1 color2 sex breed ## &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 A685067 2014-08-14 18:45:00 Lucy blue white Female domestic shorthair ## 2 A678580 2014-06-29 17:45:00 Frida white black Female domestic shorthair ## 3 A675405 2014-03-28 14:55:00 Stella Luna black white Female domestic mediumhair ## 4 A684460 2014-08-13 15:04:00 Elsa brown &lt;NA&gt; Female domestic shorthair ## 5 A686497 2014-08-31 15:45:00 Chester black &lt;NA&gt; Male domestic shorthair ## 6 A687965 2014-10-31 18:29:00 Oliver orange &lt;NA&gt; Male domestic shorthair Say we want to know how many male and female cats of each breed we have. With tidyverse, we would do it like this: cats %&gt;% group_by(breed,sex) %&gt;% count() ## # A tibble: 87 x 3 ## # Groups: breed, sex [87] ## breed sex n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 abyssinian Female 2 ## 2 abyssinian Male 1 ## 3 american curl shorthair Female 4 ## 4 american curl shorthair Male 1 ## 5 american shorthair Female 28 ## 6 american shorthair Male 44 ## 7 angora Female 4 ## 8 angora Male 2 ## 9 angora/persian Male 1 ## 10 balinese Female 3 ## # ... with 77 more rows This gives us a lot of information. In fact, we have 87 rows of data. However, we could next sort by the newly created n column, to see which sex/breed combination we have the highest amount of. We can use arrange() to do this: cats %&gt;% group_by(breed,sex) %&gt;% count() %&gt;% arrange(-n) ## # A tibble: 87 x 3 ## # Groups: breed, sex [87] ## breed sex n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 domestic shorthair Male 6303 ## 2 domestic shorthair Female 4757 ## 3 domestic mediumhair Male 702 ## 4 domestic mediumhair Female 512 ## 5 domestic longhair Male 381 ## 6 domestic longhair Female 328 ## 7 siamese Male 214 ## 8 siamese Female 135 ## 9 maine coon Male 54 ## 10 snowshoe Male 45 ## # ... with 77 more rows Another thing we can do is to count how many there are of a given category or categories that satisfy certain conditions. For example, lets say we wanted to know the most popular name of each breed for orange cats. We could first filter the data by color1 to only keep orange cats, then group by name and breed and then use count() and arrange(): cats %&gt;% filter(color1 == &quot;orange&quot;) %&gt;% group_by(name,breed) %&gt;% count() %&gt;% arrange(-n) ## # A tibble: 1,304 x 3 ## # Groups: name, breed [1,304] ## name breed n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Oliver domestic shorthair 16 ## 2 Oscar domestic shorthair 12 ## 3 Ginger domestic shorthair 11 ## 4 Sam domestic shorthair 11 ## 5 Garfield domestic shorthair 10 ## 6 Simba domestic shorthair 10 ## 7 Tiger domestic shorthair 10 ## 8 Toby domestic shorthair 10 ## 9 Charlie domestic shorthair 9 ## 10 Milo domestic shorthair 9 ## # ... with 1,294 more rows It turns out that the most popular names overall for orange cats are for domestic shorthairs who are called Oliver, then Oscar, Ginger, Sam, Garfield and so on. To do exactly what we said above, we can do something a bit different. After weve done all the above, we can then tell the chain that we only want to group by breed this time, and we want to keep the highest value with top_n(1). This returns the following: cats %&gt;% filter(color1 == &quot;orange&quot;) %&gt;% group_by(name,breed) %&gt;% count() %&gt;% arrange(-n) %&gt;% group_by(breed) %&gt;% top_n(1) ## # A tibble: 23 x 3 ## # Groups: breed [11] ## name breed n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Oliver domestic shorthair 16 ## 2 Boris domestic mediumhair 5 ## 3 Pumpkin domestic mediumhair 5 ## 4 Charlie domestic longhair 3 ## 5 Gilbert domestic longhair 3 ## 6 Pumpkin domestic longhair 3 ## 7 Amos american shorthair 2 ## 8 Roxy cymric 2 ## 9 Alise manx 1 ## 10 Ami british shorthair 1 ## # ... with 13 more rows Charlie, Gilbert and Pumpkin are all the most common names for orange domestic longhairs! "],["distributions.html", "Chapter 7 Distributions 7.1 Z-scores 7.2 What is a Sampling Distribution ? 7.3 Central Limit Theorem 7.4 Sampling distribution problems 7.5 The t-distribution", " Chapter 7 Distributions In statistical terms a distribution refers to the range of possible values that can come from a sample space. Another way of stating that is to say that a distribution represents how the probabilities of getting various values are distributed. There are several classic distributions in statistics. There are distributions such as the normal, t, Poisson, bimodal etc. Were going to dig a bit deeper into distributions, in particular the normal distribution. The best way to look at distributions is to plot histograms. Lets look at some distributions. 7.0.1 Uniform Distribution The first distribution well look at is the uniform. A uniform distribution is one where theres an equal probability of getting each value from the distribution. In the example below, weve grabbed 1,000,000 numbers from a uniform distribution that starts at 0 and ends at 100. Lets look at its shape: As you can see, the histogram that we have generated is roughly flat across the top. This means that we have equal frequency counts in each bin. Each bin here is 5 across, so the first bin is 0-5, the next bin is 5-10, and so on. We have 25 bins in total in this histogram, and each has roughly 50,000 values in it. This is the classic shape of the uniform distribution. 7.0.2 Bimodal Distribution Another family of distributions that is worth our attention are bimodal distributions. In these distributions we have two peaks in the distribution. You can see an example below: 7.0.3 Normal Distribution In most domains, the type of histograms i.e. distributions, that we most commonly observe dont have 0 peaks like the uniform distribution or 2 peaks like the bimodal distribution, but have just one peak. These are called unimodal distributions. One such classic distribution that is very important to statistics is the normal distribution. The normal distribution has one peak and is symmetrical, with the same proportion of data on each side of the distribution. In addition, we say that a normal distribution has a skewness of 0 and a kurtosis of 3. Well talk about what those are a little bit more about what that means very shortly. Lets look at a normal distribution. The following normal distribution has a mean of 100 and a standard deviation of 5. We can generate it by collecting 1,000,000 datapoints in R: set.seed(1) x &lt;- rnorm(n = 1000000, mean = 100, sd = 5.0) mean(x) ## [1] 100.0002 sd(x) ## [1] 5.000926 dfnorm &lt;- data.frame(vals = x) p &lt;- ggplot(dfnorm, aes(x = vals)) + geom_histogram(aes(y = ..density..), color = &quot;black&quot;, fill = &quot;purple&quot;, alpha=.4, binwidth = 0.5) + geom_density(alpha = 0.7, fill = &quot;mistyrose&quot;) + theme_classic() + xlab(&quot;values&quot;) p Normal distributions can vary in their means and standard deviations. Below is an image of a selection of four different normal distributions that all vary in their means and standard deviations. The dotted vertical black line in each graph indicates where their respective means lie. We use special notation to indicate that a distribution is a Normal Distribution. For instance, for the normal distribution that has a mean of 17 and a standard deviation of 7, we would write: \\(N(\\mu=17, \\sigma^{2}=49)\\) which demonstrates that the distribution is approximately normal with a mean of 17 and variance of 49 (which is the standard deviation, 7, squared). 7.0.4 Standard Normal Distribution Although normal distributions can have various means or standard deviations, there is one case that we reserve and call the standard normal distribution. This is for the situation where the mean of the distribution is 0 and the standard deviation (and the variance) is equal to 1. \\(N(\\mu=0, \\sigma^{2}=1)\\) How does the standard normal distribution come about? We will discuss more about this distribution in section 7.0.3, but briefly it is obtained by converting all the values of a normal distribution into z-scores. z-scores are calculated by: \\(\\Large z=\\frac{x - {\\mu}_x}{\\sigma_x}\\) This standard normal distribution is very useful in statistics because we can precisely calculate the proportion of the distribution that is to the left or right under the curve at any point of it. This principle forms the basis of several statistical tests. 7.0.5 Skewness and Kurtosis Above we described that a normal distribution has a skewness of 0 and a kurtosis of 3, but then we just skipped along and didnt really say anything else. Its important to take a quick step back and think about these two things. 7.0.5.1 Skewness We most commonly evaluate skewness for unimodal distributions (those with one peak). The skewness of a distribution can be either negative or positive, or, if it has no skew whatsoever it will be 0. It is probably easiest to describe skewness by looking at examples. In the picture below, all distributions have a mean of 100 and a standard deviation of 20. However, they differ in their skewness. The one on the left has a skew of +0.68, the one on the right has a skew of -0.68. The one in the middle has a skew of 0 and is the only one that is normally distributed. Skewness Distributions that have negative skew are also called left skewed because their longest tail extends to the left of the distribution. Similarly, distributions that have positive skew are called right skewed because their longest tail extends to the right of the distribution. Another thing to consider about the skew of the distribution is what happens to the mean, median and mode of the distributions. First, lets look at the normal distribution we made earlier in this section that had a mean of approximately 100 and a standard deviation of approximately 5. If we get the median, mode and mean of that distribution, we get the following: mean(x) ## [1] 100.0002 median(x) ## [1] 100.0025 estimate_mode(x) # R doesn&#39;t have a built in mode function, so I&#39;m using this as a proxy ## [1] 100.05 We can see here, that all three values are really close to 100. We can look at this in our plot of the distribution. Weve overlaid a red line for the mean, a blue line for the median and an orange line for the mode. However, they all lie on top of each other at x=100, so its hard to distinguish them: p + geom_vline(xintercept = median(x), color = &quot;blue&quot;, lwd=1)+ geom_vline(xintercept = mean(x), color = &quot;red&quot;, lwd=1)+ geom_vline(xintercept = estimate_mode(x), color = &quot;darkorange&quot;, lwd=1) Now lets look at some skewed distributions as to what happens to the mode, median and mean in such distributions. In this dataset, we have 7486 rows of data (observations). Each row is a MLB player. The three numerical columns refer to the career total hits (totalH), career total at bats (totalAB), and career batting average (avg) of each player. bats &lt;- read_csv(&quot;data/batting.csv&quot;) ## Rows: 7486 Columns: 4 ## -- Column specification ----------------------------------------------------------------------------------- ## Delimiter: &quot;,&quot; ## chr (1): playerID ## dbl (3): totalH, totalAB, avg ## ## i Use `spec()` to retrieve the full column specification for this data. ## i Specify the column types or set `show_col_types = FALSE` to quiet this message. nrow(bats) ## [1] 7486 head(bats) ## # A tibble: 6 x 4 ## playerID totalH totalAB avg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aaronha01 3771 12364 0.305 ## 2 aaronto01 216 944 0.229 ## 3 abbated01 772 3044 0.254 ## 4 abbeybe01 38 225 0.169 ## 5 abbeych01 493 1756 0.281 ## 6 abbotfr01 107 513 0.209 #histogram p1 &lt;- ggplot(bats, aes(x = avg)) + geom_histogram(aes(y = ..density..), color = &quot;black&quot;, fill = &quot;lightseagreen&quot;, alpha = 0.2, binwidth = .005) + geom_density(colour = &#39;black&#39;, lwd=1) + theme_classic() + xlab(&quot;Career Batting Average&quot;) p1 As you can see, this distribution is very negatively (left) skewed. This means that there are many players who have career batting averages between 0.2 and 0.3. There are relatively few players with career averages over 0.3. There are more averages that are less than 0.2 causing the skew. We can directly measure the skewness using the skewness() function from the moments package. We can see that it is highly negatively skewed with a value of -1.01: library(moments) skewness(bats$avg) ## [1] -1.012683 Lets look at where the median, mean and mode are for this negatively skewed distribution. median(bats$avg) ## [1] 0.2466792 mean(bats$avg) ## [1] 0.237972 estimate_mode(bats$avg) ## [1] 0.2538827 This time, these descriptive values are not equal. The median and mean are lower than the mode. In fact, the mean is lowest of all. In negative skewed distributions, the mean and median get pulled towards the skewed tail of the distribution, but the mean gets pulled further. p1 + geom_vline(xintercept = median(bats$avg), color = &quot;blue&quot;, lwd=1)+ geom_vline(xintercept = mean(bats$avg), color = &quot;red&quot;, lwd=1)+ geom_vline(xintercept = estimate_mode(bats$avg), color = &quot;darkorange&quot;, lwd=1) Now lets look at what happens to the mode, median and mean in right skewed distributions. Lets look at the career at-bats of MLB players. #histogram p2 &lt;- ggplot(bats, aes(x = totalAB)) + geom_histogram(aes(y = ..density..), color = &quot;black&quot;, fill = &quot;plum&quot;, alpha = 0.2, binwidth = 200) + geom_density(colour = &#39;black&#39;, lwd=1) + theme_classic() p2 This distribution is extremely right (positive) skewed. If we measure the skewness we find that the skewness is 1.63. skewness(bats$totalAB) ## [1] 1.626115 Now, lets look at the median, mean and mode and plot these on the distribution: median(bats$totalAB) ## [1] 1037 mean(bats$totalAB) ## [1] 1958.68 estimate_mode(bats$totalAB) ## [1] 454.0069 p2 + geom_vline(xintercept = median(bats$totalAB), color = &quot;blue&quot;, lwd=1)+ geom_vline(xintercept = mean(bats$totalAB), color = &quot;red&quot;, lwd=1)+ geom_vline(xintercept = estimate_mode(bats$totalAB), color = &quot;darkorange&quot;, lwd=1) In these severely right skewed distribution, we again see that the median and mean get pulled towards the tail of the distribution, with the mean getting pulled even further than the median. Lets have a look at a quick summary figure of where the mean, median and mode lie with respect to each other in skewed distributions. As you can see, the mean always gets pulled the furthest to the tail of distributions.The reason for this is that the mean is much more affected by extreme outliers than the median. The median is simply the boundary which divides the top 50% of the data from the bottom 50% of the data. The mean has to include all values in its calculation, so can be largely affected by extreme values more so than the median. Skewness 7.0.5.2 Kurtosis Another measure of the shape of a data distribution is called kurtosis. Kurtosis relates to how heavy or thin the tails of the distribution are. Often people talk about kurtosis being related to how peaky the distribution is, because distributions of different kurtosis values often look to have different peaks - but strictly, kurtosis is about the tails. We can broadly describe three basic patterns of kurtosis. Mesokurtic - tails are not too thick or too thin - this is the case with the normal distribution. Platykurtic - data is moved more from the center of the distribution to the tails. Leptokurtic - data is taken from the shoulders of the distribution and moved more into the center and slightly to the tails These are illustrated below: In addition, to using the above funny names to describe the shapes of distributions, we can also actually calculate a kurtosis value. We wont show the formula here, but effectively the values relate to the patterns as follows. Mesokurtic distributions have a kurtosis value of approximately 3. Leptokurtic distributions have a kurtosis value greater than 3. Platykurtic distributions have a kurtosis value lower than 3. We can compute this kurtosis value in R using the kurtosis() function from the package moments. Both our distributions of total at bats and averages have kurtosis values above 3, indicating that they are leptokurtic - i.e. quite peaky and have less data in the shoulders of the distribution. kurtosis(bats$totalAB) # &gt;3 = &#39;peaky&#39; less in shoulders of tails ## [1] 5.325537 kurtosis(bats$avg) # &gt;3 = &#39;peaky&#39; less in shoulders of tails ## [1] 4.18612 Another thing of interest is what happens with small sample sizes. Typically small sample sizes are not that normal - that is they are fairly skewed. They are also often fairly platykurtic - they have less data in the center and more in the shoulders and tails of the distribution. The below shows an example for a sample of 10 scores that come from a normal population of \\(\\mu=100\\) and \\(\\sigma=5\\). set.seed(10) x1 &lt;- rnorm(n = 10, mean = 100, sd = 5.0) x1 ## [1] 100.09373 99.07874 93.14335 97.00416 101.47273 101.94897 93.95962 98.18162 91.86664 98.71761 skewness(x1) ## [1] -0.4021377 kurtosis(x1) ## [1] 1.848359 You can see that this small sample is skewed with a skewness of -0.40 and platykurtic with a kurtosis of 1.85. 7.1 Z-scores z-scores are a useful way of comparing different scores in different distributions. As an example, lets look at the two distributions below. On the left we have the population of all Airedale Terriers that is normally distributed with a mean of 60 lbs with a standard deviation of 6 lbs. On the right we have the population of all Scottish Terriers that is normally distributed with a mean of 20 lbs and a standard deviation of 0.4 lbs. terriers If you owned an Airedale Terrier that was 65 lbs and a Scottish Terrier that was 20.5 lbs, would you be able to say which one was relatively larger than their breed average? Both of them are above the mean of their breeds, but by how much? The Airedale Terrier is (65-60) 5 lbs heavier than the mean of the breed, whereas the Scottish Terrier is only (20.5-20) 0.5 lbs heavier than its breed average. Looking at things in absolute terms however is misleading. It would be better if we could somehow standardize these differences. This is where z-scores come in. z-scores enable us to calculate how far any datapoint is from the mean of its distribution by saying how many standard deviations away from the mean it is. Lets look at the Airedale Terrier. Your Airedale is 5 lbs heavier than the mean of 60 lbs. This is a bit less than one standard deviation above the mean, as the standard deviation is 6 lbs. However, your Scottish Terrier is 0.5 lbs heavier than the mean of 20 lbs, which is a bit more than the standard deviation of 0.4 lbs for that breed. We can calculate precisely how many standard deviations away from the mean they are using z-scores. This is the formula: \\(\\Large z=\\frac{x - {\\mu}_x}{\\sigma_x}\\) Using this formula, lets calculate the z-scores for each of our dogs: #Airedale (65 - 60) / 6 ## [1] 0.8333333 #Scottish (20.5 - 20) / 0.4 ## [1] 1.25 The z-score for our 65 lb Airedale is z=0.83. The z-score for our 20.5 lb Scottish is z=1.25. This shows us that our Scottish Terrier is actual more standard deviations away from its breed mean than is our Airedale Terrier dog. We could also plot each of these z-scores on top of the standard normal distribution. Remember, this is the specific case of the normal distribution where the mean of the distribution is 0 and the standard deviation is 1. Shown below, weve plotted on the top row the breed population histograms with red vertical lines the weights of each of these dogs on their respective population histogram. On the bottom row we have these values converted to their z-scores and still shown with a red line. Each is overlaid on top of a standard normal distribution. z-scores z-scores can be very useful ways of standardizing observed values into ways that we can directly compare across different distributions. If we calculate a negative z-score then its clear that our observed value is below the population mean, and if we calculate a positive z-score then our value is greater than the population mean. The size of the z-score relates to how many population standard deviations from the mean each value is overall. 7.1.1 z-scores in samples. Often we may not know the population mean or standard deviation. In such cases if all we have is a sample mean and a sample standard deviation, we can still calculate z-scores for such samples. We effectively use the same formula to calculate the z-scores, just substituting in the sample mean and standard deviation. \\(\\Large z=\\frac{x - {\\overline{x}}}{s_{x}}\\) For instance, lets look at the following sample of ages for players on a village cricket team: ages &lt;- c(23, 19, 21, 33, 51, 40, 16, 15, 61, 55, 30, 28) mean(ages) ## [1] 32.66667 sd(ages) ## [1] 15.74417 These data clearly dont look normally distributed, but we still are able to calculate a mean and standard deviation. We can also still calculate the z-scores for each age: z &lt;- (ages - mean(ages)) / sd(ages) z ## [1] -0.61398401 -0.86804636 -0.74101519 0.02117186 1.16445243 0.46578097 -1.05859312 -1.12210871 ## [9] 1.79960831 1.41851478 -0.16937490 -0.29640607 Youll notice that those individuals that have negative z-scores are younger than the mean age of 32.67. Those individuals with positive z-scores are older than the mean age of 32.67. The largest z-score in terms of magnitude (either in the positive or negative direction) is 1.8. This person was 61 and was 1.8 standard deviations older than the average age. Although its possible to calculate z-scores for any sample, if the sample data come from a normally distributed population then we can use this z-score principle to perform inferential statistics (see xxx.xxx) 7.1.2 Using z-scores to determine probabilities One of the great things about the normal distribution, is that we can calculate quite straightforwardly what proportion of the distribution lies under the curve for any distance away from the mean measured in standard deviation. With computers, this is a very trivial task to perform, as well see shortly. Prior to the computer age, these computations werent as easy, so wed often use something called the empirical rule. This is basically a framework that tell us what proportion of the distribution lies under the curve at 1\\(\\sigma\\), 2\\(\\sigma\\), 3\\(\\sigma\\), etc. from the mean. Lets look at the distribution below, which is normally distributed with a mean (\\(\\mu\\)) of 14 and a standard deviation (\\(\\sigma\\)) of 4. The first thing to note is that a normal distribution is perfectly symmetrical, with equal area under the curve on either side of the mean. Therefore, in our example, 50% of the distribution lies below the mean of 14, and 50% of datapoints lie above the mean. The area colored in green in the distribution represents the area of the distribution that lies \\(\\mu \\pm1\\sigma\\). The area colored in pinky-purple lie between \\(\\mu+1\\sigma\\) and \\(\\mu+2\\sigma\\) or between \\(\\mu-1\\sigma\\) and \\(\\mu-2\\sigma\\). The area colored in yellow lie between \\(\\mu+2\\sigma\\) and \\(\\mu+3\\sigma\\) or between \\(\\mu-2\\sigma\\) and \\(\\mu-3\\sigma\\). The blue areas represent the proportion of the distribution that lies beyond \\(\\mu \\pm4\\sigma\\). Rather than look at the proportion that lies between two boundaries, often instead we describe the proportion of the distribution that lies to the left of a certain value. The table below shows what proportion of the distribution lie to the left of each value. For instance, in the above distribution \\(\\mu+2\\sigma=22\\). According to the table below, we have 97.72% of the data/distribution that are to the left of \\(x=22\\). The above table and histogram are obviously useful guides for knowing what proportion of the data exist at certain breakpoints. But, what if you had a value of \\(x=17.5\\) in the above distribution? What proportion of the data are below this value? Well we can actually work this out if we convert our raw scores to z-scores. Once we have a z-score, we can calculate the area to the left of any point on the standard normal curve. Our value of \\(x=17.5\\) has a z-score of 0.875, so it is 0.875 standard deviations above the mean. (17.5 - 14) /4 ## [1] 0.875 Lets look at that on the standard normal curve, with the value \\(z = 0.875\\) represented by the red solid line. We can obtain the area in the distribution to the left of this value that is shaded in light red in R, using the function pnorm(). pnorm(0.875) ## [1] 0.809213 This shows us that 80.92% of the distribution lie to the left of \\(z=0.875\\). To get what proportion of the distribution lie to the right of this value, we just subtract it from 1. 1 - pnorm(0.875) ## [1] 0.190787 Lets look at the proportions under the curve to the left of plus or minus 0, 1, 2, or 3 standard deviations from the mean. zvals &lt;- c(-3,-2,-1,0,1,2,3) pnorm(zvals) ## [1] 0.001349898 0.022750132 0.158655254 0.500000000 0.841344746 0.977249868 0.998650102 Hopefully you can see that these values mirror those in the table provided above. 7.1.2.1 z-score and probability problems. Lets take this a little further with some small examples. Example 1 Lets assume that the weights of pineapples are normally distributed with a mean of 1003.5g and a standard deviation of 35g. You bought a random pineapple and it turned out to only be 940g. What proportion of pineapples are less than 940g? How unlucky did you get to buy such a small pineapple? First, lets take a look at the population distribution of pineapples with \\(\\mu=1003.5\\) and \\(\\sigma=35\\). Our pineapple is 940g and is shown with the solid red line below. As our distribution is normal, if we convert this to a z-score we can compare it to where z is in the standard normal distribution on the right. (940 - 1003.5) / 35 ## [1] -1.814286 So we calculated that \\(z = -1.81\\), and we visualize that on our standard normal distribution. Were interested in what proportion of pineapples from the distribution are 940g or less. That is the light red shaded area. To calculate that we can just use pnorm() in R: pnorm(-1.81) ## [1] 0.03514789 From this we can see that only 3.5% of pineapples are less than 940g. We got super unlucky to get such a tiny pineapple. Example 2 What is the probability of getting a pineapple of greater than 1050g ? To answer this we first get the z-score for a pineapple of 1050g, and find that \\(z = 1.33\\). (1050 - 1003.5) / 35 ## [1] 1.328571 Next we recognize that if were interested in what proportion of pineapples weigh more than 1050g, we need to know what proportion of the standard normal curve is greater than \\(z = 1.33\\) (the shaded light red area below). We can calculate that by using pnorm() to figure out what proportion is to the left of \\(z = 1.33\\), and then subtract that from 1 to get what proportion is to the right. 1 - pnorm(1.33) #0.003 (so 9.18% chance) ## [1] 0.09175914 Example 3 What is the probability of getting a pineapple between 950g and 1045g ? For this question, were interested in the shaded light red area between \\(z = x\\) and \\(z = z\\) on the standard normal curve. Why these z-values? Because these are the z scores you get if you convert a 950g and a 1045g pineapple to z scores. z1 &lt;- (950 - 1003.5) / 35 z2 &lt;- (1045 - 1003.5) / 35 z1 #-1.53 ## [1] -1.528571 z2 #1.19 ## [1] 1.185714 In R we can calculate the proportion to the left of each of these z-scores using pnorm(). What we need is the shaded area, which we can get if we subtract the area to the left of \\(z = -1.53\\) from the area to the left of \\(z = 1.19\\). We do it like this: ## get proportion of curve to left of each z-score pnorm(z1) # 0.06 ## [1] 0.06318536 pnorm(z2) # 0.88 ## [1] 0.8821324 # so the area between them is: pnorm(z2) - pnorm(z1) #0.82 ## [1] 0.8189471 So, 82% of the distribution lie between 950g and 1040g. 7.2 What is a Sampling Distribution ? Another type of distribution that we will discuss a lot! is the sampling distribution. There are different types of sampling distributions, so for now well focus on the sampling distribution of the sample means. The best way to illustrate a sampling distribution, is to show it by example. Say we have a population of 1 million adult Archerfish. The population mean of their bodylength is \\(\\mu = 100.0mm\\), and the population standard deviation is \\(\\sigma = 15.0mm\\). Lets create this population: set.seed(3) archerfish &lt;- rnorm(n = 1000000, mean = 100, sd = 15) mean(archerfish) ## [1] 100.0061 sd(archerfish) ## [1] 15.02418 Lets also plot what this normally distrusted population looks like by making a histogram: # histogram of the population: ggplot(data.frame(archerfish), aes(x = archerfish)) + geom_histogram(color=&#39;black&#39;, fill=&#39;#f584ed&#39;, alpha=.5, binwidth =2) + theme_classic() + xlab(&quot;Body Length mm&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Body Sizes of 1 million Archerfish&quot;) + geom_vline(xintercept = 100.0, lwd=1, color=&#39;black&#39;) OK, lets say that you didnt have the time to go out and catch 1 million archerfish and measure the body length of every single fish. What should you do? One thing you might decide is to just go and take a random sample of 10 archerfish (you could have picked another sample size - lets just stick with 10 for now). Once you have your sample of 10 archerfish, you could then measure them and you will be able to calculate the sample mean of that sample. Well use sample() to randomly select 10 archerfish. set.seed(1) # so we all get the same sample. samp1 &lt;- sample(archerfish, 10, replace = T) samp1 ## [1] 95.84601 106.80884 95.77485 89.09022 94.47476 85.87588 90.68373 98.53963 91.73568 75.49939 mean(samp1) ## [1] 92.4329 Our sample mean is \\(\\overline{x}=95.6\\) - thats fairly close to the actual population mean of 100.0mm. Lets grab another three samples of 10 archerfish and see what the sample means are of those sample: mean(sample(archerfish, 10, replace = T)) # mean of our 2nd sample ## [1] 99.65941 mean(sample(archerfish, 10, replace = T)) # mean of our 3rd sample ## [1] 98.52547 mean(sample(archerfish, 10, replace = T)) # mean of our 4th sample ## [1] 96.3643 These next three samples are closer to 100.0, with one just above and two just below 100. What would happen if we collected thousands and thousands of samples of size 10? Lets do it - you dont need to follow the exact code here of how were doing this, but essentially were grabbing 20,000 samples of size 10. From each of these were getting the sample mean. That means well end up with 20,000 sample means. Were storing those results in the object called res. ### What if we were to collect 20,000 samples and for each one get the mean results&lt;-vector(&#39;list&#39;,20000) for(i in 1:20000){ results[[i]] &lt;- mean(sample(archerfish, 10, replace = T)) } res &lt;- unlist(results) Now we have our 20,000 sample means we could make a histogram of these sample means. psd &lt;- ggplot(data.frame(res), aes(x = res)) + geom_histogram(color=&#39;black&#39;, fill=&#39;#4adbe0&#39;, alpha=.5, binwidth = 1) + theme_classic() + xlab(&quot;Mean Body Length of each sample - mm&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Sampling Distribution of Sample Means for n=10&quot;) + geom_vline(xintercept = mean(res), lwd=1) psd This distribution that weve just plotted is the sampling distribution of sample means for samples of size n=10. We picked 20,000 as the number of samples of size 10 to collect as its reasonably large enough to get enough sample means that we can see the shape of the distribution. We could have picked 100,000 samples to collect, or 1,000,000 in fact the more the better, but 20,000 is enough to get the point across. Out of all of these sample means that we collected, what is the average across all of the 20,000 samples? - and whats more, what is the standard deviation of that distribution? mean(res) ## [1] 100.0479 sd(res) #standard deviation of the sampling distribution, aka standard error ## [1] 4.77202 Lets first focus on the mean. Our mean of sample means was 99.996, which is very approximately the same as the mean of 1,000,000 archerfish in the population! It turns out that the mean of the sampling distribution is approximately equal to the population mean. By the way, the notation that we use to depict the mean of the sampling distribution of sample means is \\(\\mu_{\\overline{x}}\\). So \\(\\mu_{\\overline{x}} = 100.0\\). What is the standard deviation of this distribution (the sampling distribution of the sample means)? We just calculated it in R to be 4.77. The notation we use for this is \\(\\sigma_{\\overline{x}} = 4.77\\). Its called the standard deviation of the sampling distribution of sample means, but for short it gets called the standard error. Of course, we never in reality actually collect thousands and thousands of samples - that defeats the point of sampling. If we had time to collect thousands and thousands of samples, we may as well just measure every archerfish in the population. Usually, we just collect one sample. In later sections, well discuss how you can estimate what \\(\\mu_{\\overline{x}}\\) and \\(\\sigma_{\\overline{x}}\\) are when you have only collected one sample. However, given we already know the population standard deviation of the 1 million archerfish is \\(\\sigma=15\\), we can calculate the standard deviation of the sampling distribution of sample means for any sample size. It is calculated using the formula: \\(\\Large \\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\) So in our case it should be equal to: 15/sqrt(10) # 4.74 ## [1] 4.743416 As you can see this is very close to the value that we got in our simulated sampling distribution above. 7.2.1 Sample Size and the Sampling Distribution In the previous section we looked at what would happen if we took samples of size 10 from our archerfish population and looked at the sample means of each sample. We found that when we made a histogram out of these sample means, that the mean of the sample means \\(\\mu_{\\overline{x}}\\) was approximately equal to the population mean of 100. The standard deviation of the sampling distribution of sample means, or standard error, \\(\\sigma_{\\overline{x}}\\) was approximately equal to 4.74. What happens if we were to take a different sized sample each time - say size 50? What would be the mean and standard deviation of this distribution of sample means? Lets find out. Well again take 20000 samples of size 50 at random from our population of 1 million archerfish. For each sample well record the sample mean length of the 50 fish. Well end up saving these 20,000 sample means in an R object called res1. Again, dont worry about how the code is doing it here, just as long as you follow what were doing. # ok, get 20,000 samples of size 50 results1&lt;-vector(&#39;list&#39;,20000) for(i in 1:20000){ results1[[i]] &lt;- mean(sample(archerfish, 50, replace = T)) } res1 &lt;- unlist(results1) Now we have our 20,000 samples, lets plot the histogram psd2 &lt;- ggplot(data.frame(res1), aes(x = res1)) + geom_histogram(color=&#39;black&#39;, fill=&#39;#4adbe0&#39;, alpha=.5, binwidth = .5) + theme_classic() + xlab(&quot;Mean Body Length of each sample - mm&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Sampling Distribution of Sample Means for n=50&quot;)+ geom_vline(xintercept = mean(res1),lwd=1) psd2 If we look at the mean and standard deviation of this sampling distribution, we get the following values: mean(res1) # still same as population mean ## [1] 100.0038 sd(res1) # smaller with larger sample size ## [1] 2.125566 The mean of the sampling distribution of sample means \\(\\mu_{\\overline{x}}\\) is still a very good estimate of the population mean. The biggest difference is in the standard deviation of the sampling distribution, \\(\\sigma_{\\overline{x}}\\) (the standard error), which is much lower than when we had samples of size 10. The reason for this is that the variability in our sample means is much more reduced when we have samples of size 50. On average, our sample means are much closer individual estimates to the population mean. This is what drastically reduces the standard deviation of this sampling distribution. We could have calculated the standard error directly using the formula provided earlier, because we already know the population standard deviation. When we use that formula we get 2.12, almost the same standard deviation as we got with our simulated sampling distribution: 15 / sqrt(50) ## [1] 2.12132 Lets directly compare the two sampling distributions for the two different sample sizes. We adjusted the x-axis so it is the same for both figures, so you can see the change in the standard deviation between the two sample sizes: 7.3 Central Limit Theorem In the previous section we discovered that if you take many, many samples from a normally distributed population and calculated the sample mean of each sample, that you would get a sampling distribution of sample means. We also saw that that sampling distribution was normally distributed with a mean \\(\\mu_{\\overline{x}}\\) that was approximately equal to the population mean, and a standard deviation \\(\\sigma_{\\overline{x}}\\) that was equal to the population standard deviation divided by the square root of n \\(\\Large \\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\). Is it just coincidence that both the population distribution and the sampling distribution was approximately normally distributed? What we will learn in this chapter is that it does not matter at all what the population distribution is - if we take thousands of samples from any shaped distribution and calculate the sample mean of each sample, when we create the histogram of those sample means we will find that they are approximately normally distributed. This is what we refer to as the central limit theorem. Further, the larger the sample size that we take, the closer to a normal distribution the sampling distribution becomes. Lets look at this by taking samples from various different population distributions. Uniform Distribution First well look at a uniform distribution of 1 million numbers between 0 and 75. You might like to think of this as the distance of trees from the center of a forest in km. Lets graph the distribution and calculate the mean and standard deviation. set.seed(1) #get data from uniform distribution x1 &lt;- runif(1000000, min = 0, max = 75) # histogram ggplot(data.frame(x1), aes(x = x1)) + geom_histogram(color=&#39;black&#39;, fill = &quot;#894ae0&quot;, alpha=.3, binwidth = 5, boundary = 0) + theme_classic() + xlab(&quot;Distance from Center of Forest km&quot;) ### Population Mean &amp; SD mean(x1) #37.5 ## [1] 37.49417 sd(x1) #21.6 ## [1] 21.6472 We can see that the mean of this population is 37.5, and the population standard deviation is 21.6. Lets take samples of size 30 at random from this population of 1 million. For each sample, well calculate the sample mean. Well take 10,000 samples (again - we could have picked any really large number here, but 10,000 seems reasonable enough to prove our point). After we get our 10,000 sample means from samples of size 30, well plot the histogram of those sample means. ## Let&#39;s get 10,000 samples of size 30 results&lt;-vector(&#39;list&#39;,10000) for(i in 1:10000){ results[[i]] &lt;- mean(sample(x1, 30, replace = T)) } res &lt;- unlist(results) # This is the sampling distribution. ggplot(data.frame(res), aes(x = res)) + geom_histogram(color=&#39;black&#39;, fill=&#39;#894ae0&#39;, alpha=.5, binwidth = 1) + theme_classic() + geom_vline(xintercept = mean(res), lwd=1) + xlab(&quot;Mean of each sample&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Sampling Distribution of Sample Means for n=30&quot;) This histogram represents our sampling distribution of sample means when we took samples of size 30 from a uniform distribution. Hopefully you notice that it is approximately normally distributed - even though the original population was uniformally distributed! Lets calculate the mean and standard deviation of this sampling distribution: mean(res) # the mean of the sample means is close to 37.5, the population mean ## [1] 37.54835 sd(res) #3.996 - this is a lot smaller than the population SD ## [1] 3.956685 The mean of our sampling distribution \\(\\mu_{\\overline{x}} = 37.5\\) which is again approximately equal to the population mean. The sampling distribution standard deviation \\(\\sigma_{\\overline{x}} = 3.99\\) which is a lot lower than the original population standard deviation. Because we only took 10,000 samples, these values arent exact, but we could have calculated the standard error by taking the population standard deviation and dividing by the square root of n \\(\\Large \\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\) as follows: sd(x1)/sqrt(30) # standard error. ## [1] 3.952219 Youll notice that this calculated value is very close to the one we estimated by running our simulation. Skewed Distributions We can see that the central limit theorem holds true for skewed distributions also. Here, we have a population of 1 million charity donations. Lets draw a histogram of the population distribution and calculate the mean and standard deviation of the population. set.seed(1) # e.g. Donations to a charity (in $s) q &lt;- rnbinom(1000000, 5, .4) # histogram ggplot(data.frame(q), aes(x = q)) + geom_histogram(color=&#39;black&#39;, fill = &quot;#1ad665&quot;, alpha=.3, binwidth = 1, boundary = 0) + theme_classic() + xlab(&quot;Charity Donation in $&quot;) ### Population Mean mean(q) #7.5 ## [1] 7.504368 sd(q) #4.33 ## [1] 4.331767 It is clear that this population distribution is highly positively skewed. The mean of the population is 7.5 and the standard deviation is 4.33. In the following code we takes samples of size 30 and calculate the sample mean of each sample. This time, well collect 50,000 samples, just to be a bit different. results&lt;-vector(&#39;list&#39;,50000) for(i in 1:10000){ results[[i]] &lt;- mean(sample(q, 30, replace = T)) } res &lt;- unlist(results) ### Let&#39;s Draw this as a histogram. ggplot(data.frame(res), aes(x = res)) + geom_histogram(color=&#39;black&#39;, fill=&#39;#31e8d0&#39;, alpha=.5, binwidth = .1) + theme_classic() + geom_vline(xintercept = mean(res), lwd=1) + xlab(&quot;Mean of each sample&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Sampling Distribution of Sample Means for n=30&quot;) So, it happened again. This is our sampling distribution of sample means, collected from samples of size 30 from a highly skewed distribution. But once again, the sampling distribution is approximately normally distributed. It might not be perfectly normally distributed, but it is close to being normal. We can calculate the mean and the standard deviation of this sampling distribution: mean(res) # the mean of the sample means is close to 7.5 ## [1] 7.514097 sd(res) #0.799 ## [1] 0.7850946 \\(\\mu_{\\overline{x}} = 7.5\\) which is once again approximately equal to the original population mean. The sampling distribution standard deviation \\(\\sigma_{\\overline{x}} = 0.799\\). We could have directly calculated that using the formula for the standard error, n \\(\\Large \\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\), as we know the original population standard deviation: sd(q)/sqrt(30) # standard error = 0.791 ## [1] 0.7908688 We could keep going with even more types of population distributions. We would find the same thing over and over again. If we take many samples from each population and calculated the sample means of all samples, they would form an approximately normally distribution. This will be especially true for larger samples. This is the basis of the central limit theorem. In the following sections well learn more about what we can do with these sampling distributions. 7.4 Sampling distribution problems We can use our knowledge of sampling distributions and z-scores to determine how likely or unlikely we are to observe any one particular sample mean. Lets use an example to illustrate this. Q. Say the weight of chicken eggs is normally distributed with mean 60g and standard deviation of 3g. What is the probability of getting a batch of a dozen eggs that have a mean of less than 58g ? The way to think about these questions is to recognize that were dealing with a sampling distribution. Were really being asked what proportion of the sampling distribution is less than a sample mean of 58g, when your sample size is 12 (a dozen). First, lets plot the population of chicken eggs and then the sampling distribution of samples means for a sample size of 12. set.seed(1) # so you get the same values as my script ### First, I&#39;ll make some plots of the &#39;population&#39; and &#39;sampling distribution&#39; ## Population x &lt;- rnorm(1000000, mean = 60, sd = 3) p1 &lt;- ggplot(data.frame(x), aes(x = x)) + geom_histogram(color=&#39;black&#39;, fill=&#39;mistyrose&#39;, alpha=.4)+ geom_vline(xintercept = 60, lwd=1) + ggtitle(&quot;Population of chicken eggs&quot;) + theme_classic() + xlab(&quot;Weight&quot;) ## Sampling Distribution of sample means with Sample size of 12 (a dozen). results&lt;-vector(&#39;list&#39;,10000) for(i in 1:10000){ results[[i]] &lt;- mean(sample(x, 12, replace = T)) } res &lt;- unlist(results) p2 &lt;- ggplot(data.frame(res), aes(x=res)) + geom_histogram(color=&quot;black&quot;, fill=&#39;lightseagreen&#39;, alpha=.4)+ geom_vline(xintercept = mean(res),lwd=1) + ggtitle(&quot;Sampling Distribution of Sample Means&quot;) + theme_classic() + xlab(&quot;sample mean&quot;) library(gridExtra) grid.arrange(p1,p2,nrow=1) What are the mean and standard deviation of this sampling distribution of sample means? Well, \\(\\mu_{\\overline{x}} = 60\\) because thats the population mean, and we know the mean of the sample means is approximately the same. We know we can calculate \\(\\sigma_{\\overline{x}}\\) because we know the population standard deviation \\(\\sigma\\). Again, the formula is \\(\\Large \\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\). Therefore, the standard deviation of the sampling distribution is \\(\\sigma_{\\overline{x}} = 0.87\\): sem &lt;- 3 / sqrt(12) sem ## [1] 0.8660254 Now, were interested in a sample of 12 that has a sample mean of 58g. Lets visualize what that looks like on the histogram of the sampling distribution: p2 + geom_vline(xintercept = 58, lty=2, lwd=1, color=&quot;red&quot;) + ggtitle(&quot;Sampling Distribution of Sample Means \\n for sample size = 12&quot;) Because we know the mean and standard deviation of this distribution, we can actually calculate the sample mean of 58g as a z-score. Doing this we find that \\(z = -2.31\\), which means that a sample mean of 58g is 2.31 standard deviations below the mean. # so, 58g as a z-score is... z &lt;- (58 - 60) / sem # -2.31 z ## [1] -2.309401 The original question asked what probability there was of getting a sample mean of less than 58g. This is basically what area is under the curve of the above sampling distribution to the left of the 58g sample mean. Because we converted that sample mean of 58g to a z-score of -2.31, we can look at that value on a standard normal curve. The area we are interested in is the filled in area to the left of z = -2.31: We can look up this value in R, using the function pnorm. pnorm(z) # prob = 0.010 ## [1] 0.01046067 So we can see that the probability of getting a sample mean of lower than 58g is p=0.0105. 7.5 The t-distribution Another distribution that is important to know about is the t-distribution. Like the normal distribution, this is a symmetrical distribution, but it has slightly fatter tails than the normal distribution. The t-distribution comes up most commonly in sampling distributions. In particular, although the central limit theorem prescribes that sampling distributions are approximately normal, it is known that often they arent approximately normal enough, and instead they follow a t-distribution shape. An important detail about the t-distribution is that there are actually several t-distributions. There are different t-distributions for different degrees of freedom. These differ slightly in how heavy the tails of the distribution are. The degrees of freedom are usually related to the sample size minus one or two (depending upon the test being employed - see sections 10.2 and @ref(theory-behind-students-t-test). The higher the degrees of freedom, the more closely the t-distribution looks like a normal distribution. You can see this in the image below. The standard normal distribution is in black, and the t-distribution is in red. Each panel shows a different t-distribution. As the degrees of freedom increase, the t-distribution essentially becomes the normal distribution. At lower degrees of freedom, there is a lot more difference between the t-distribution and the normal distribution in the tails. "],["confidence-intervals.html", "Chapter 8 Confidence Intervals 8.1 Sample means as estimates. 8.2 Calculating a confidence interval with z-distribution 8.3 Confidence Intervals with t-distribution 8.4 Calculating a t-distribution Confidence Interval 8.5 Comparing CIs using the z- and t-distributions", " Chapter 8 Confidence Intervals When we collect a sample, we typically calculate a sample mean \\(\\overline{x}\\). We use this as our estimate of the real population mean \\(\\mu\\), as this is unknown to us typically. If we were to collect another sample, we would most likely get a sample mean \\(\\overline{x}\\) that is slightly different to the first one, but would also be an estimate of the population mean \\(\\mu\\). Given that we are not completely sure of what the population mean \\(\\mu\\) is, or how close our sample mean \\(\\overline{x}\\) is to that population mean, one thing that we like to do is to put confidence limits around our sample mean estimate. The confidence interval gives us a range of values which likely contains our population mean. In essence, a confidence interval can be considered to be a margin of error around our sample mean estimate. In this course, we use two separate approaches to calculate confidence intervals around a sample mean \\(\\overline{x}\\). The first method uses the \\(z\\)-distribution to generate the confidence interval. The second method uses the \\(t\\)-distribution. In practice, we almost always use the \\(t\\)-distribution when doing this. In fact, the only time we really use the \\(z\\)-distribution is when teaching introductory stats. The reason for this, is that learning how to make a confidence interval using the \\(z\\)-distribution is a good stepping stone to using the \\(t\\)-distribution. Technically, we can use the \\(z\\)-distribution to calculate the confidence interval when we know the population standard deviation \\(\\sigma\\) and our sample size is relatively large. However, we almost never know \\(\\sigma\\), and so thats why in practice we use the \\(t\\)-distribution. Well start this chapter by talking about the relationship between the sampling distribution and confidence intervals. Then well describe how to use both the \\(z\\)- and \\(t-\\)distributions to generate confidence intervals. 8.1 Sample means as estimates. Let us imagine we have a population of butterflies and were interested in their wingspan. The population is normally distributed with a population mean \\(\\mu = 7.8cm\\), with a population standard deviation \\(\\sigma = 0.3cm\\). This is what this population distribution looks like: set.seed(1) x &lt;- rnorm(100000, mean = 7.8, sd = 0.3) library(tidyverse) p1 &lt;- ggplot(data.frame(vals=x),aes(vals))+ geom_histogram(aes(y = ..density..), color = &quot;black&quot;, fill = &quot;purple&quot;, alpha=.4, binwidth = 0.05) + geom_density(alpha = 0.7, fill = &quot;mistyrose&quot;) + theme_classic() + xlab(&quot;Wingspan cm&quot;)+ geom_vline(xintercept = 7.8, color=&#39;black&#39;,lwd=1) p1 Now, lets collect samples of size \\(n=15\\). Heres one sample, and its sample mean \\(\\overline{x}\\): samp1 &lt;- sample(x, size = 15, replace = T) samp1 ## [1] 7.887566 7.639644 8.262722 7.672396 7.782234 7.775534 8.027597 7.531366 7.729648 7.901003 7.223573 ## [12] 8.133634 7.818117 7.646168 7.760589 mean(samp1) ## [1] 7.786119 Our observed sample mean is \\(\\overline{x}=7.71\\) which is close to the population mean of \\(\\mu=7.8\\). But if we were to collect another sample, then that sample mean will be slightly different. Lets do it again: samp2 &lt;- sample(x, size = 15, replace = T) samp2 ## [1] 7.807306 7.777020 8.016991 8.039386 7.775994 7.418834 8.143334 7.808769 7.883243 7.687917 7.760589 ## [12] 7.898745 7.644706 8.286361 8.140363 mean(samp2) ## [1] 7.872637 This time our sample mean (our estimate) is \\(\\overline{x}=7.81\\) which is very close. If we did this thousands of times, then wed get our sampling distribution of sample means (see section 7.0.3). This is what our sampling distribution for sample sizes of \\(n=15\\) looks like: #get sample means for sampling distribution results&lt;-vector(&#39;list&#39;,100000) for(i in 1:100000){ results[[i]] &lt;- mean(sample(x, 15, replace = T)) } res &lt;- unlist(results) p2 &lt;- ggplot(data.frame(res), aes(x = res)) + geom_histogram(aes(y = ..density..), color = &quot;black&quot;, fill = &quot;#4adbe0&quot;, alpha=.4, binwidth = 0.01) + geom_density(alpha = 0.7, fill = &quot;ghostwhite&quot;) + theme_classic() + xlab(&quot;Sample Mean&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Sampling Distribution of Sample Means for n=15&quot;) + geom_vline(xintercept = mean(res), lwd=1) p2 According to Central Limit Theorem, this sampling distribution is approximately normally distributed. The mean of this sampling distribution is \\(\\mu_{\\overline{x}}=7.8\\) which is the same as the population mean \\(\\mu\\). The standard deviation of this sampling distribution \\(\\Large \\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\). 0.3 / sqrt(15) ## [1] 0.07745967 Therefore the standard deviation of this sampling distribution is \\(\\sigma_{\\overline{x}} = 0.077\\). Remember, this sampling distribution represents thousands and thousands of potential means from individual samples of size 15 that we could have collected. Each one of them in isolation would be our point estimate of the true population mean \\(\\mu\\). Sometimes well be really close to the true population mean, and other times we might be quite far away. This is why we like to put confidence intervals around our sample means, to give a range of values that likely contain our population mean. One thing we can do first is to think about this - between which two values on the sampling distribution shown above would 95% of the data lie? That is the same as asking, which two values represent the part where 2.5% of the distribution is in each tail (leaving 95% in the middle). To answer this, we just need to remember that according to Central Limit Theorem that our sampling distribution is normally distributed. Therefore we can use the standard normal curve. According to the standard normal distribution, the values of z that leave 2.5% in each tail are \\(z=-1.96\\) and \\(z=1.96\\). That means values that 95% of the distribution lie between 1.96 standard deviations below and above the mean. If you didnt want to take our word for it that +1.96 and -1.96 are the values of z that leave 2.5% in each tail, you could also directly calculate it in R: qnorm(c(0.025, 0.975)) # get the values of z that are the boundaries of 2.5% to the left, and 97.5% to the left. ## [1] -1.959964 1.959964 So if we go back to thinking about our sampling distribution - because we say it is approximately normally distributed, 95% of the distribution will also lie between 1.96 standard deviations below and above the mean. We know that the mean of the sampling distribution is \\(\\mu_{\\overline{x}=7.8}\\) and the standard deviation of the sampling distribution is \\(\\sigma_{\\overline{x}} = 0.077\\), as we calculated it above. Therefore, we can use this to calculate which sample mean values in the distribution are 1.96 standard deviations either side of the mean. They are: 7.8 + (1.96 * 0.077) ## [1] 7.95092 7.8 - (1.96 * 0.077) ## [1] 7.64908 So, 95% of our sample means in our sampling distribution lie between 7.65 and 7.95. That area is represented by the shaded red area on our sampling distribution below: What we have just done is the basic principle behind a confidence interval using a \\(z\\)-distribution. Lets look at this in more detail. 8.2 Calculating a confidence interval with z-distribution Lets go back to our first sample of size 15 that we collected, with \\(\\overline{x}=7.71\\). samp1 ## [1] 7.887566 7.639644 8.262722 7.672396 7.782234 7.775534 8.027597 7.531366 7.729648 7.901003 7.223573 ## [12] 8.133634 7.818117 7.646168 7.760589 mean(samp1) ## [1] 7.786119 What we want to do now is put a confidence interval around 7.71. We want to say that our population mean is equal to \\(7.71 \\pm margin.of.error\\) The actual formula for the \\(z\\)-distribution confidence interval is: \\(\\Large CI_{95\\%} = \\overline{x} \\pm z \\times \\frac{\\sigma}{\\sqrt{n}}\\) In this scenario, we are presuming that we dont know what the population mean \\(\\mu\\) is - thats why were building a confidence interval. Consequently, we also dont precisely know what the mean of the sampling distribution \\(\\mu_{\\overline{x}}\\) is. What well do instead, is to assume that our sample mean \\(\\overline{x}\\) is the mean of the sampling distribution \\(\\mu_{\\overline{x}}\\). We already know what the standard deviation of the sampling distribution \\(\\sigma_{\\overline{x}}\\) is because we know the population standard deviation \\(\\sigma\\) is. So, \\(\\Large \\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\). What value of \\(z\\) should we use? The short answer is 1.96 for the same reasons as above. If our sampling distribution is normally distributed, then we want to know the values that are \\(\\pm 1.96\\) of the sample mean \\(\\overline{x}\\). So, lets just do it - this is how we calculate the 95% confidence interval if we have \\(\\overline{x}\\), \\(\\sigma\\) and \\(n\\). x_bar &lt;- mean(samp1) # sample mean = 7.71 x_bar ## [1] 7.786119 n &lt;- length(samp1) # sample size = n = 15 n ## [1] 15 sigma &lt;- 0.3 # the pop SD given in the example sem &lt;- sigma/sqrt(n) # standard error of the mean (SD of sampling distribution) sem ## [1] 0.07745967 z &lt;- 1.96 # the value of &#39;z&#39; we need to get the middle 95% of the distribution # margin of error z * sem ## [1] 0.1518209 # upper bound of confidence interval x_bar + (z * sem) ## [1] 7.93794 # lower bound of confidence interval x_bar - (z * sem) ## [1] 7.634298 We have just calculated our 95% confidence interval! It has a lower bound of 7.56cm and an upper bound of 7.86cm. We can write this confidence interval in two ways: \\(CI_{95\\%} = 7.71 \\pm 0.152\\) \\(CI_{95\\%} = 7.71 [7.56, 7.86]\\) Below is a graphical representation of our confidence interval around our sample mean \\(\\overline{x}\\). You can see that the true population mean \\(\\mu\\) is within the confidence interval. Remember we collected a second sample that had a sample mean \\(\\overline{x}=7.81\\) ? mean(samp2) ## [1] 7.872637 We could also create a 95% confidence interval for our estimate of the population mean \\(\\mu\\) using this sample mean \\(\\overline{x}\\). We just use the same formula: # upper bound of confidence interval 7.87 + (z * sem) ## [1] 8.021821 # lower bound of confidence interval 7.87 - (z * sem) ## [1] 7.718179 \\(CI_{95\\%} = 7.81 \\pm 0.152\\) \\(CI_{95\\%} = 7.81 [7.72, 8.02]\\) Lets compare this confidence interval with the first one we created: Note that both include the true population mean of 7.8 in their confidence interval. What if we collected 25 new samples, and calculated 25 sample means, and made 25 confidence intervals? Well, the chart below shows 25 95% confidence intervals collected from samples of size 15 selected at random from our population of butterflies: First, notice that the margin of error is equal for all of our confidence intervals around the sample means. This is because we are using the same value of \\(\\sigma\\) and same value of \\(z\\) for all of these confidence intervals. Secondly, youll notice that not all the confidence intervals include the population mean \\(\\mu\\). Two of them - highlighted in green - do not include the population mean. In this sense, our sample mean and associated confidence interval is not doing a terrific job of estimating the population mean. Actually, it turns out that if you collect enough samples and generate enough sample means, then you will capture the population mean within your confidence interval 95% of the time. So roughly 5 out of every 100 confidence intervals you make from samples will not include the population mean. Technically, this is the definition of a 95% confidence interval. That is, in 95% of your samples you will include the true population mean. However, when talking about confidence intervals in lay-speak, when we have our one confidence interval around our one sample mean e.g. \\(CI_{95\\%} = 7.72 [7.568, 7.871]\\), we often say \"theres a 95% chance that the true population mean is between 7.568 and 7.871. This is technically lazy shorthand although it does kind of help us understand the point of a confidence interval. But, please remember, the real definition is that in 95% of samples well include the true population mean in our samples. Assumptions We should also briefly just remark on what the assumptions are when generating these \\(z\\)-distribution based confidence intervals. We are assuming that our data are normally distributed and that our sample is randomly drawn from the population, and that all data points are independent of each other. 8.2.1 Other Confidence Intervals ranges We can actually construct confidence intervals for any % value. Most commonly people make 95% confidence intervals, but other common ones include 80%, 90% and 99% confidence intervals. These have the same interpretation as the 95% CI. For instance, a 99% confidence interval means that if you were to take 100 samples from a population and calculate 99% confidence intervals for each, only 1 out of a 100 on average would not include the true population mean \\(\\mu\\). The formulas for each of these confidence intervals when using the \\(z\\)-distribution are as follows: \\(\\Large CI_{80\\%} = \\overline{x} \\pm 1.28 \\times \\frac{\\sigma}{\\sqrt{n}}\\) \\(\\Large CI_{90\\%} = \\overline{x} \\pm 1.64 \\times \\frac{\\sigma}{\\sqrt{n}}\\) \\(\\Large CI_{95\\%} = \\overline{x} \\pm 1.96 \\times \\frac{\\sigma}{\\sqrt{n}}\\) \\(\\Large CI_{99\\%} = \\overline{x} \\pm 2.58 \\times \\frac{\\sigma}{\\sqrt{n}}\\) Where did each of these different numbers come from for \\(z\\) ? Well, if we wish to make a 99% CI, we need to know what values of \\(z\\) are the boundaries that leave 99% of the distribution inside them on the standard normal curve. We exclude 0.5% in each tail. Likewise, for the 80%CI, we want to know the values of \\(z\\) that leave 5% in each tail and 10% in the middle. We can calculate these values in R like this: qnorm(.9)# for 80% CI ## [1] 1.281552 qnorm(.95)# for 90% CI ## [1] 1.644854 qnorm(.975)# for 95% CI ## [1] 1.959964 qnorm(.995)# for 99% CI ## [1] 2.575829 So, if we were to calculate the 80% confidence interval for a sample with a sample mean of \\(\\overline{x}=7.72\\), we would do: #margin of error 1.281552 * sem ## [1] 0.09926859 # upper bound of confidence interval 7.72 + (1.281552 * sem) ## [1] 7.819269 # lower bound of confidence interval 7.72 - (1.281552 * sem) ## [1] 7.620731 Our 80% confidence interval is: \\(\\Large CI_{80\\%} = 7.72 \\pm 0.099\\) \\(\\Large CI_{80\\%} = 7.72 [7.620, 7.819]\\) The figure below shows the CIs that we would create using each of these different values of \\(z\\) for 80%, 90%, 95% and 99% CIs for a sample of size 15 with a sample mean of \\(\\overline{x}=7.72\\). Clearly, confidence intervals widen with higher percentages. This makes sense, because out of 100 samples, with a 99% CI wed expect only one confidence interval out of 100 to not include the true population mean, but with a 80% CI wed expect 20/100 CIs not to include the true population mean. Lets look at this same figure, but this time for a sample that has an sample mean \\(\\overline{x}=7.95\\): As you can see here, this time the 90% and 80% CIs do not include the true population mean \\(\\mu\\). We increase the chances of including the true population mean \\(\\mu\\) inside our confidence interval by increasing the level of our confidence interval. A 99% confidence interval has an increased probability of including the confidence interval compared to a 95% confidence interval and so on. 8.2.2 Confidence Intervals and Sample Size The other variable inside the confidence interval formula that we should think about is the sample size \\(n\\). Lets look at the formula again: \\(\\Large CI = \\overline{x} \\pm z \\times \\frac{\\sigma}{\\sqrt{n}}\\) What happens when we get different sized sample sizes? For instance, look at the 95% confidence intervals below that all have a sample mean of \\(\\overline{x}=7.72\\) but are for different sample sizes: There are two things to note. First, as your sample size increases, for any confidence level (in this situation a 95% CI) the confidence interval is going to shrink. It gets tighter for larger sample sizes. Increasing sample sizes increases certainty. This is because the denominator of the confidence interval formula \\(\\sqrt{n}\\) gets larger, meaning that the margin of error gets smaller. The second thing might seem counter-intuitive. Why does it look like in the graph above that a sample size of \\(n=50\\) is only just able to have \\(\\mu\\) contained within it? It would seem that a larger sample size should do a better job of including \\(\\mu\\). Well, remember, that a 95% CI really means that 95% of your sample means will contain \\(\\mu\\). so across all these sample sizes you have a 95% chance of having captured \\(\\mu\\) inside your CI. The key thing to remember is that with a bigger sample size you are much more likely to get a sample mean \\(\\overline{x}\\) that is close to the population mean \\(\\mu\\). Thats because the sampling distribution of sample means is much tighter. In some ways the figure above is a bit misleading as all the sample means are at 7.72. What is more likely to be the case for many samples is that they will be closer to the true population mean. Look at the figure below, that compares samples sizes of 10 with sample sizes of 50. Larger samples lead to CIs that have a sample mean closer to \\(\\mu\\) and that are tighter - but still with a 95% chance of having captured \\(\\mu\\). 8.3 Confidence Intervals with t-distribution Hopefully the preceding sections on creating a confidence interval with the \\(z\\)-distribution helped you in understanding some of the theory about confidence intervals in general. Perhaps there is still one thought going through your mind - isnt all of this a bit strange? Why are we trying to estimate the population mean \\(\\mu\\) from the sample mean \\(\\overline{x}\\) when we also already know the population standard deviation \\(\\sigma\\)? How could you know \\(\\sigma\\) but not know \\(\\mu\\) - that makes no sense, and indeed it doesnt. It turns out that in the real world, that when we collect a sample of data and get our sample mean \\(\\overline{x}\\), and we want to create our confidence interval around it to have some certainty about where the population mean \\(\\mu\\) might lie, we also do not know \\(\\sigma\\). We need a backup plan for how to construct confidence intervals. This back up plan is making confidence intervals with the \\(t\\)-distribution. First, lets look at the formula for making a confidence interval with a \\(t\\)-distribution: \\(\\Large CI = \\overline{x} \\pm t \\times \\frac{s}{\\sqrt{n}}\\) Two things are different about this one compared to the formula for calculating a CI with the \\(z\\)-distribution. First, we are using a \\(t\\) value rather than a \\(z\\) value. Secondly, we are using the sample standard deviation \\(s\\) rather than the population standard deviation \\(\\sigma\\). If we do not know \\(\\sigma\\) then our next best option is to use our estimate of the population standard deviation, which is our sample standard deviation \\(s\\). We briefly introduced the \\(t\\)-distribution in section 7.5. Why do we need to use it here? Essentially, the key thing is that when we collect many sample means to create our sampling distribution of sample means, it is not always the case that this sampling distribution will be perfectly normally distributed. In fact, this is especially true for smaller sample sizes. If we collect smaller samples and calculate the sample mean of each, it turns out our sampling distribution will be slightly heavier in the tails than a normal distribution. How far away from normal our sampling distribution will be depends on our sample size. For bigger sample sizes, our sampling distribution will look more normal. This is illustrated below: Its important to remember that the shape of the \\(t\\)-distribution varies for different sample sizes. In fact, we actually state the distribution not in terms of the sample size, but in terms of the degrees of freedom. For instance, for a sample size of 15, we would say that the sampling distribution follows a \\(t\\)-distribution with a shape of degrees of freedom 14. The degrees of freedom is equal to \\(n-1\\) when describing sampling distributions of sample means. As a result of this issue, if we were to assume that our sampling distribution was normally distributed and used \\(z=1.96\\) to calculate our 95% confidence interval, we would be inaccurately determining where the middle 95% of the distribution was. In fact, for a \\(t\\)-distribution, because the tails are heavier, the value of \\(t\\) that leaves 2.5% in each tail will be a larger value than 1.96. Compare the standard normal curve below to the \\(t\\)-distribution for \\(df=14\\). As we can see, the value of \\(t\\) that leaves 2.5% in each tail is 2.145 which is higher than 1.96. Consequently, all else being equal, this will increase our margin of error. 8.4 Calculating a t-distribution Confidence Interval Lets look more practically at how we calculate a 95% confidence interval for a \\(t\\)-distribution. The formula we use is: \\(\\Large CI95\\% = \\overline{x} \\pm t \\times \\frac{s}{\\sqrt{n}}\\) First, lets grab a sample of size 15 from our population. We need to calculate the sample standard deviation and sample mean. set.seed(1) samp3 &lt;- sample(x, size = 15, replace = T) samp3 ## [1] 7.526290 8.190835 8.421406 7.249145 7.783075 8.226420 7.204423 7.836702 7.802225 7.326620 7.820477 ## [12] 8.123679 7.519023 7.649536 7.465758 mean(samp3) ## [1] 7.743041 sd(samp3) ## [1] 0.3731486 We can see that \\(\\overline{x}=7.74\\) and \\(s=0.373\\): Next, we need to calculate \\(t\\). This value will be the value that leaves 2.5% in the tails of a \\(t\\)-distribution for degrees of freedom = 14 (\\(n-1 = 14\\)). We can calculate that in R using the function qt(). We enter 0.975 to ask it to return the value of \\(t\\) that leaves 2.5% in the upper tail, and then we enter df=14 to ensure we are using the correct \\(t\\)-distribution: qt(p = 0.975, df = 14) ## [1] 2.144787 This shows us that our value of \\(t=2.145\\). We can now create our estimate of the standard error (the standard deviation of the sampling distribution of sample means), and our confidence intervals: #standard error sem1 &lt;- sd(samp3)/sqrt(15) sem1 ## [1] 0.09634654 # upper bound of confidence interval 7.74 + (2.144787 * sem1) ## [1] 7.946643 # lower bound of confidence interval 7.74 - (2.144787 * sem1) ## [1] 7.533357 Our confidence interval is therefore: \\(\\Large CI95\\% = 7.74[7.53,7.95]\\) Lets compare this to a 95% CI calculated with a \\(z\\)-distribution for this sample. # remember we calculated the standard error sem above in the z-distribution section 7.74 + (1.96 * sem) ## [1] 7.891821 7.74 - (1.96 * sem) ## [1] 7.588179 We can graphically compare these 95% confidence intervals Weve plotted the \\(t\\) confidence interval in purple and the \\(z\\) confidence interval in blue for this same sample. Notice that the confidence interval based on the \\(t\\)-distribution is a little wider than that based on the \\(z\\)-distribution. This is because we are using a higher value of \\(t\\) than of \\(z\\) in the equation. This is because we are assuming our sampling distribution is following the \\(t\\) shape rather than the classic \\(z\\) shape, and as the tails are heavier in a \\(t\\)-distribution, the value of \\(t\\) that leaves 2.5% in the tail is further away from 0. There is also one other detail that is a little hard to see from just the one sample above. That is that the size of the confidence intervals constructed using the \\(z\\)-distribution are fixed - i.e. they are always the same size. This is because the value of \\(z\\) is fixed (1.96 in this case) and the value of \\(\\sigma\\) is fixed - its always the same population standard deviation, which doesnt change. However, for confidence intervals made using the \\(t\\)-distribution, the size of these may change from sample to sample. This is because the sample standard deviation changes from sample to sample, meaning that not all confidence intervals will be the same length. We can illustrate this below. Here are 20 95% confidence intervals made using either the \\(z-\\) or \\(t\\)-distribution for 20 different samples of sample size \\(n=15\\). You can see that the \\(z\\)-distribution CIs are all equal in length, whereas the \\(t\\)-distribution ones vary from sample to sample. This is because of the use of the sample standard deviation \\(s\\) in the formula. Most of the time, because of the higher \\(t\\) value in the formula than the \\(z\\) value, it leads to the CIs being wider for those calculated with the \\(t\\)-distribution. This sometimes has important implications. For instance, notice the 9th sample down from the top. Using the \\(z\\)-distribution, this CI does not capture the true population mean \\(\\mu\\), but using the \\(t\\)-distribution does capture it. However, the CIs are not always bigger when using the \\(t\\)-distribution. Sometimes, the sample may just have very little variation in it meaning that the sample standard deviation \\(s\\) is very small. This could lead to a smaller margin of error - as seen with the 19th and 20th samples from the top in the figure. 8.4.1 t-distribution CIs and sample size. With the \\(z\\)-distribution based confidence intervals, when we increased the sample size \\(n\\), the margin of error always decreased because both \\(z\\) and \\(\\sigma\\) are fixed in the formula. For instance, for a 95% CI with a population standard deviation \\(\\sigma=10\\) and sample size \\(n=10\\) or \\(n=30\\), the margin of error using the \\(z\\)-distribution in the CI would be for each sample size: 1.96 * (10/sqrt(10)) ## [1] 6.198064 1.96 * (10/sqrt(30)) ## [1] 3.578454 Clearly, increasing the sample size reduces the margin of error. The situation is not as consistent when constructing confidence intervals with the \\(t\\)-distribution, although the general pattern remains true. When we collect samples of different sample sizes, two things change in the \\(t\\)-distribution confidence interval formula. Firstly, the value of \\(t\\) used is dependent upon the degrees of freedom. As sample sizes increase, the \\(t\\)- distribution becomes more normal shaped and less heavy in the tails. If, for example, we are interested in making 95% Confidence Intervals, then the value of \\(t\\) that leaves 2.5% in each tail (and 95% of the distribution in the middle) is going to get closer to 1.96 (and negative -1.96) as the sample size increases. This is illustrated in the figure below: Each of these \\(t\\) values can be calculated, by finding the value of \\(t\\) on the \\(t\\)-distribution for the respective degrees of freedom that leaves 2.5% in the upper tail: qt(.975, df = 9) ## [1] 2.262157 qt(.975, df = 19) ## [1] 2.093024 qt(.975, df = 29) ## [1] 2.04523 So, as sample size increases, the value of \\(t\\) decreases for a given confidence interval. This would seem to suggest that this would decrease the margin of error for the confidence interval. This is for the most part true, but not always. Remember the \\(t\\) value is multiplied by the estimated standard deviation of the sampling distribution (the standard error) which is \\(\\frac{s}{\\sqrt{n}}\\). Now again, it looks like increasing \\(n\\) would lead to a larger denominator and a smaller overall margin of error. This is also true. But, because we are using the sample standard deviation \\(s\\) in the formula to estimate the standard error, then \\(s\\) is going to vary from one sample to another. This means that for any given sample, we may actually end up with a wider confidence interval even if we increase our sample size. However, the main point remains - generally increasing your sample size, will lead to a tighter confidence interval for a given CI range. The final thing that is worth mentioning is a repeat of what is discussed above in the \\(z\\)-distribution section. Increasing sample sizes also leads to sample means that will be, on average, much closer to the true population mean \\(\\mu\\) than you get when using smaller sample sizes. 8.4.2 Other Confidence Intervals ranges for t-distribution Like with the confidence intervals made with the \\(z\\)-distribution, we can create confidence intervals for any range with the \\(t\\)-distribution. The rationale is the same. If we were to make an 80% CI around a sample mean, what we are effectively saying is that in 80% of all samples that we could collect, we would capture the true population mean \\(\\mu\\). Practically, we use a different value of \\(t\\) for each CI range. This value of \\(t\\) will be the positive and negative value of the \\(t\\)-distribution for a given degree of freedom that leaves the appropriate percentage in the middle of the distribution. For instance, for an 80% CI for a sample size of 25, which had degrees of freedom 24, the value would be \\(t=1.32\\). We calculated this as follows: For an 80% CI, we wish to have 80% of the distribution in the middle (40% either side of our sample mean), leaving 20% in the tails - i.e. 10% in each tail. Therefore, we wish to know the value of \\(t\\) that demarks this boundary. The easiest way to do that is to use the qt() function in R, and ask for the 90%th percentile (the value that leaves 10% in the upper tail) for a \\(t\\) distribution of degrees of freedom = 24. We do that like this: qt(0.90, df=24) ## [1] 1.317836 Thus, if we had a sample mean of \\(\\overline{x}=15.52\\), a sample standard deviation of \\(s=3.3\\) and a sample size of \\(n=25\\), then our 80% confidence interval of the true population mean \\(\\mu\\) would be \\(CI = 15.52[14.65, 16.39]\\): 15.52 + (1.32 * (3.3 / sqrt(25))) ## [1] 16.3912 15.52 - (1.32 * (3.3 / sqrt(25))) ## [1] 14.6488 The value of \\(t\\) used in the confidence interval formula therefore changes based on both your confidence interval size, and your degrees of freedom. Below are some other values of \\(t\\) that would be used for different sample sizes and CI ranges: # 99% CI, n = 20 qt(.995, df = 19) ## [1] 2.860935 # 90% CI, n = 12 qt(.95, df = 11) ## [1] 1.795885 # 99.9% CI, n = 35 qt(.9995, df = 34) ## [1] 3.600716 8.5 Comparing CIs using the z- and t-distributions You might be thinking that using the \\(t\\)-distribution to make 95% confidence intervals seems like a lot of extra legwork to figure out what value of \\(t\\) to use, compared to just using \\(z=1.96\\) when using the \\(z\\)-distribution. This mini section hopefully is an illustration of why you have to do this. These are the two formulas that we use to generate confidence intervals: \\(\\Large CI_{95\\%} = \\overline{x} \\pm z \\times \\frac{\\sigma}{\\sqrt{n}}\\) \\(\\Large CI_{95\\%} = \\overline{x} \\pm t \\times \\frac{s}{\\sqrt{n}}\\) But, what if we did just decide to use \\(z\\) when we dont know the population standard deviation \\(\\sigma\\) and we used this formula: \\(\\Large CI_{95\\%} = \\overline{x} \\pm z \\times \\frac{s}{\\sqrt{n}}\\) Well, in the figure below, we did just that for 25 sample means collected from samples of size \\(n=8\\) from a population with a mean of \\(\\mu=4\\) and standard deviation of \\(\\sigma=1.5\\). As you can see, the first and third columns that are using the appropriate \\(z\\)- and \\(t\\)-distribution formulas have 23/25 confidence intervals that include the true population mean. In fact, out of 1000 simulations of these data (i.e. 1000 sample means collected) precisely 95% of confidence intervals included the population mean for both, which is what we would expect. Conversely, the middle column includes the confidence intervals calculated using \\(z=1.96\\) and using \\(s\\) as an estimate of the population standard deviation. With this formula, 6/25 confidence intervals fail to include the true population mean. Out of the 1000 simulations of the data, actually 9.6% of CIs failed to capture the true population mean. This shows that the margin of error calculated using this formula is consistently too small. The reason for this is that when we estimate the population standard deviation, we generally are under-estimating the true value. This is another reason why we should be using the \\(t\\)-distribution. "],["hypothesis-testing.html", "Chapter 9 Hypothesis Testing 9.1 Two-tailed and One-tailed tests 9.2 Examples of 1- and 2-tailed tests 9.3 Significance Levels and p-values", " Chapter 9 Hypothesis Testing Descriptive statistics allows you to describe or summarize your data, whereas inferential statistics allows you to make predictions or draw inferences from the data. In particular, we often wish to make predictions about our data samples. For instance, if we collected a sample of 12 Granny Smith apples, we may wish to predict if they came from a population of Granny Smith apples with a particular mean weight using a statistical test. This sort of procedure is called hypothesis testing or significance testing. A statistical hypothesis is a clear prediction to specific claims about a population parameter (almost always in intro stats a population mean, or the difference between the means of two populations). This prediction may or may not be true. There are two types of statistical hypotheses. For one-sample tests they would be as follows: Null Hypothesis (H0): a statistical hypothesis that states there is no difference between a parameter (e.g. population mean) and a specific value. Alternative Hypothesis (H1): a statistical hypothesis that states there is a meaningful difference between a parameter (e.g. population mean) and a specific value For two-sample tests (see chapter 11) they would be as follows: Null Hypothesis (H0): a statistical hypothesis that states there is no difference between two parameters e.g. population means. Alternative Hypothesis (H1): a statistical hypothesis that states there is a meaningful difference between two parameters e.g. population means. Lets think of an example. Say we predicted that a species of bird, a Silvereye, had beaks that were 11mm long. Were essentially making the prediction that the population mean of Silvereye beaks is \\(\\mu=11\\). We therefore would make our null hypothesis that the population mean (parameter) of this species beak length was 11mm. We would then make the alternative hypothesis that the population mean beak length was not equal to 11mm. We can write those like this using shorthand abbreviations: \\(H_0: \\mu = 11.0\\) \\(H_1: \\mu \\ne 11.0\\) We would then collect a sample of data and conduct a significance test to determine if we have sufficient evidence, or not, to suggest that the population mean was likely to be 11mm. If we have sufficient evidence from our sample to suggest that the population mean is indeed highly unlikely to be 11mm then we reject the null hypothesis and accept the alternative hypothesis. If the null hypothesis is rejected then we must accept the the alternative hypothesis is true. These hypotheses are always stated together. The other outcome is that our sample of data does not provide us with sufficient evidence to reject the null hypothesis. In this case, we fail to reject the null hypothesis. This isnt quite the same as saying we accept it - it just means that it is a likely prediction and we dont have enough evidence to say that its wrong. An important point to consider here is what constitutes sufficient evidence?. Well typically we should use two pieces of information together. First, we can run a significance test and we will obtain a p-value for that test. We discuss more about p-values below, but essentially if we get a p-value of &lt;0.05 then we generally say that is sufficient evidence that our test is significant and we can reject the null hypothesis. If the p-value is &gt;0.05, then we say we fail to reject the null hypothesis. Second, we can use confidence intervals (see chapter 8). 9.1 Two-tailed and One-tailed tests Two-tailed tests The above example is a type of significance test called a two-tailed test. Here, we made a prediction about the parameter (the population mean in this case) for the null hypothesis. For the alternative hypothesis we did not make a prediction as to whether it would be higher or lower than the predicted value. In the test that we would carry out, we will account for this when calculating our probability of how likely the parameter is that specific value. We call these tests two-tailed (as will become evident later in the chapter). (Essentially we just double the p-value for the one-tailed tests - see below). One-tailed tests When we do make a prediction as to the direction of the alternative hypothesis, we term these tests one-tailed tests. For instance, we could have suggested for the alternative hypothesis that that the population mean bill length was larger than 11mm. This would mean that the null hypothesis would be that the bill length was 11mm or less. We would write these out like this: \\(H_0: \\mu \\le 11.0\\) \\(H_1: \\mu \\gt 11.0\\) Practically, the p-value that we derive from one-tailed significance tests will be half the value of the p-value of that calculated with the same data but doing a two-tailed test. We discuss this more in chapters 10 and 11. Alternative hypotheses that test whether some value is higher than a parameter are sometimes called right-tailed tests. Alternative hypotheses that test whether some value is less than a parameter are sometimes called left-tailed tests. But I dont really like these terms - I think just calling them both one-tailed is better. It is important to have specific predictions before doing significance testing, and to state ahead of time whether you will do a one-tailed or two-tailed test. 9.2 Examples of 1- and 2-tailed tests Given the above information, consider why the null and alternative hypotheses for the following experiments are as they are. Biologists are interested in determining whether mice given a dopamine infusion directly into the nucleus accumbens (a specific region of the brain) show an increase in aggression above the standard 3 aggressive interactions per minute. The biologists hypotheses are: H0: \\(\\mu \\le 3\\) H1: \\(\\mu &gt; 3\\) A botanist is interested in determining whether sunflower seedlings treated with an Epsom salt fertilizer resulted in a lower average height of sunflower seedlings than the standard height of 23 cm. The botanists hypotheses are: H0: \\(\\mu \\ge 23\\) H1: \\(\\mu &lt; 23\\) An ecologist claims that the migration distance of the Arctic tern is 22,000 km. Another group of ecologists regularly check this claim. The ecologists hypotheses are: H0: \\(\\mu = 22,000\\) H1: \\(\\mu \\ne 22,000\\) 9.3 Significance Levels and p-values In the sections below, we will introduce different types of significance tests. These differ in how they are implemented but they all are based on the principle of determine how likely it was for your sample data to have come from a population with specific characteristics/parameters. Each time we run a significance or hypothesis test, we also need to set a significance or alpha level, symbolized as \\(\\alpha\\). By convention, this tends to be \\(\\alpha = 0.05\\), or a 95% significance level. There is a long history of why this value was chosen and why it may or may not be appropriate for different types of experiments - but for now, well go with it as an example. What we will do with each test is to generate a p-value. This p-value is a measure of the likelihood or probability of our sample data coming from a population with certain parameters (e.g. a mean equal to 11mm, a mean less than or equal to 11mm).If this p-value is very, very small then we say, wow, it was really unlikely for our sample data to have come from a population with that parameter. The question becomes, how small is small enough for a p-value? This is where we use the convention of 0.05. If our p-value is lower than 0.05 we reject the null hypothesis and accept the alternative. If our p-value is higher than 0.05 we fail to reject the null hypothesis. The above is a general overview of the decision making process: make hypotheses about population parameter(s) test sample data against hypotheses get p-value determine if p-value is lower than alpha level (usually 0.05) if p-value lower than alpha level, reject null, accept alternative if p-value higher than alpha level, fail to reject null However, we do urge caution in following this pattern too dogmatically. In this course, we do use p-values to gain insights into whether samples of data are meaningfully different from population parameters, or in chapter 10 whether the inferred population parameters of two samples are different from each other - but p-values and significance tests are just one method. They should be used in conjunction with confidence intervals, effect sizes and other tools to gain insight into our data. Further, should 0.05 really be the alpha level that is appropriate for your data? This can be a hard question sometimes to answer, and people tend to ignore it and just use it regardless. Another important caveat is that even when we collect awesome sample data, there is always a chance that we just wont have sample data that is reflective of the population. For example, say that our Silvereye bird population actually had a mean beak length of 11.5mm. We might take 20 samples of 10 birds and measure their beaks and run a significance test and find in 19 of these samples a p-value of less than 0.05 suggesting that the population mean was not equal to 11mm. However, by chance, in one of our samples we might get a p-value of greater than 0.05 and fail to reject the null (even though its not true). We just have to accept with these kinds of inferential statistics that by chance these situations may occur. In fact there are two types of errors we could make here. The first error is called Type I error. This is when you reject the null hypothesis when it is actually true (i.e. a false positive). The other error is Type II error. This is when you do not reject the null hypothesis when it is false (i.e. a false negative). With an alpha level of 95%, we are essentially saying that we have a 1 in 20 chance of producing a type II error. This might be fine for some studies (e.g. measuring bird beak lengths) but be a real problem for other studies. "],["one-sample-inferential-statistics.html", "Chapter 10 One Sample Inferential Statistics 10.1 One-sample z-tests 10.2 One-sample t-tests 10.3 Conducting one-sample t-tests in R 10.4 Assumptions of the one-sample t-test", " Chapter 10 One Sample Inferential Statistics The general question at hand with one-sample inferential tests, is that we wish to test the probability that our one sample of data comes from a population that has a true population mean \\(\\mu\\) that is equal to, greater than, or less than, some specific value. We will look at two ways of doing this - firstly using a z-test and then using t-tests. 10.1 One-sample z-tests 10.1.1 Sampling Distribution Recap In one-sample z-tests we are provided with the population mean \\(\\mu\\) and standard deviation \\(\\sigma\\). We then collect or are given one sample of data of size \\(n\\). From this sample, we calculate the sample mean \\(\\overline{x}\\). The question then becomes, how likely were we to get a sample mean as large or as small as the sample that we got? To answer this, we need to think in terms of the sampling distribution of sample means. We need to recognize that our one observed sample mean \\(\\overline{x}\\) is just one sample mean that we could have got from a sampling distribution of sample means. For instance, look at the population below. This is a normally distributed population of IQ scores, with a population mean \\(\\mu = 100\\) and a population standard deviation \\(\\sigma = 15\\). Lets take a sample of size \\(n=25\\) from this population, round the individual scores to 1dp, and get the mean of the sample: set.seed(1) samp1 &lt;- round(rnorm(n=25, mean=100, sd=15),1) samp1 ## [1] 90.6 102.8 87.5 123.9 104.9 87.7 107.3 111.1 108.6 95.4 122.7 105.8 90.7 66.8 116.9 99.3 99.8 ## [18] 114.2 112.3 108.9 113.8 111.7 101.1 70.2 109.3 mean(samp1) ## [1] 102.532 Our one observed sample has a sample mean of \\(\\overline{x}=102.5\\) If we repeated this step and got a second sample of \\(n=25\\), we could get another sample mean \\(\\overline{x}\\): samp2 &lt;- round(rnorm(n=25, mean=100, sd=15),1) samp2 ## [1] 99.2 97.7 77.9 92.8 106.3 120.4 98.5 105.8 99.2 79.3 93.8 94.1 99.1 116.5 111.4 97.5 96.2 ## [18] 110.5 108.3 89.7 89.4 105.5 111.5 98.3 113.2 mean(samp2) ## [1] 100.484 This time the sample mean is \\(\\overline{x}=100.5\\). If you remember back to section 7.2, if we were to repeat this process thousands and thousands of times, we would get a sampling distribution of sample means. We could visualize all of our sample means from many thousands of samples in a histogram, which shows the shape of the sampling distribution: Because of Central Limit Theorem (see section 7.3) then this sampling distribution is normally distributed and its mean \\(\\mu_{\\overline{x}}\\) is equal to the population mean \\(\\mu\\). Therefore \\(\\mu_{\\overline{x}}=10.0\\). We also know the standard deviation of this sampling distribution, also known as the standard error as it can be calculated by: \\(\\Large \\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\). Therefore, the sampling distribution standard deviation is \\(\\sigma_{\\overline{x}}=3.0\\): sem &lt;- 15 / sqrt(25) sem ## [1] 3 Because this sampling distribution is normally distributed, we can determine how far away from the mean any sample mean is in terms of how many standard deviations from the mean they are. For instance, our first sample we got a mean of \\(\\overline{x}=102.5\\). How many sampling distribution standard deviations is this from the mean of the sampling distribution? We can use an amended z-score formula (see section 7.1) to determine this: \\(z = \\frac{\\overline{x} - \\mu_{\\overline{x}}}{\\sigma_{\\overline{x}}}\\) (102.5 - 100.0) / 3 ## [1] 0.8333333 So, our sample mean of \\(\\overline{x}=102.5\\) is 0.833 standard deviations above the mean. Because our sampling distribution is normally distributed, then we can visualize how far above the mean this value is on the standard normal curve as well as on the sampling distribution: If we were asked what proportion of sample means were at least as big as 102.5, then wed be interested in knowing what proportion of sample means are to the right of the red lines above. We can calculate the area under a standard normal curve to the left of any z value in R using pnorm(): pnorm(0.833) ## [1] 0.7975776 To find the proportion of the curve to the right of the red line (i.e. the proportion of sample means that are greater than 102.5), we just subtract this value from 1. 1 - pnorm(0.833) ## [1] 0.2024224 So 20.2% of samples have a sample mean greater than 102.5. In this situation, our one sample mean was not therefore that unusual or surprising. 10.1.2 Calculating p-values for z-test One-tailed tests We can calculate the proportion of sample means that are greater or less than any value. For instance, if we were interested in whether a new reading program in a school boosted the IQ of subjects. We might take a sample of 25 of these students and measure their IQ. If we got a sample mean of \\(\\overline{x}= 105.7\\), we may wish to test whether this value is surprisingly large given that the population of IQ has a \\(\\mu=100.0\\) and \\(\\sigma=15.0\\). If we were to formally write this in hypothesis terms, it would look like this: \\(H_{0}: \\mu \\le 100.0\\) \\(H_{1}: \\mu &gt; 100.0\\) This is saying that the alternative hypothesis \\(H_{1}\\) is that our sample of 25 come from a population whose mean is greater than 100.0. The null hypothesis that we are testing is that they come from a population whose mean is equal to or less than 100.0. For the test, we assume with the null hypothesis that our sample did indeed come from a population with \\(\\mu=100.0\\) and \\(\\sigma=15.0\\). We already calculated above that the sampling distribution of sample means for \\(n=25\\) has a \\(\\mu_{\\overline{x}}=100.0\\) and \\(\\sigma_{\\overline{x}}=3.0\\). How unusual is our one observed sample mean of \\(\\overline{x}=105.7\\)? We need to calculate this in terms of z: \\(\\Large z = \\frac{\\overline{x} - \\mu_{\\overline{x}}}{\\sigma_{\\overline{x}}}\\) (105.7 - 100.0) / 3 ## [1] 1.9 This suggests that our observed sample mean is 1.9 sampling distribution standard deviations away from the mean of the sampling distribution. We next need to work out what proportion of sample means are greater than this. That is akin to the red shaded area below: We can do this with pnorm(): 1 - pnorm(1.9) ## [1] 0.02871656 This shows that only 2.9% of sample means are greater than our observed sample mean, given the population data of \\(\\mu=100.0\\) and \\(\\sigma=15.0\\). Therefore, our observed sample mean is quite surprising. We can write the likelihood of getting this sample mean as p = 0.029. Because we a priori had a prediction as to the direction of the mean in our sample (we predicted it to be higher than the population mean of 100.0), then we have in fact just done a one-tailed test. If we decide that any p-values that are below p=0.05 are significant, then we can say that our reading program in the school has a significant effect on improving IQ scores. Two-tailed tests What instead we had implement a reading program that was quite radical, and we were not sure whether it would be successful or not. We were interested in seeing whether it could increase or decrease IQ? In this situation, we do not have a direction of prediction, and we set up our hypotheses slightly differently: \\(H_{0}: \\mu = 100.0\\) \\(H_{1}: \\mu \\ne 100.0\\) Here, we are interested in whether our observed sample mean of \\(\\mu=105.7\\) could have come from a sampling distribution with a population mean of $_{=100.0} or not. The initial steps are the same. We calculate how unusual our observed sample mean was in terms of standard deviations away from the mean of the sampling distribution: (105.7 - 100.0) / 3 ## [1] 1.9 Its still 1.9 standard deviations away. What we have to now thing about, is that because we did not predict the direction of the difference, we have to double our p-value. This is to account for the fact that in terms of surprising results, we are interested in results that are more extreme than 1.9 standard deviations either side of the mean - i.e. the sum of the shaded area below: Therefore, our p-value for this test is: 2 * (1 - pnorm(1.9)) ## [1] 0.05743312 Which is p=0.057. This would suggest that our reading program did not sufficiently shift the population IQ scores in our school away from 100.0, as our p-value is greater than 0.05. But is this really true? It is important to consider two things here. One, it is important what initial hypothesis that you set up. If you have a strong a priori belief in the directionality of the hypothesis, then you are justified in doing a one-tailed z-test and using that p-value from that. Secondly, the difference between p=0.029 and p=0.057 in this case isnt that great. We shouldnt be overly focused on the cut-off value of p=0.05 as the criteria as to whether our results are significant or not significant. We should see the bigger picture, that our p-value is just one piece of information as to how different our sample of data is from the population that we believe it came from. 10.1.3 Using critical values In the preceding section, we ran one-tailed and two-tailed z-tests and calculated exact p-values. It is reasonably straightforward to do this in R. There is no reason not to use that approach. We prefer it. However, most often in introductory textbooks, a different approach is used. In this approach, the step of calculating the p-value is missed out. Instead, you are asked to just determine whether your observed z-value is more extreme than youd expect by chance. By chance in this context means, that your observed z-value is less than 5% likely to occur. One-tailed z-test The population mean for SAT scores is \\(\\mu=500\\) with a population standard deviation \\(\\sigma=100\\). A tutoring company says that they improve SAT scores. A random sample of 12 students who took the tutoring program had a sample mean of \\(\\overline(x)=551\\). Lets test whether this sample mean came from the population. \\(H_{0}: \\mu \\le 500.0\\) \\(H_{1}: \\mu &gt; 500.0\\) Next, we calculate the mean and standard deviation of the sampling distribution for a sample size \\(n=12\\). For the test, we assume that the sampling distribution mean \\(\\mu_{\\overline{x}}=500.0\\) - i.e. is the same as the population mean. The standard deviation of the sampling distribution \\(\\sigma_{\\overline{x}} = 28.87\\): 100 / sqrt(12) ## [1] 28.86751 Next, we work out how many sampling distribution standard deviations from the sampling distribution mean is our observed sample: (551 - 500) / 28.86751 ## [1] 1.766692 This shows that our observed sample mean of \\(\\overline{x}=551\\) is 1.77 standard deviations above the mean of the sampling distribution. We could convert this to a p-value and calculate precisely how many sample means are larger than this in the sampling distribution. Instead, well take a different approach: Because this sampling distribution is approximately normal, we can determine how many standard deviations above the mean youd have to be to be larger than 95% of all samples. It turns out that \\(z=1.645\\) is the value of \\(z\\) that leaves 5% in the right hand tail. Therefore, any value of \\(z\\) greater than \\(z=1.645\\) will be unusually large and have a p-value of less than 0.05. If we were dealing with sample means that were surprisingly small (so a one-tailed test where we are predicting that the sample mean comes from a population with a mean that is smaller than the population mean), then we are looking for \\(z\\) values that are lower than \\(z=-1.645\\). We can work out where these critical values are in R using the qnorm() function: qnorm(0.95) # leaves 5% in right of tail ## [1] 1.644854 qnorm(0.05) # leaves 5% in left of tail ## [1] -1.644854 If we get back to our observed \\(z\\) value of \\(z=1.77\\), we can overlay this over the graph above like this: As you can see, our observed \\(z\\) value is more extreme than the critical value of \\(z=1.645\\). We say that our observed value is therefore in the region of rejection and we can therefore reject the null hypothesis with a p-value of \\(p&lt;0.05\\) and accept the alternate hypothesis that our sample comes from a population with a population mean that is greater than 500. In other words, the tutoring program appears to have a population mean of SAT scores greater than 500. Two-tailed z-test With the one-tailed z-test, we see that our critical values of z are \\(z=1.645\\) for situations in which we are testing whether our sample mean is unexpectedly large, or \\(z=-1.645\\) for situations in which we are testing that our sample mean is unexpectedly small. In a two-tailed situation, we are testing whether our sample mean is unexpectedly large or small. Because we still want only 5% of sample means to be in this unexpectedly large or small category, this time we need a value of \\(z\\) that leaves a total of 5% in the ends of both tails of the normal distribution. This is the same as leaving 2.5% in each tail: qnorm(.975) # leaves 2.5% in right tail ## [1] 1.959964 qnorm(.025) # leaves 2.5% in left tail ## [1] -1.959964 Lets illustrate this a bit further with the following example. Say we have a bakery that makes cupcakes. The population mean weight of cupcakes is \\(\\mu=6.5\\) ounces, with a standard deviation of \\(\\sigma=0.15\\) ounces. A customer wants to test if a new cupcake variety is heavier or lighter than 6.5 ounces. They purchase a random sample of 10 cupcakes and find that the sample mean is \\(\\overline(x)=6.42\\) ounces. If we were to conduct a two-tailed test, to test if the population mean that our sample come from is equal to 6.5 ounces or not, then our hypotheses would be: \\(H_{0}: \\mu = 6.5\\) \\(H_{1}: \\mu \\ne 6.5\\) We need to calculate the z-score for our sample mean, to determine how many standard deviations of the sampling distribution it is away from the mean. sem &lt;- 0.15 / sqrt(10) #standard deviation of the sampling distribution z &lt;- (6.42 - 6.5) / sem # how many SD away from the mean is our sample z ## [1] -1.686548 We can overlay this observed value of \\(z\\) onto our standard normal curve like this: Our value of \\(z=-1.69\\) is therefore not inside either of the regions of rejection. This means that we do not have sufficient evidence to reject the null hypothesis that our sample comes from a population with mean of equal to 6.5. 10.2 One-sample t-tests As with confidence intervals based on the \\(z\\)-distribution (the standard normal curve), the major issue with z-tests is that they require you to know the population mean \\(\\sigma\\) to perform the calculations. This is almost never the case - with exceptions like standardized tests including IQ and SAT that are designed to have specific means and standard deviations. If you wish to test whether your observed sample mean is likely or not to come from a population with a given mean, what approach should you take when you do not know \\(\\sigma\\)? As with confidence intervals, the approach we take is to use the \\(t\\)-distribution. In this situation, because we dont know our population standard deviation \\(\\sigma\\), we have to estimate it using the sample standard deviation \\(s\\). Further, because our sampling distribution may not be precisely normal given this estimation, we say that it comes from a \\(t\\)-distribution. \\(t\\)-distributions have slightly heavier tails than the normal distribution. One-tailed t-test example Lets illustrate the steps we take in a one-sample t-test with an example. These steps are identical to the two-tailed t-test up until we calculate the p-value. The population mean number of words spoken by two year olds by their 2nd birthday is \\(\\mu=50\\) words and this is normally distributed. We dont know the population standard deviation \\(\\sigma\\). A researcher wanted to investigate if reading to children increases their word knowledge. They collected data from 12 children (\\(n=12\\)) who were read to for at least two hours every day. These are the number of words spoken by the 12 children: x &lt;- c(45, 53, 71, 35, 51, 59, 49, 55, 78, 27, 66, 59) x ## [1] 45 53 71 35 51 59 49 55 78 27 66 59 mean(x) ## [1] 54 Our one observed sample mean \\(\\overline{x}=54\\). This is higher than 50, but is it meaningfully higher? If we were to formalize our hypothesis for this test, we would write: \\(H_{0}: \\mu \\le 50.0\\) \\(H_{1}: \\mu &gt; 50.0\\) We are testing whether our sample was likely to have come from a population with mean 50 or less (null hypothesis), or if it was more likely to come from a population with a mean of greater than 50. The first step is to think about the sampling distribution. We need to recognize that our one observed sample mean is just one sample mean that we theoretically could have got from a sample distribution. Under the null hypothesis, we are going to assume that the mean of our sampling distribution \\(\\mu_{\\overline{x}}\\) is equivalent to the population mean \\(\\mu\\). Therefore, \\(\\mu_{\\overline{x}}=50.0\\) Next, we need to calculate the standard deviation of the sampling distribution of sample means for n=12 (i.e. the standard error). As we do not know the population standard deviation \\(\\sigma\\), we estimate this by using the following formula: \\(\\Large \\sigma_{\\overline{x}} = \\frac{s}{\\sqrt{n}}\\) Therefore, our estimate of the standard error is \\(\\sigma_{\\overline{x}}=4.14\\): sem &lt;- sd(x) / sqrt(12) sem ## [1] 4.143268 Now that we know both the mean and standard deviation of the t-shaped sampling distribution, next we need to calculate how many standard deviations from this mean is our one observed sample mean. We calculate that using the formula that is similar to the \\(z\\) formula: \\(\\Large t = \\frac{\\overline{x} - \\mu_{\\overline{x}}}{\\sigma_{\\overline{x}}}\\) Our sample mean of \\(\\overline{x}=54\\) has a \\(t\\) value of \\(t = 0.965\\): (54-50) / sem ## [1] 0.9654216 This means that it is approximately 0.965 standard deviations higher than the mean. We can calculate the proportion of sample means in the sampling distribution that are higher than our one observed sample mean by calculating the area under the curve to the right of our \\(t\\)-value. Remember, that our sampling distribution is \\(t\\)-shaped and has 11 degrees of freedom. The degrees of freedom are \\(df = n-1\\) for a one-sample t-test. To determine what proportion of the curve lies to the right of \\(t=0.965\\) we can use pt() which calculates the proportion to the left of the given value. pt(0.965, df=11) ## [1] 0.8223595 So, this tells us that for a \\(t\\)-distribution with 11 degrees of freedom, that 82.2% of values are to the left (lower than) of this value. That means that 17.8% of values are to the right, or 17.8% of sample means that could be drawn from the sampling distribution with a mean of 54 and standard deviation of 4.14 will be higher than our observed sample mean. 1 - pt(0.965, df=11) ## [1] 0.1776405 We can also get this value by setting lower.tail = FALSE: pt(0.965, df=11, lower.tail = FALSE) ## [1] 0.1776405 Because we made a prediction as to the direction of the hypothesis - i.e. we predicted that reading to children would increase the population mean, we are effectively running a one-tailed test. Our p-value is simply \\(p=0.177\\). As we use an alpha level of 0.05 (a p-value of 0.05 as our critical value), this suggests that we do not have sufficient evidence to suggest that our sample of 12 children have a mean value that comes from a population with a mean that is greater than 50. We do not reject our null hypothesis. Two-tailed t-test example There are 20 psychology students in Dr. Zeppos class. Here are their scores on a test, as well as their sample mean \\(\\overline{x}\\) and sample standard deviation \\(s\\): zeppo &lt;- c(50,60,60,64,66,66,67,69,70,74,76,76,77,79,79,79,81,82,82,89) zeppo ## [1] 50 60 60 64 66 66 67 69 70 74 76 76 77 79 79 79 81 82 82 89 length(zeppo) # 20 - there are 20 students in the sample. ## [1] 20 mean(zeppo) # the mean of the sample is 72.3 ## [1] 72.3 sd(zeppo) # the sample SD is 9.52 ## [1] 9.520615 Historically, students in this class get a score of 65 points on this test - that is the population mean \\(\\mu\\). Dr Zeppo wishes to test if this one sample (one class) has a mean (technically comes from a population with a mean) that is different to 65. We would write out this hypothesis like this: \\(H_{0}: \\mu = 65.0\\) \\(H_{1}: \\mu \\ne 65.0\\) As with all of these tests, our first job is to recognize that our one sample mean is just one sample mean that we theoretically could have got from lots of samples of size \\(n=20\\). All of those sample means together are referred to as the sampling distribution of sample means. Under the null hypothesis, we assume that the mean of the sampling distribution of sample means \\(\\mu_{\\overline{x}}=65\\), i.e. it is equivalent to the population mean \\(\\mu\\). Next, we have to estimate the standard deviation of the sampling distribution of sample means, i.e. the standard error \\(\\sigma_{\\overline{x}}\\). We do this using the same formula as before: \\(\\large \\sigma_{\\overline{x}} = \\frac{s}{\\sqrt{n}}\\) So, our standard error is \\(\\sigma_{\\overline{x}} = 2.13\\): sem &lt;- sd(zeppo) / sqrt(20) sem ## [1] 2.128874 Following this, we need to calculate how expected or unexpected our one sample mean \\(\\overline{x}\\) was. We do this by calculating it in terms of how many sampling distribution standard deviations is it away from the sampling distribution mean. We use the formula: \\(\\Large t = \\frac{\\overline{x} - \\mu_{\\overline{x}}}{\\sigma_{\\overline{x}}}\\) (mean(zeppo) - 65) / sem ## [1] 3.429042 Our observed sample mean of \\(\\overline{x} = 72.3\\) is 3.43 sampling distribution standard deviations from the sampling distribution mean. We can picture this as follows: To calculate our p-value for our 2-tailed t-test, we need to calculate not just the area underneath the curve with values of \\(t\\) greater than our observed \\(t=3.43\\), but also the values under the curve with \\(t\\) values that are more negative than \\(t=-3.43\\). This is because in a 2-tailed test, we need to test for the probability of getting a \\(t\\)-value as large in both directions. Our p-value can be calculated using pt(): pt(3.43, df = 19, lower.tail = F) + pt(-3.43, df = 19) ## [1] 0.002807258 Or alternatively, we could just multiply those values greater than \\(t=3.43\\) by 2: pt(3.43, df = 19, lower.tail = F) * 2 ## [1] 0.002807258 Either way, we can see that our p-values is \\(p = 0.003\\), which tells us that our observed mean of \\(\\overline{x}=72.3\\) is quite unlikely to have come from a distribution with a population mean \\(\\mu=65\\). This leads us to rejecting our null hypothesis and accepting the alternative, that these psychology students come from a population with a mean that is greater than 65. 10.2.1 Critical values for the one-sample t-test As with the z-test (see section ??, instead of calculating the p-values for our observed values of \\(t\\), you can simply test whether your value of \\(t\\) exceeds (in either the positive or negative direction) some critical value of \\(t\\). Again, this approach was more often taken when it wasnt as easy to run computers to do these tests, so it seems a bit obsolete to do it this way. We recommend just doing it the way outlined above. Nevertheless, just for completeness, here is how to calculate these critical values of \\(t\\). Well use data from a different dataset. Here, we have the times taken to complete a crossword puzzle. We have a sample of \\(n=10\\) subjects. library(tidyverse) xt &lt;- read_csv(&quot;data/crosstimes.csv&quot;) xt$time3 ## [1] 15.04036 15.38213 15.70967 14.71215 12.18972 19.90511 20.00015 12.45357 12.86255 12.82682 Lets say we wish to test whether this sample comes from a population with a mean of less than 16.0. We would be doing a one-tailed test and our hypotheses would look like this: \\(H_{0}: \\mu \\ge 16.0\\) \\(H_{1}: \\mu &lt; 16.0\\) When using critical values, all the steps up to completing the observed \\(t\\)-value are the same as before. So, we assume that our one sample mean \\(\\overline{x}\\) comes from a sampling distribution of sample means that has a mean equivalent to the population mean of 16. The standard deviation of this sampling distribution is: sem &lt;- sd(xt$time3) / sqrt(10) sem ## [1] 0.9027857 Next, we calculate our observed \\(t\\) which is a measure of how many sampling deviation standard deviations our observed sample mean is away from the mean of the sampling distribution: (mean(xt$time3) - 16) / sem ## [1] -0.9878071 Our observed \\(t\\) values is \\(t = -0.99\\). As we are conducting a one-tailed t-test, we need to think in terms of what value of \\(t\\) leaves 5% in the tail for a t-distribution with 9 degrees of freedom. This is visualized below: If our observed sample mean with a \\(t\\)-value of -0.99 was unexpectedly small, then it would need to be in the region of rejection (red shaded area). This would mean that it was in the bottom 5% of sample means from such a distribution. The value of \\(t\\) that is the boundary of the the lower 5% can be calculated using the qt() function like this: qt(0.05, df=9) ## [1] -1.833113 Two-tailed tests For this example, lets use the penguins data. Say we are interested in the flipper length of male Adelie penguins on Biscoe island (a bit specific, but lets go with it), and wanted to know if their mean length was different from \\(\\mu = 188\\). Our hypotheses would be: \\(H_{0}: \\mu = 188.0\\) \\(H_{1}: \\mu \\ne 188.0\\) From our data, we can calculate our \\(n\\) and sample mean \\(\\overline{x}\\): ### Read in the Data Penguins penguins &lt;- read_csv(&quot;data/penguins.csv&quot;) # just look at the females. adelie &lt;- penguins %&gt;% filter(species == &quot;Adelie&quot;, sex == &quot;MALE&quot;, island == &quot;Biscoe&quot;) nrow(adelie) ## [1] 22 mean(adelie$flipper_length_mm) ## [1] 190.4091 We can also represent these data as a boxplot: ggplot(adelie, aes(x = 0, y = flipper_length_mm)) + geom_boxplot() + geom_jitter(width = .1, size=2)+ theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) + coord_flip() + geom_hline(yintercept = 187, color=&quot;red&quot;) Our sample of penguins has a sample size of \\(n=22\\), with a sample mean of \\(\\overline{x}=190.4\\). Lets calculate our observed value of \\(t\\) for this sample: sem &lt;- sd(adelie$flipper_length_mm) / sqrt(22) tobs &lt;- (mean(adelie$flipper_length_mm) - 188) / sem tobs ## [1] 1.748218 This means that our observed sample mean is 1.75 sample standard deviations above the sampling distribution mean. In terms of critical regions, for a 2-tailed test, we need to know the values of \\(t\\) that leave 2.5% in each tail for a \\(t\\)-distribution with 21 degrees of freedom. qt(.975, df = 21) ## [1] 2.079614 qt(.025, df = 21) ## [1] -2.079614 For a \\(t\\)-distribution with 21 degrees of freedom, the top 2.5% of t-values are greater than \\(t=2.08\\), whilst the lowest 2.5% of t-values are below \\(t=-2.08\\). Therefore, for us to reject the null hypothesis, our observed t-value needs to be higher than 2.08 or lower than -2.08. That would leave it in the region of rejection (red shaded areas above). Our observed t-value is \\(t=1.75\\) which is not in these areas, so we cannot reject the null hypothesis. We do not have sufficient evidence to suggest that our penguins come from a population with a mean of \\(\\mu = 188\\). 10.3 Conducting one-sample t-tests in R Conducting one-sample t-tests in R is very straightforward. First, lets consider the sample of 12 two-year olds and their word scores. x &lt;- c(45, 53, 71, 35, 51, 59, 49, 55, 78, 27, 66, 59) mean(x) ## [1] 54 To test whether this sample mean is likely to have come from a population with a mean greater than 50, we use t.test() in the following way: t.test(x, mu = 50, alternative = &quot;greater&quot;) # one-tailed test ## ## One Sample t-test ## ## data: x ## t = 0.96542, df = 11, p-value = 0.1775 ## alternative hypothesis: true mean is greater than 50 ## 95 percent confidence interval: ## 46.55917 Inf ## sample estimates: ## mean of x ## 54 mu specifies the mean that we are testing against. alternative = \"greater\" states that it is a one-tailed test, where we are testing the prediction that the population mean is greater than 50. The output gives us the same observed t-value that we calculated by hand \\(t=0.97\\), the degrees of freedom and the p-value. To conduct a two-tailed test, where we just make the prediction that the population mean that the sample came from is not equal to some value, we just drop the alternative argument. For instance, we can test whether the zeppo data come from a population with a mean equal to 65: t.test(zeppo, mu = 65) # two tailed test ## ## One Sample t-test ## ## data: zeppo ## t = 3.429, df = 19, p-value = 0.002813 ## alternative hypothesis: true mean is not equal to 65 ## 95 percent confidence interval: ## 67.84422 76.75578 ## sample estimates: ## mean of x ## 72.3 Again, we see the same t-value, degrees of freedom and p-value as we calculated by hand. Also with a two-tailed t-test we get the 95% confidence interval of the true population mean (see section @ref(calculating -a-t-distribution-confidence-interval). Finally, if we wished to do a one-tailed t-test where we were testing whether the sample came from a population with a population mean of less than some value, we would use alternative = \"less\". For example, to test if the sample of puzzle competitors came from a population that completed their puzzles in less than 16 minutes: t.test(xt$time3, mu = 16, alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: xt$time3 ## t = -0.98781, df = 9, p-value = 0.1745 ## alternative hypothesis: true mean is less than 16 ## 95 percent confidence interval: ## -Inf 16.76313 ## sample estimates: ## mean of x ## 15.10822 10.4 Assumptions of the one-sample t-test The main assumptions of the one-sample t-test are that the observations should be independent of each other, and the values should be approximately normally distributed. We can more formally test if our data come from a population that is approximately normally distributed using a Shapiro-Wilk test. This test essentially examines the distribution of our data, and determines the probability that it came from a normal distribution. We can perform this test in R using shapiro.test(). shapiro.test(x) ## ## Shapiro-Wilk normality test ## ## data: x ## W = 0.98341, p-value = 0.9938 shapiro.test(zeppo) ## ## Shapiro-Wilk normality test ## ## data: zeppo ## W = 0.96205, p-value = 0.5856 shapiro.test(xt$time3) ## ## Shapiro-Wilk normality test ## ## data: xt$time3 ## W = 0.84497, p-value = 0.0506 shapiro.test(adelie$flipper_length_mm) ## ## Shapiro-Wilk normality test ## ## data: adelie$flipper_length_mm ## W = 0.96541, p-value = 0.6056 As you can see, all four of the datasets that we have performed one-sample t-tests on have p-values for this test that are greater than p=0.05. This suggests that our data are approximately normally distributed. The p-value for the crossword puzzle times is very low p=0.0501 which probably suggests that we should look at that data in more detail to be sure that our data are normally distributed. If your data are not normally distributed, then one option is to perform a non-parametric alternative to the one-sample t-test. This test is called the one-sample Wilcoxon signed rank test. We will not go into the details of the test, but effectively it tests whether your sample is likely to have come from a population with a median of a specified value. It is run like this in R: wilcox.test(xt$time3, mu = 16, alternative = &quot;less&quot;) ## ## Wilcoxon signed rank exact test ## ## data: xt$time3 ## V = 19, p-value = 0.2158 ## alternative hypothesis: true location is less than 16 Again, with this test, we are looking for a p-value lower than 0.05 to reject the null hypothesis and accept the alternative that our sample comes from a population with a median of less than 16. "],["two-sample-inferential-statistics.html", "Chapter 11 Two Sample Inferential Statistics 11.1 Independent Samples t-test 11.2 Sampling Distribution of the Difference in Sample Means 11.3 Pooled Standard Deviation 11.4 Theory behind Students t-test 11.5 Confidence Interval for Difference in Means 11.6 Conducting the Student t-test in R 11.7 Assumptions of the Independent t-test 11.8 Welchs t-test 11.9 Effect Size for Independent two sample t-tests: 11.10 Paired t-tests 11.11 Non-parametric Alternatives for Independent t-tests 11.12 Non-parametric Alternatives to the Two Sample t-tests", " Chapter 11 Two Sample Inferential Statistics A very common question in statistics is whether two groups differ from each other in their distributions. Most often, what we are really asking is whether the population means \\(\\mu\\) of two groups differ from each other. In the figure below, we are plotting four different comparisons of two normal distributions. In the top left, both populations have the same mean \\(\\mu\\) and same variance \\(\\sigma^2\\) (remember that variance is standard deviation \\(\\sigma\\) squared). These two populations are essentially identical. In the top right, both populations have the same mean \\(\\mu\\), but differ in their variance \\(\\sigma^2\\). The bottom two distributions show populations with different population means \\(\\mu\\) and the same variance \\(\\sigma^2\\). In the bottom left, the means are quite far apart, whereas in the bottom right the means are different to each other but the effect is much smaller. It is also possible to have the situation where the means are different to each other, and so is the variance, as is shown in the figure below: 11.1 Independent Samples t-test The most typical situation where we are interested in comparing two means is when we collect two samples of data. Perhaps we collect data on the reaction times of subjects after drinking water or coffee. We would be able to directly calculate the sample means \\(\\overline{x}\\) of both samples and could find out if there was a different or not in these values. But, what we really want to know is whether the respective populations that these two samples come from (i.e. the population of water drinkers vs the population of coffee drinkers) actually differ in their population means \\(\\mu\\). This is the situation that we will discuss more in this chapter. Independent t-tests examine whether two independent samples (meaning different individuals in each group) could theoretically have come from populations with the same population mean \\(\\mu\\), or whether this is unlikely and their populations likely had different population means \\(\\mu\\). Independent t-tests also get called two-sample t-tests. There are two types of independent t-tests. The first, and the one that we will discuss most extensively, is called the Students t-test. This test assumes that the populations we are comparing have the same variances \\(\\sigma^2\\). The other independent t-test is called the Welchs t-test and makes a correction that allows for the two populations we are comparing to have different variances \\(\\sigma^2\\). We will talk more about this version when performing t-tests in R, as it is actually the default version that R performs. For now, we shall dive deeper into the principles behind the Students t-test. 11.2 Sampling Distribution of the Difference in Sample Means For a two-sample independent \\(t\\)-test, the sampling distribution we use is different to the sampling distribution for a one-sample \\(t\\)-test. In a one-sample test, we are focused on the sampling distribution of sample means. In a two-sample test we are focused on the sampling distribution of the differences in sample means. 11.2.1 Visualizing the Sampling Distribution Lets try to illustrate this by looking at the sampling distributions we could get when we draw samples from two populations. In the first example, the populations differ in their means. In the second example, the populations do not differ in their means. Difference in Population Means Say we have two normally distributed populations, A and B and each has 500,000 subjects. A has a population mean \\(\\mu = 15\\), and a population standard deviation \\(\\sigma = 2.5\\). B has a population mean \\(\\mu = 12\\), and a population SD \\(\\sigma = 2.5\\). We can generate these population by simulation in R like this: set.seed(1) A &lt;- rnorm(n = 500000, mean = 15, sd = 2.5) B &lt;- rnorm(n = 500000, mean = 12, sd = 2.5) We can check their mean and standard deviation, and we see that the standard deviations are identical at 2.5, but the population means differ by 3: # and their summary descriptive stats mean(A) ## [1] 14.99879 sd(A) ## [1] 2.500036 mean(B) ## [1] 12.00144 sd(B) ## [1] 2.500893 We can visualize these population distributions as follows: data.frame(A, B) %&gt;% pivot_longer(1:2) %&gt;% ggplot(aes(x=value, fill=name)) + geom_density(alpha = .1) + theme_classic() + ggtitle(&quot;Comparison of Population A vs B&quot;)+ geom_vline(xintercept = 15, color=&quot;red&quot;, lwd=1, lty=1)+ geom_vline(xintercept = 12, color=&quot;dodgerblue&quot;, lwd=1, lty=2) + scale_x_continuous(breaks = seq(2,26,2)) We can calculate the true difference in population means \\(\\mu_{A} - \\mu_{B}\\) to be 2.997. mean(A) - mean(B) ## [1] 2.997348 If we knew nothing about the populations of A and B, but were interested in whether they differed in their population means, then we might take a sample of A and a sample of B and compare their sample means. In this example, we are going to take a sample of size \\(n=17\\) from population A, and a sample of size \\(n=20\\) from population B. These sample sizes were picked more or less at random - they can be the same sized samples, or they can be different sizes. Lets take our samples, and then examine the differences in our sample means: set.seed(1) # so we get the same values a1 &lt;- sample(A, 17) a1 ## [1] 12.71908 12.92693 15.57082 10.12655 14.14926 11.05455 18.55350 16.56168 13.77205 11.85902 15.30585 ## [12] 19.11841 11.05516 17.02920 13.56701 12.41693 12.65853 b1 &lt;- sample(B, 20) b1 ## [1] 13.858019 13.763390 15.319093 16.544886 12.123180 16.548515 14.489600 12.282872 17.494397 18.731744 ## [11] 15.390983 12.210302 4.441760 13.084765 16.135640 10.655858 8.588236 12.964748 8.492675 15.173493 mean(a1) ## [1] 14.02615 mean(b1) ## [1] 13.41471 mean(a1) - mean(b1) ## [1] 0.6114413 Here we find that the difference in means between our sample from population A and our sample from population B is 2.85. In other words, \\(\\overline{x}_{A} - \\overline{x}_{B} = 2.85\\) This is pretty close to the true difference in population means (although were currently pretending that we dont know that difference). We could do this again. Lets select another sample from population A of size 17, and another sample from population B of size 20: a2 &lt;- sample(A, 17) a2 ## [1] 14.64137 14.27232 17.76051 13.22621 19.59587 17.75243 13.07573 15.97000 12.42184 15.64717 16.65829 ## [12] 14.46694 15.17954 14.59615 11.85324 12.99201 13.77815 b2 &lt;- sample(B, 20) b2 ## [1] 12.329417 15.680443 11.936513 9.741032 9.916850 10.447803 13.427905 6.700831 4.941283 11.883711 ## [11] 10.742010 13.571716 12.491560 10.580204 12.902325 10.743859 14.281017 13.581381 6.859580 9.807432 mean(a2) ## [1] 14.93457 mean(b2) ## [1] 11.12834 mean(a2) - mean(b2) ## [1] 3.806231 This time, \\(\\overline{x}_{A} - \\overline{x}_{B} = 3.37\\) This is again pretty close to the true difference in population means, but a bit higher this time. We can keep drawing samples of size 17 from A, and size 20 from B and examining the difference in sample means \\(\\overline{x}_{A} - \\overline{x}_{B}\\). A quick way of writing that code in R is as follows - where we repeat it 5 more times: mean(sample(A,17)) - mean(sample(B,20)) ## [1] 2.490821 mean(sample(A,17)) - mean(sample(B,20)) ## [1] 3.011947 mean(sample(A,17)) - mean(sample(B,20)) ## [1] 1.923342 mean(sample(A,17)) - mean(sample(B,20)) ## [1] 4.408804 mean(sample(A,17)) - mean(sample(B,20)) ## [1] 2.454322 As you can see, the values we get tend to be a little bit above or a little bit below 3. If we did this thousands and thousands of times, wed get a distribution of the differences in sample means. This would be the sampling distribution of the differences in sample means. Below we repeat the above step 10,000 times to get a visualization of what this sampling distribution looks like: set.seed(1) difs &lt;- vector(&#39;list&#39;, 10000) for(i in 1:10000){ difs[[i]] &lt;- mean(sample(A, 17)) - mean(sample(B, 20)) } df &lt;- data.frame(dif = unlist(difs)) #get mean difference mean(unlist(difs)) # 3.00 ## [1] 3.001222 #make histogram ggplot(df, aes(x=dif)) + geom_histogram(color=&#39;black&#39;, fill=&#39;dodgerblue&#39;, alpha=.5, binwidth = .1)+ theme_classic() + xlab(&quot;Difference in Sample Means&quot;)+ ggtitle(&quot;Sampling Distribution of Difference in Sample Means&quot;) + geom_vline(xintercept = mean(unlist(difs)), lwd=1, color=&quot;red&quot;) As you can see, the sampling distribution is approximately symmetrical. If we ran it for more simulations, it would become even more symmetrical. The average (mean) difference in sample means across all samples is \\(\\Large \\mu_{\\overline{x}_{A} - \\overline{x}_{B}}= 2.987\\) which is approximately equal to the real difference in population means \\(\\mu_{A} - \\mu_{B}\\). An obvious next question is what is the standard deviation of this sampling distribution? i.e. the standard deviation of the sampling distribution of the differences in sample means. If we knew this, then wed be able to describe how likely or unlikely we were to get any particular difference in sample means in terms of how many sampling distribution standard deviations that score is away from the mean. There are actually two different formulas that we can use to work out the standard deviation - which we will discuss shortly (see section 11.3). In brief, it is a bit trickier to calculate the standard deviation of this sampling distribution compared to the standard deviation of the sampling distribution of sample means, because our difference in sample means uses two different samples each with their own sample standard deviation. Given we just simulated our sampling distribution of the differences in sample means, we could just look at its standard deviation in R: sd(unlist(difs)) ## [1] 0.8317358 This tells us that the standard deviation of this sampling distribution is approximately equal to \\(\\sigma_{\\overline{x}_{A} - \\overline{x}_{B}}= 0.82\\). As well as using the formulas that we will introduce shortly to estimate this value, there is actually another shortcut we could have used too as we know the original population standard deviations. This is not essential to know or remember, as its not something we would ever do in practice, its just something to point out in passing. There is something called variance sum law. Essentially, if you want to know the difference of two variables, then it is equal to the sum of the variance of the two variables. Our variables in this scenario are the sampling distributions of sample means for A and B. We can calculate the variance of each by \\(\\sigma^2/n\\). If we add them together according to the variance sum law, we get the variance for the difference in these two variables. Then we square-root to get the standard deviation - which is equivalent to our sampling distribution of the difference in sample means standard deviation: sqrt((var(A)/17) + (var(B)/20)) ## [1] 0.8248519 Again, this tells us that \\(\\Large \\sigma_{\\overline{x}_{A} - \\overline{x}_{B}}= 0.82\\). However, in practice, we only have our two samples (our one sample of population A and our one sample of population B). We dont know anything else about the sampling distribution or the populations. Therefore we cannot use either of the above methods to calculate the standard deviation of the sampling distribution. Well get to what method you need to use soon. No Difference in Population Means Lets look at an example of what the sampling distribution of difference in sample means looks like when there is no difference between population means. In this scenario, we have two normally distributed populations, C and D and each has 500,000 subjects. C has a population mean \\(\\mu = 10\\), and a population standard deviation \\(\\sigma = 3\\). D has a population mean \\(\\mu = 10\\), and a population SD \\(\\sigma = 3\\). We can generate these populations in R: set.seed(1) C &lt;- rnorm(n = 500000, mean = 10, sd = 3) D &lt;- rnorm(n = 500000, mean = 10, sd = 3) We can then examine their means and standard deviations: # and their summary descriptive stats mean(C) ## [1] 9.99855 sd(C) ## [1] 3.000043 mean(D) ## [1] 10.00173 sd(D) ## [1] 3.001071 We can then calculate the true difference between these populations in their population means and see that \\(\\mu_{C}-\\mu_{D} = 0.00\\) mean(C) - mean(D) ## [1] -0.003182429 We can also visualize these populations - it can be hard to see both populations because they are identical and D is directly on top of C, but they are both there! data.frame(C, D) %&gt;% pivot_longer(1:2) %&gt;% ggplot(aes(x=value, fill=name)) + geom_density(alpha = .1) + theme_classic() + ggtitle(&quot;Comparison of Population C vs D&quot;)+ geom_vline(xintercept = 10, color=&quot;red&quot;, lwd=1, lty=1)+ geom_vline(xintercept = 10, color=&quot;dodgerblue&quot;, lwd=1, lty=2) For this example, lets look at one sample of size 11 for population C and one sample of size 14 for population D. set.seed(1) c1 &lt;- sample(C, 11) d1 &lt;- sample(D, 14) c1 ## [1] 7.262896 7.512318 10.684988 4.151858 8.979111 5.265462 14.264197 11.874013 8.526464 6.230820 ## [11] 10.367020 d1 ## [1] 17.197083 5.400551 16.514897 11.088543 4.907419 9.764055 12.229622 12.116067 13.982912 15.453863 ## [11] 10.147816 15.458219 12.987520 10.339447 mean(c1) ## [1] 8.647195 mean(d1) ## [1] 11.97057 mean(c1) - mean(d1) ## [1] -3.323377 Here we find that the difference in sample means \\(\\Large \\overline{x}_C - \\overline{x}_D = -0.197\\). If we did this lots of times, wed sometimes get sample means that were larger in C and sometimes they would be larger in D. Lets do it five more times: mean(sample(C,11)) - mean(sample(D,14)) ## [1] -0.01121902 mean(sample(C,11)) - mean(sample(D,14)) ## [1] 1.922217 mean(sample(C,11)) - mean(sample(D,14)) ## [1] 0.3862263 mean(sample(C,11)) - mean(sample(D,14)) ## [1] 2.134201 mean(sample(C,11)) - mean(sample(D,14)) ## [1] -1.017785 If we did this thousands and thousands of times, we will get our sampling distribution of the differences in sample means. We can use the code below to simulate this, collecting 20,000 samples of C and D and determining the difference in means for each sample collected: set.seed(1) difs1 &lt;- vector(&#39;list&#39;, 20000) for(i in 1:20000){ difs1[[i]] &lt;- mean(sample(C, 11)) - mean(sample(D, 14)) } We can visualize this sampling distribution: ## plot as a histogram: df1 &lt;- data.frame(dif = unlist(difs1)) ggplot(df1, aes(x=dif)) + geom_histogram(color=&#39;black&#39;, fill=&#39;mistyrose&#39;, alpha=.5, binwidth = .2)+ theme_classic() + ggtitle(&quot;Sampling Distribution of Difference in Sample Means&quot;) + geom_vline(xintercept = mean(unlist(difs1)), lwd=1, color=&quot;red&quot;) mean(unlist(difs1)) ## [1] 0.001920558 After 20,000 samples of size 11 from population C and size 14 from population D, we find that the mean difference in sample means is \\(\\mu_{\\overline{x}_C - \\overline{x}_D} = -0.006\\). This is pretty close to the true difference in population means of 0. If we were to run our simulation for longer to get even more differences in sample means, then our mean difference would get even closer to 0. So, we have a symmetrical sampling distribution of differences in sample means with a mean \\(\\mu_{\\overline{x}_C - \\overline{x}_D}\\) approximately equal to 0.0. What is the Standard Deviation of this distribution? Again, we can calculate this directly from our simulated data: sd(unlist(difs1)) ## [1] 1.22066 Here we find that the standard deviation is \\(\\sigma_{\\overline{x}_C - \\overline{x}_D} = 1.2\\). The question becomes, how do you estimate this standard deviation when you only have one sample of each population? We shall get to how this is done in the next few sections. 11.3 Pooled Standard Deviation In the Students t-test, we assume that our two samples come from populations that have equal variance, i.e. that their standard deviations are equivalent. The reason behind this is that it enables us to use this estimate in calculating the Standard Deviation of the Sampling Distribution of Differences in Sample Means. When we assume that two samples have the same variance, we estimate this by pooling their standard deviations. Lets look in more detail about how standard deviations are pooled. When we have two samples, they each have sample standard deviations, that are usually not equal. For example, lets look at these two samples. The first sample is of size 9 and the second sample is of size 10. samp1 &lt;- c(23, 25, 33, 19, 21, 27, 26, 31, 20) samp1 ## [1] 23 25 33 19 21 27 26 31 20 sd(samp1) ## [1] 4.821825 samp2 &lt;- c(21, 22, 23, 19, 20, 24, 25, 21, 23, 22) samp2 ## [1] 21 22 23 19 20 24 25 21 23 22 sd(samp2) ## [1] 1.825742 Its clear that our sample standard deviations differ, with sample 1 having a sample standard deviation \\(s=4.82\\) and sample 2 having a sample standard deviation of \\(s=1.83\\). So, if we assumed that they both come from populations with the same SD, how do we estimate that value? We have to pool the standard deviation. Essentially, we want to calculate a value that is likely somewhere in between the two sample SDs. In practice we use a weighted average. There are two ways of going about calculating this pooled standard deviation value. The first is to use first principles as to what a standard deviation is. This is the one that makes most sense logically (to us at least), but it is also the more long-winded way. The second way is to use a shortcut formula. Option 1: From first principles Lets first calculate the pooled standard deviation based on first principles. Remember, a standard deviation is the square root of the average squared deviation of each value from the mean. What we could do is to get the difference of every score from its group mean, then square those differences, then add them all up, then divide by \\(n-2\\), and then square root. We divide by \\(n-2\\) instead of \\(n-1\\) because we have two samples that we are using and we are using two different estimated means to determine the standard deviation. Therefore, to avoid underestimating our standard deviation we divide by \\(n-2\\). Here, we break this down step by step: Calculate each sample mean: mean(samp1) ## [1] 25 mean(samp2) ## [1] 22 Lets get the differences of each score from their group mean: # get the differences from each group mean: samp1_dif &lt;- samp1 - mean(samp1) samp2_dif &lt;- samp2 - mean(samp2) samp1_dif ## [1] -2 0 8 -6 -4 2 1 6 -5 samp2_dif ## [1] -1 0 1 -3 -2 2 3 -1 1 0 Now square all these differences: samp1_dif2 &lt;- samp1_dif^2 samp2_dif2 &lt;- samp2_dif^2 samp1_dif2 ## [1] 4 0 64 36 16 4 1 36 25 samp2_dif2 ## [1] 1 0 1 9 4 4 9 1 1 0 Get the sum of all the squares and add them up across both samples: ssq &lt;- sum(samp1_dif2) + sum(samp2_dif2) ssq ## [1] 216 Now we get the average squared deviation by dividing by n-2 n2 &lt;- 9 + 10 - 2 asd &lt;- ssq/n2 asd ## [1] 12.70588 Finally, we get the pooled standard deviation by square-rooting this: sqrt(asd) ## [1] 3.564531 Thus our estimate of the pooled standard deviation is \\(\\hat\\sigma_p = 3.56\\). This value is between our original sample standard deviations of 4.82 and 1.83, which makes sense. You may see the following formula for calculating the pooled standard deviation in the way we just did. \\(\\Large \\hat\\sigma_p = \\sqrt{\\frac{\\Sigma_{ik}(x_{ik} - \\overline{x}_k)^2}{n_1 + n_2 -2}}\\) Here, \\(n_1\\) is the sample size of sample 1, \\(n_2\\) is the sample size of sample 2. The \\(\\Sigma_{ik}(x_{ik} - \\overline{x}_k)^2\\) part of the formula is simply saying to take each data point away from its sample mean and then square it - and then add all of these up. It looks quite intimidating, but hopefully the steps we outlined above make sense. Option 2: Using a shortcut formula The second way to calculate the pooled standard deviations doesnt require you to know the raw data. It is a formula that only requires you to know the sample standard deviation of each sample and the sample size of each sample. This is the formula: \\(\\Large \\hat\\sigma_p = \\sqrt{\\frac{w_1s_1^2 + w_2s_2^2 }{w_1 + w_2}}\\) Here, \\(w_1 = n_1-1\\) where \\(n_1\\) is the sample size of sample 1. \\(w_2 = n_2-1\\) where \\(n_2\\) is the sample size of sample 2. \\(s_1\\) is the sample standard deviation of sample 1, and \\(s_1\\) is the sample standard deviation of sample 2. We can calculate the pooled standard deviation using this formula for our two samples above: s1sd &lt;- sd(samp1) s2sd &lt;- sd(samp2) w1 &lt;- length(samp1) - 1 w2 &lt;- length(samp2) - 1 numerator &lt;- (w1*s1sd^2) + (w2*s2sd^2) denominator &lt;- w1+w2 sqrt(numerator/denominator) ## [1] 3.564531 This approach also calculates the pooled standard deviation to be \\(\\hat\\sigma_p = 3.56\\). 11.4 Theory behind Students t-test Hopefully you have a sense of the sampling distribution for the difference in sample means, and an idea about what pooled standard deviation is. In this section well bring together these ideas to show you the behind the scenes working of the Students t-test. Well use some example data from the Navarro book. These data are the exam scores of students who were TA-ed by either Anastasia or Bernadette. The question at hand is whether these two samples could have come from populations with the same mean, or from populations with different means. We assume that the variance (and therefore standard deviations) of both of these populations are the same. Here are the data: anastasia &lt;- c(65, 74, 73, 83, 76, 65, 86, 70, 80, 55, 78, 78, 90, 77, 68) bernadette &lt;- c(72, 66, 71, 66, 76, 69, 79, 73, 62, 69, 68, 60, 73, 68, 67, 74, 56, 74) We can make a boxplot to compare the two samples: # plot the data: d &lt;- data.frame(values = c(anastasia, bernadette), group = c(rep(&quot;anastasia&quot;,15), rep(&quot;bernadette&quot;, 18)) ) ggplot(d, aes(x = group, y = values, fill = group)) + geom_boxplot(alpha=.3) + geom_jitter(width=.1, size=2) + theme_classic() + scale_fill_manual(values = c(&quot;firebrick&quot;, &quot;dodgerblue&quot;)) We can also get the sample mean \\(\\overline{x}\\), standard deviation \\(s\\) and \\(n\\) of each sample: mean(anastasia) ## [1] 74.53333 sd(anastasia) ## [1] 8.998942 nA&lt;-length(anastasia) nA ## [1] 15 mean(bernadette) ## [1] 69.05556 sd(bernadette) ## [1] 5.774918 nB &lt;- length(bernadette) nB ## [1] 18 Although we are actually interested in whether these two samples come from populations that differ in their population means, we can directly measure if the two samples differ in their sample means by subtracting one from the other: mean(anastasia) - mean(bernadette) ## [1] 5.477778 We can see that Anastasia students have on average 5.48 higher scores than Bernadette students. But how meaningful is this difference? What we are going to do is to work out how unusual or usual our one observed sample mean difference is. We need to construct the sampling distribution of differences in sample means. We hypothesize that the true mean difference in population means is 0 (no difference between groups). These are our hypotheses: \\(H_{0}: \\mu_A = \\mu_A\\) \\(H_{1}: \\mu_A \\ne \\mu_A\\) If we assume that the true difference in population means is 0 (the null hypothesis), then this will mean that the mean of the sampling distribution of the difference in sample means will also be 0 (\\(\\mu_{\\overline{x}_A-\\overline{x}_B}=0\\)). If we know the standard deviation of this sampling distribution, then we could work out how many standard deviations away our one observed sample mean difference of 5.48 is from 0. That would enable us to work out the probability of getting a value that extreme. So our next step is to work out the standard deviation of the sampling distribution. If we assume equal variances between the populations of group A and B, then we can calculate the standard deviation of this sampling distribution as \\(\\Large \\sigma_{\\overline{x}_A-\\overline{x}_B} = \\hat\\sigma_p \\times \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2} }\\) So, the first step in this formula is to calculate the pooled standard deviation \\(\\hat\\sigma_p\\). We can either calculate this from first principles or using the shortcut formula (see section 11.3). Below, we calculate the pooled standard deviation using the first principles: # from first principles, calculating deviations from each group mean difA2 &lt;- (anastasia - mean(anastasia))^2 difB2 &lt;- (bernadette - mean(bernadette))^2 sumsq &lt;- sum(difA2) + sum(difB2) n &lt;- nA + nB #33 sd.pool &lt;- sqrt(sumsq/(n-2)) sd.pool # 7.41 this is the estimated pooled s.d. ## [1] 7.406792 This gives us a value of \\(\\hat\\sigma_p = 7.407\\). This makes sense being in between our two sample standard deviations: sd(anastasia) #8.999 ## [1] 8.998942 sd(bernadette) #5.775 ## [1] 5.774918 Once weve calculated the pooled standard deviation \\(\\hat\\sigma_p\\), we can insert it to the formula above to calculate the standard deviation of the sampling distribution of differences in sample means \\(\\sigma_{\\overline{x}_A-\\overline{x}_B}\\). sedm &lt;- sd.pool * sqrt( (1/nA) + (1/nB)) sedm # this is the Standard Deviation of the Sampling Distribution of differences in sample means ## [1] 2.589436 Doing this, we calculate \\(\\sigma_{\\overline{x}_A-\\overline{x}_B} = 2.59\\). This tells us that the standard deviation of the theoretical sampling distribution of the differences in sample means is 2.59. With that information, we can work out how many standard deviations away from the mean (of 0) our one observed difference in sample means of \\(\\overline{x}_A - \\overline{x}_B = 5.48\\) is. Lets visualize what this sampling distribution looks like: By estimating the standard deviation, we are able to plot what our sampling distribution would look like. We are then able to see that our observed difference in sample means of \\(\\overline{x}_A - \\overline{x}_B = 5.48\\) is quite far away from the hypothesized mean of 0 in terms of standard deviations. What we have not discussed so far is what kind of shape that this distribution is other than it is symmetrical. Similar to when we were looking at sampling distributions of sample means (see section 11.2), the shape of this sampling distribution of the difference in sample means is also \\(t\\)-shaped. In the example given above, it is a \\(t\\)-distribution with 31 degrees of freedom. The degrees of freedom of these sampling distributions are equal to \\(df = n_A + n_B - 2\\). So in our case, this is equal to: nA + nB - 2 ## [1] 31 Given that the shape of our distribution is \\(t\\)-shaped, when we determine how many standard deviations away from the mean our single observed difference in sample means is, we call this our observed t-value. We calculate t by the following formula: \\(\\Large t = \\frac{(\\overline{x}_A - \\overline{x}_B) - (\\mu_{\\overline{x}_{A} - \\overline{x}_{B}})}{\\sigma_{\\overline{x}_{A} - \\overline{x}_{B}}}\\) But because we assume the mean of the sampling distribution of sample means to be zero, we can write it as: \\(\\Large t = \\frac{\\overline{x}_A - \\overline{x}_B}{\\sigma_{\\overline{x}_{A} - \\overline{x}_{B}}}\\) Essentially, it is taking our observed difference in sample means and dividing it by the standard deviation of the sampling distribution. tobs &lt;- (mean(anastasia) - mean(bernadette)) / sedm tobs ## [1] 2.115432 Our observed t-value is \\(t = 2.115\\). Our final thing to do is to calculate the p-value. The first step in this is to work out the proportion of the t distribution that is higher than our observed t-value. Because we are running a two-tailed test (we are examining whether the true difference in population means is 0 or not equal to 0), we need to double that proportion to get our p-value. The image above helps to explain how wed calculate the p-value. Because its a two-tailed test, we need to calculate the proportion of the distribution in the red area - this is the area that is greater than \\(t=2.115\\) and less than \\(t=-2.115\\). We can use the pt() function in R to calculate this - remember that for this example, our degrees of freedom are 31: 1 - pt(2.115, df=31) ## [1] 0.02128454 This tells us that the area beyond \\(t=2.115\\) is 0.021. Therefore to get our p-value we double this: (1 - pt(2.115, df=31)) * 2 ## [1] 0.04256907 Our p-value is \\(p = 0.043\\). ** One-tailed test ** If we had predicted beforehand that Anastaias students would score more highly on the exam than Bernadettes then we could perform a one-tailed test. The hypotheses in this situation would be: \\(H_{0}: \\mu_A \\le \\mu_A\\) \\(H_{1}: \\mu_A &gt; \\mu_A\\) All of the steps for calculating the observed \\(t\\)-value would be the same as above. The only difference is when calculating the p-value, we only need to consider the area under the curve beyond \\(t=2.115\\). Therefore the p-value for this one-tailed test is \\(p = 0.021\\). 11.5 Confidence Interval for Difference in Means When we take a sample of two independent groups, we are able to directly calculate the observed difference in sample means \\(\\overline{x}_A - \\overline{x}_B\\). What we are usually interested in is whether this observed difference is meaningfully different from 0. One way to gain more insight into whether this might be true or not is to generate a 95% confidence interval around our observed sample difference \\(\\overline{x}_A - \\overline{x}_B\\). If 0 is not inside our confidence interval, then this is pretty good evidence that the true difference in population means \\(\\mu_A - \\mu_B\\) between the two groups is not 0, and therefore there is a difference in population means. As with all confidence intervals, the longer explanation as to what a 95% confidence interval really is, is a bit more long winded. Technically, this 95% confidence intervals means that if we were to take many samples of A and many samples of B, and calculate the difference and confidence intervals for all of them, in 95% of these confidence intervals we would have captured the true difference in population means \\(\\mu_A - \\mu_B\\). Lets illustrate with a small example, and then well turn to our TA data. Example 1 A researcher compares the reaction times of two groups on a motor test. Individuals are different (independent) in each group. GroupA took a stimulant prior to the test. GroupB is the control group. groupA &lt;- c(5.5, 5.1, 3.5, 5.1, 4.6, 4.6, 5.9, 4.0, 3.1, 3.8) mean(groupA) # 4.52 ## [1] 4.52 sd(groupA) # 0.91 ## [1] 0.9065196 groupB &lt;- c(5.7, 5.3, 5.9, 5.0, 5.0, 4.3, 4.1, 5.9, 5.9, 5.8, 5.4, 5.2, 4.9, 5.5) mean(groupB) # 5.28 ## [1] 5.278571 sd(groupB) # 0.58 ## [1] 0.5766996 # plot the data: dd &lt;- data.frame(values = c(groupA, groupB), group = c(rep(&quot;A&quot;,10), rep(&quot;B&quot;, 14)) ) head(dd) ## values group ## 1 5.5 A ## 2 5.1 A ## 3 3.5 A ## 4 5.1 A ## 5 4.6 A ## 6 4.6 A ggplot(dd, aes(x = group, y = values, fill = group)) + geom_boxplot(alpha=.3) + geom_jitter(width=.1, size=2) + theme_classic() + scale_fill_manual(values = c(&quot;firebrick&quot;, &quot;dodgerblue&quot;)) We can see from our data, that the observed difference in sample means between the two groups is \\(\\overline{x}_A - \\overline{x}_B = -0.759\\) mean(groupA) - mean(groupB) ## [1] -0.7585714 Group B (the control group) is slower by 0.76 seconds on average than group A (the group that took the stimulant). The first step in constructing a confidence interval around this observed difference in sample means, is to imagine that this observed difference in sample means came from a sampling distribution of difference in sample means. Our observed value was just one possible sample that we could have got from this sampling distribution. This sampling distribution of differences in sample means, is \\(t\\)-shaped with degrees of freedom equal to \\(n-2\\). We assume that our observed value of the difference in sample means is a good estimate of the true difference in population means. If we know the standard deviation of this sampling distribution, then we can create the 95% confidence interval around it using the formula: \\(\\Large CI_{95\\%} = \\overline{x}_A - \\overline{x}_B \\pm t \\times \\sigma_{\\overline{x}_A - \\overline{x}_B}\\) As discussed in the previous sections (see 11.2 and 11.3) the standard deviation of the sampling distribution of differences in sample means, can be calculated using the formula: \\(\\Large \\sigma_{\\overline{x}_A - \\overline{x}_B} = \\hat{\\sigma_{\\rho}} \\times \\sqrt{\\frac{1}{n_A} + \\frac{1}{n_B}}\\) where \\(\\hat{\\sigma_{\\rho}}\\) is the pooled estimate of the standard deviation between the two groups. The below diagram illustrates how this formula determines the 95% confidence interval. Our margin of error is the value of \\(t\\) for the \\(t\\)-distribution with the appropriate degrees of freedom that leaves 2.5% in each tail, and therefore 95% in the middle of the distribution. We multiply this value of \\(t\\) by the standard deviation of the sampling distribution of the differences in sample means to get the margin of error. The first step is therefore to calculate the pooled estimate of the common standard deviation \\(\\hat{\\sigma_{\\rho}}\\) (see section ??). Using the method of calculating deviations from each group mean, we can find that \\(\\hat{\\sigma_{\\rho}} = 0.73\\) difA2 &lt;- (groupA - mean(groupA))^2 difB2 &lt;- (groupB - mean(groupB))^2 sumsq &lt;- sum(difA2) + sum(difB2) n &lt;- length(groupA) + length(groupB) #24 sd.pool &lt;- sqrt(sumsq/(n-2)) sd.pool ## [1] 0.7298683 Putting this value into the above formula, we can calculate the standard deviation of the sampling distribution of the differences in sample means to be \\(\\sigma_{\\overline{x}_A - \\overline{x}_B} = 0.302\\). sedm1 &lt;- sd.pool * sqrt( (1/length(groupA)) + (1/length(groupB))) sedm1 ## [1] 0.3021942 So now we have the estimated mean \\(\\overline{x}_A - \\overline{x}_B\\) and the standard deviation \\(\\sigma_{\\overline{x}_A - \\overline{x}_B}\\) of our sampling distribution of differences in sample means. Because our sampling distribution approximates to a \\(t\\)-distribution of degrees of freedom = \\(n-2\\), we calculate the value of \\(t\\) which leaves 2.5% in the tails using the qt() function in R. n &lt;- length(groupA) + length(groupB) tval &lt;- qt(.975, df = n-2) tval ## [1] 2.073873 Using this value, we can calculate the margin of error, and the lower and upper bounds of the confidence interval: dm &lt;- mean(groupA) - mean(groupB) dm ## [1] -0.7585714 tval*sedm1 ## [1] 0.6267124 dm + (tval*sedm1) # upper bound = -0.13 ## [1] -0.131859 dm - (tval*sedm1) # lower bound = -1.39 ## [1] -1.385284 So the confidence interval is equal to: \\(CI_{95\\%} = -0.76 \\pm 0.63\\) or \\(CI_{95\\%} = -0.76[-0.13, -1.39]\\). This shows us that the true difference in population means is unlikely to include 0, so we can conclude that there is likely a true difference in population means between the two groups. The above is the by hand way to calculating the confidence interval, that explains the theory behind it. The quickest way is to use the R function t.test(). Inside the brackets, you include the two groups of data, and use var.equal=T. This last argument ensures that you are assuming that the variances between the two populations that the samples come from are equal. The $conf.int bit on the end just gives us the confidence intervals: t.test(groupA, groupB, var.equal = T)$conf.int ## [1] -1.385284 -0.131859 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 Example 2 Lets look at this with a second example using the anastasia and bernadette data. First, we get the value of \\(\\overline{x}_A - \\overline{x}_B\\), which is our observed difference in sample means. This is what we assume the mean of the sampling distribution of differences in sample means is. meandif &lt;- mean(anastasia) - mean(bernadette) # 5.48 meandif # this is our observed difference in sample means ## [1] 5.477778 We find that \\(\\overline{x}_A - \\overline{x}_B=5.48\\) Previously in section @ref(theory-behind-students-t-test) we calculated the standard deviation of this sampling distribution to be \\(\\sigma_{\\overline{x}_A - \\overline{x}_B} = 2.59\\). We saved this earlier as sedm. sedm ## [1] 2.589436 To calculate our margin of error, we need to multiply \\(\\sigma_{\\overline{x}_A - \\overline{x}_B}\\) by the value of \\(t\\) that leaves 2.5% in each tail for a \\(t\\)-distribution with \\(n-2\\) degrees of freedom: tval &lt;- qt(.975, df = length(anastasia) + length(bernadette) -2) tval #2.04 ## [1] 2.039513 We can now calculate the confidence intervals: tval*sedm ## [1] 5.28119 meandif + (tval*sedm) # 10.76 ## [1] 10.75897 meandif - (tval*sedm) # 0.20 ## [1] 0.1965873 So the 95% confidence interval is equal to: \\(CI_{95\\%} = 5.48 \\pm 5.28\\) or \\(CI_{95\\%} = 5.48[0.20, 10.76]\\). These CIs suggest that 0 is not likely to be the true difference in population means, therefore it is plausible that the population of students that have Anastasia as a TA score more highly on their exams than the population of students that have Bernadette as a TA. We could have checked this using ttest(): t.test(anastasia, bernadette, var.equal = T)$conf.int ## [1] 0.1965873 10.7589683 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 Finally, we could also plot confidence intervals like this: ## plotting confidence intervals dci &lt;- data.frame(group = &quot;&quot;, mean = meandif, lower = meandif - (tval*sedm), upper = meandif + (tval*sedm) ) ggplot(dci, aes(x=group, y=mean, ymax=upper, ymin=lower)) + geom_errorbar(width=0.2, size=1, color=&quot;blue&quot;) + geom_point(size=4, shape=21, fill=&quot;white&quot;) + theme_minimal() + coord_flip() + xlab(&quot;&quot;) + ylab(&quot;Estimate of Difference in Sample Means&quot;) 11.6 Conducting the Student t-test in R The function we need to conduct a Students t-test in R is t.test(). For instance, we want to test whether there is a difference in population means between the anastasia and bernadette samples. Here is the data visualized: # boxplot ggplot(d, aes(x = group, y = values, fill = group)) + geom_boxplot(alpha=.3, outlier.shape = NA) + geom_jitter(width=.1, size=2) + theme_classic() + scale_fill_manual(values = c(&quot;firebrick&quot;, &quot;dodgerblue&quot;)) You can do the Students t-test either from data stored in vectors, or in dataframes. Heres how to do it for data stored in vectors. The first two arguments should be the names of the two independent samples. The final argument of var.equal = T tells R to do a Students t-test by assuming that variances are equal between the two groups. t.test(anastasia, bernadette, var.equal = T) ## ## Two Sample t-test ## ## data: anastasia and bernadette ## t = 2.1154, df = 31, p-value = 0.04253 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.1965873 10.7589683 ## sample estimates: ## mean of x mean of y ## 74.53333 69.05556 As you can see from the output, the default Students t-test is to do a 2-tailed test. In this output you are given the observed \\(t\\)-value, the degrees of freedom and the p-value. Also given are the sample means of each sample, as well as a 95% confidence interval of the true difference in population means (see section 11.5 for more on this confidence interval). Its also possible to use t-test with data in long format in dataframes. Here is our data in long data format (we used this to make the boxplot): head(d) ## values group ## 1 65 anastasia ## 2 74 anastasia ## 3 73 anastasia ## 4 83 anastasia ## 5 76 anastasia ## 6 65 anastasia tail(d) ## values group ## 28 73 bernadette ## 29 68 bernadette ## 30 67 bernadette ## 31 74 bernadette ## 32 56 bernadette ## 33 74 bernadette In this format, we use t.test() and use the tilde ~ like this: t.test(values ~ group, data=d, var.equal = T) ## ## Two Sample t-test ## ## data: values by group ## t = 2.1154, df = 31, p-value = 0.04253 ## alternative hypothesis: true difference in means between group anastasia and group bernadette is not equal to 0 ## 95 percent confidence interval: ## 0.1965873 10.7589683 ## sample estimates: ## mean in group anastasia mean in group bernadette ## 74.53333 69.05556 As you can see, this gives the exact same output as using data stored in vectors. To perform a one-tailed t-test, you need to add the argument alternative = \"greater\" or alternative = \"less\" as appropriate for your hypothesis. So, if you had hypothesized Anastasias students came from a population with a greater population mean than the population that Bernadettes you would use alternative = \"greater\": t.test(anastasia, bernadette, var.equal = T, alternative = &quot;greater&quot;) ## ## Two Sample t-test ## ## data: anastasia and bernadette ## t = 2.1154, df = 31, p-value = 0.02126 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 1.08734 Inf ## sample estimates: ## mean of x mean of y ## 74.53333 69.05556 You could also do the following if your data are in long format: t.test(values ~ group, data=d, var.equal = T, alternative = &quot;greater&quot;) ## ## Two Sample t-test ## ## data: values by group ## t = 2.1154, df = 31, p-value = 0.02126 ## alternative hypothesis: true difference in means between group anastasia and group bernadette is greater than 0 ## 95 percent confidence interval: ## 1.08734 Inf ## sample estimates: ## mean in group anastasia mean in group bernadette ## 74.53333 69.05556 Notice that the observed \\(t\\)-value is the same as are the degrees of freedom compared to the two-tailed t-test. The p-value has actually halved. The confidence intervals also changed in the output, but you should use the confidence intervals from the two-tailed version of the test. 11.7 Assumptions of the Independent t-test The Students t-test has two important assumptions that should be checked before you can fully be confident in the results of your statistical test. Independence fo Data As the name suggests, the data in each sample should be independent of each other. That is, individuals in each sample must be different - no individual should be in both groups. Also, subjects in one sample should not influence subjects in the other sample. e.g. If students who were in the Anastasia group studied with students who were in the Bernadette group, then the groups are no longer independent. Normality Firstly, the test assumes that both samples comes from populations that are normally distributed. We can test this by examining whether each sample is approximately normally distributed. Two options for doing this are to run a Shapiro-Wilk test, or make a QQ plot. Both test whether the distribution of data in your sample follow the normal distribution shape. The Shapiro-Wilk test is run using shapiro.test() shapiro.test(anastasia) ## ## Shapiro-Wilk normality test ## ## data: anastasia ## W = 0.98186, p-value = 0.9806 shapiro.test(bernadette) ## ## Shapiro-Wilk normality test ## ## data: bernadette ## W = 0.96908, p-value = 0.7801 Because the p-values of these tests are both \\(p&gt;.05\\), we do not have sufficient evidence to suggest that our data are not approximately normal. In other words, we can assume that our data are approximately normal. To make a QQ plot, we use qnorm() followed by qqline(). What we are looking for here is for our data points to roughly stick to the diagonal line. This would mean that our data values are approximately in line with what wed expect for a normal distribution: qqnorm(anastasia) qqline(anastasia, col = &quot;steelblue&quot;, lwd = 2) # looks ok qqnorm(bernadette) qqline(bernadette, col = &quot;steelblue&quot;, lwd = 2) # looks ok In both of our plots, the data points are roughly against the line, which is good evidence that our data are normally distributed. Having some variability is fine, especially with datapoints at the tail of the x-axes. Homogeneity of Variance Another assumption of the Students t-test is that we have homogeneity of variance between the populations that our samples come from. We assume this when we calculate the pooled standard deviation which helps us calculate the standard deviation of the sampling distribution. We can examine the standard deviations of each group with sd() but this alone does not tell us if they are similar enough to qualify for having equality of variance: sd(anastasia) ## [1] 8.998942 sd(bernadette) ## [1] 5.774918 There is a formal test you can to to test for the equality of variance. This test is called a Levenes test. To do it, we use the leveneTest() function from the car library. To use it, you need your data in long format: library(car) leveneTest(y = d$values, group = d$group) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 2.1287 0.1546 ## 31 A p-value of \\(p&gt;.05\\) indicates that there is not sufficient evidence to suggest that the groups have different variances. In other words, we can assume that their variances are equal and we are ok to do a Students t-test. 11.8 Welchs t-test It is important to mention that the Students t-test is only on version of an Independent t-test. It is the one that is taught first because computationally it is the most straightforward (even though it has several strange formulas in it). An issue with the Students t-test is that we assume equal variances between the groups. As we saw in the previous section, we can test for this formally with a Levenes test, and proceed with a Students t-test if we pass that. However, a more commonly taken approach is to computationally account for any difference in variance between the groups using the Welchs test. We wont go into how this is done, but you can do this in R using t.test(). To do the Welchs test, just remove the var.equal=T: t.test(anastasia, bernadette) # notice the changes... ## ## Welch Two Sample t-test ## ## data: anastasia and bernadette ## t = 2.0342, df = 23.025, p-value = 0.05361 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.09249349 11.04804904 ## sample estimates: ## mean of x mean of y ## 74.53333 69.05556 Youll notice here that the t-value, degrees of freedom and p-value have all changed! This is a result of the test accounting for the slight differences in variance between the two samples. For a one-tailed Welchs test, you again use alternative = \"greater\", and remove equal.var=T: t.test(anastasia, bernadette, alternative = &quot;greater&quot;) ## ## Welch Two Sample t-test ## ## data: anastasia and bernadette ## t = 2.0342, df = 23.025, p-value = 0.0268 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 0.8627718 Inf ## sample estimates: ## mean of x mean of y ## 74.53333 69.05556 Again, the t-value and degrees of freedom are the same of the two-tailed Welchs t-test, and the p-value is halved. We actually recommend that if you are doing an independent t-test then you should do a Welchs test rather than a Students t-test, even though weve spent a lot of time in this chapter outlining how to do a Students t-test by hand! There is no real penalty for doing a Welchs test, and it is technically more accurate. 11.9 Effect Size for Independent two sample t-tests: Just because you observe a significant difference in the population means between your two groups doesnt mean that its necessarily interesting or relevant. The test is designed to pick up on whether there is a difference in the population means, not to tell you how large or small that difference is. We can determine how great the difference in means is by calculating the effect size. There are many different effect size measures we could choose, but perhaps the most commonly used is Cohens \\(\\delta\\). The formula for this effect size measure is: \\(\\Large \\delta = \\frac{\\overline{X}_{1} - \\overline{X}_{2}}{\\hat{\\sigma}_{\\rho}}\\) This is saying to divide the difference between the means of each group by the pooled standard deviation. For our anastasia and bernadette samples, we could do this by hand like this (note: we calculated the pooled standard deviation \\({\\hat{\\sigma}_{\\rho}}\\) for these samples earlier in section 11.5 and saved it as sd.pool.: (mean(anastasia) - mean(bernadette)) / sd.pool ## [1] 7.505159 Here, \\(\\delta = 0.74\\). The t.test() function does not produce the effect size by default. Instead, if we didnt want to calculate the effect size by hand, we can use the function cohensD from the library lsr on long format data: library(lsr) cohensD(values ~ group, data = d) ## [1] 0.7395614 Again, \\(\\delta = 0.74\\). This would be considered a large effect size. Any value of \\(\\delta\\) above 0.7 or 0.8 is considered to be a large effect size. Anything above 0.5 is a moderate effect size, and anything above 0.2 or 0.3 is a small effect size. The precise boundaries were not exactly defined by Cohen, so they are a little fuzzy. When reporting results of your t-test, you should always give the effect size too. 11.10 Paired t-tests Paired data are data where we have two related scores per individual subject. For instance, we may have a before and after score on some test. Importantly, the scores need to be measured on the same scale. For instance, if we had height and weight of each subject, then these would not be paired data as they are measured on different scales. When we have paired data, we may be interested in knowing if there is a difference between the two groups of scores in means over all subjects. We cannot proceed with an independent t-test to examine this, as the data all violate the assumption of independence. These data by their very nature are non-independent of each other. Instead, we need to account for the fact the data are paired. Lets look at the following data which show exam scores in Dr Chicos class. We have 20 students who took two exams during the class. These are called grade_test1 and grade_test2. As you can see below, the data are currently in wide format. chico &lt;- read_csv(&quot;data/chico.csv&quot;) chico ## # A tibble: 20 x 3 ## id grade_test1 grade_test2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 student1 42.9 44.6 ## 2 student2 51.8 54 ## 3 student3 71.7 72.3 ## 4 student4 51.6 53.4 ## 5 student5 63.5 63.8 ## 6 student6 58 59.3 ## 7 student7 59.8 60.8 ## 8 student8 50.8 51.6 ## 9 student9 62.5 64.3 ## 10 student10 61.9 63.2 ## 11 student11 50.4 51.8 ## 12 student12 52.6 52.2 ## 13 student13 63 63 ## 14 student14 58.3 60.5 ## 15 student15 53.3 57.1 ## 16 student16 58.7 60.1 ## 17 student17 50.1 51.7 ## 18 student18 64.2 65.6 ## 19 student19 57.4 58.3 ## 20 student20 57.1 60.1 A typical visualization would be to look at the boxplot to compare the two samples: chico.long &lt;- chico %&gt;% pivot_longer(2:3) # boxplot ggplot(chico.long, aes(x=name, y=value, fill=name))+ geom_boxplot(outlier.shape = NA, alpha=.5) + geom_jitter(width=.1, size=1) + theme_classic() + scale_fill_manual(values=c(&quot;lightseagreen&quot;,&quot;darkseagreen&quot;)) The issue with this plot however, is that our data our paired. It looks as if there isnt much of a difference between the two groups. But with paired data we are actually more interested in how each individual changes their own score over time, not how the overall mean of each group changes. There are two other ways to present paired data. A good visualization to use on paired data is a scatterplot, plotting each subjects twos scores as one datapoint. ## Scatterplot head(chico) ## # A tibble: 6 x 3 ## id grade_test1 grade_test2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 student1 42.9 44.6 ## 2 student2 51.8 54 ## 3 student3 71.7 72.3 ## 4 student4 51.6 53.4 ## 5 student5 63.5 63.8 ## 6 student6 58 59.3 ggplot(chico, aes(x=grade_test1, y=grade_test2)) + geom_point() + theme_classic()+ geom_abline(intercept =0 , slope = 1) As you can see with this scatterplot, we have also added a diagonal line through the plot. This line represents values that are equivalent on test 1 and test 2. Most dots are above this diagonal line. This shows us that for most subjects, they have a higher score on test 2 than they do on test 1. We have 20 dots because we have 20 individual subjects. Another option is to use a slope graph. Here we plot the individual dots of each group as if we are doing a boxplot, but then we join dots who are the same individual in each group together: ## Slope Graph ggplot(chico.long, aes(x=name, y=value, group=id))+ geom_point(alpha=.6, size=2)+ geom_line(color=&quot;gray50&quot;)+ theme_classic() What we are looking for in a plot such as this is whether a majority of the lines are going in the same direction as each other - either up or down. In this particular example, the absolute increase of each line is quite small and so its harder to see the differences in this graph than the scatterplot. For other datasets, where there is a lot more variation in the magnitude and direction of the differences for each individual, the slope graph might be better than the scatterplot. 11.10.1 The paired t-test is a one-sample t-test The general principle behind a paired t-test is that it is not a new test at all. In fact, we will be doing a one-sample t-test based on the difference between the two scores. We will be testing whether the true population mean difference in scores is likely to include 0. Please see section 10.2 for more information on the theory of one-sample t-tests. Here, well just walk through how to write the code in R, what assumptions you need to check, and what plots you should make. Lets first do this by hand, before we see the very quick way to do this in R. The first step is to calculate the difference scores. We create a new column called improvement that shows whether the test 2 score is higher or lower than the test 2 score. Positive scores indicate that subjects scored more highly on test 2 than on test 1. # Create a new column that is the difference between our two columns: chico$improvement &lt;- chico$grade_test2 - chico$grade_test1 chico ## # A tibble: 20 x 4 ## id grade_test1 grade_test2 improvement ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 student1 42.9 44.6 1.70 ## 2 student2 51.8 54 2.20 ## 3 student3 71.7 72.3 0.600 ## 4 student4 51.6 53.4 1.80 ## 5 student5 63.5 63.8 0.300 ## 6 student6 58 59.3 1.30 ## 7 student7 59.8 60.8 1 ## 8 student8 50.8 51.6 0.800 ## 9 student9 62.5 64.3 1.80 ## 10 student10 61.9 63.2 1.30 ## 11 student11 50.4 51.8 1.40 ## 12 student12 52.6 52.2 -0.400 ## 13 student13 63 63 0 ## 14 student14 58.3 60.5 2.20 ## 15 student15 53.3 57.1 3.80 ## 16 student16 58.7 60.1 1.40 ## 17 student17 50.1 51.7 1.60 ## 18 student18 64.2 65.6 1.40 ## 19 student19 57.4 58.3 0.900 ## 20 student20 57.1 60.1 3 We can visually inspect these data by doing a histogram of the difference scores: # histogram of difference scores ggplot(chico, aes(x=improvement)) + geom_histogram(color=&#39;black&#39;, fill=&#39;green&#39;, alpha=.4, boundary=0, binwidth = .5) + theme_classic()+ geom_vline(xintercept = 0, lty=2, color=&#39;black&#39;) This shows us that the vast majority of scores are positive, indicating that the students generally did better on test 2 than test 1. The fist bar contains two values - one is a negative improvement score, and the other is 0. We can also get the observed sample mean and standard deviation of these differences: # descriptives of the difference scores mean(chico$improvement) ## [1] 1.405 sd(chico$improvement) ## [1] 0.9703363 This shows us that the average improvement score from our one sample of difference scores is \\(\\overline{x}=1.405\\). Before we proceed with the test, we need to check that our data is normally distributed, as this is one of the assumptions of the test. We can do that using QQ plots or a Shapiro-Wilk test: # check normality of difference scores qqnorm(chico$improvement) qqline(chico$improvement, col = &quot;steelblue&quot;, lwd = 2) # bit better shapiro.test(chico$improvement) ## ## Shapiro-Wilk normality test ## ## data: chico$improvement ## W = 0.9664, p-value = 0.6778 As the dots fall largely on the line, especially in the middle of our plot, the QQ plot suggests that date are approximately normal. Likewise, because the p-value of the Shapiro-Wilk test is greater than 0.05, this indicates that our data are approximately normal. If you recall back to section 10.2, to run a one-sample t-test in R we use t.test() and set mu=0 to indicate that we are testing whether our sample comes from a population whose mean could be equal to 0. t.test(chico$improvement, mu=0) ## ## One Sample t-test ## ## data: chico$improvement ## t = 6.4754, df = 19, p-value = 3.321e-06 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.9508686 1.8591314 ## sample estimates: ## mean of x ## 1.405 This output suggests that the 95% confidence interval of the true population mean of difference (improvement) scores is \\(CI_{95\\%} = 1.405[0.951, 1.859]\\). The observed \\(t\\)-value is \\(t=6.475\\) suggesting that the observed sample mean difference score is relatively large compare to the range of mean difference scores we could have got. Indeed, we have a significant p-value, \\(p=0.000003\\) which indicates that our difference scores are unlikely to have come from a population with a true mean difference of \\(\\mu=0\\). Above, we just performed a one-sample t-test on the difference scores (the improvement column in our data). To do this, we needed to create the extra column of data. We actually didnt need to do this, we could have just done a paired t-test directly on our data: t.test(chico$grade_test2, chico$grade_test1, paired=T) ## ## Paired t-test ## ## data: chico$grade_test2 and chico$grade_test1 ## t = 6.4754, df = 19, p-value = 3.321e-06 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.9508686 1.8591314 ## sample estimates: ## mean of the differences ## 1.405 By including paired=T we are telling R to essentially run a one-sample t-test on the difference scores - this is the paired t-test. Notice the output is identical. 11.10.2 One-tailed paired t-tests The above test we just completed is a two-tailed t-test. This is testing the null hypothesis that the population mean of the difference scores is equal to 0, and the alternative hypothesis is that it is not equal to 0. If we had made an a priori prediction that test 2 would have higher scores than test 1, we could run a one-tailed test. The hypotheses for a one-tailed test would be: \\(H_0: \\mu \\le 0\\) \\(H_1: \\mu &gt; 0\\) To run the one-tailed test, we simply add alternative=\"greater\" or alternative=\"less\" depending upon which direction we are testing. In our case, we predict that test 2 will be greater than test 1, therefore we do the following: t.test(chico$grade_test2, chico$grade_test1, paired=T, alternative = &quot;greater&quot;) # this is 1-tailed ## ## Paired t-test ## ## data: chico$grade_test2 and chico$grade_test1 ## t = 6.4754, df = 19, p-value = 1.66e-06 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 1.029823 Inf ## sample estimates: ## mean of the differences ## 1.405 11.10.3 Calculating effect sizes As with the independent t-test, just because you observe a significant difference between the two groups does not indicate that that difference is particularly large or interesting. To get a better indicator of how large or meaningful a difference in group means is, we should calculate an effect size. There are numerous different effect size measures that exist. One that is commonly reported for the paired t-test is another version of Cohens \\(\\delta\\). For a paired t-test, this is calculated by the formula: \\(\\Large \\delta = \\frac{\\overline{d}}{s_d}\\) where \\(\\overline{d}\\) refers to the mean of the differences in scores for each individual, and \\(s_d\\) is the standard deviation of the differences in scores for each individual. Therefore for our chico dataset, the effect size \\(\\delta = 1.45\\). # Effect Size is equal to: mean(chico$improvement) / sd(chico$improvement) ## [1] 1.447952 This indicates a very large effect. As with the independent t-test (see section 11.9) anything above 0.7 can be considered a large effect, anything above 0.5 is a moderate effect, and anything above 0.3 is a small effect. 11.11 Non-parametric Alternatives for Independent t-tests One of the key assumptions of the independent t-test is that the data come from populations that are approximately normally distributed. If your data do not appear to be normal, there are several options still available. One of these is to do a permutation t-test (see section 14.1). Another option is to run a non-parametric statistical test. We wont go into the theory here, instead just focusing on how to run these tests in R. Lets look at an example dataset of how we would do these non-parametric tests. In a study, a researcher measured anxiety scores of subjects 1hour after taking placebo or anti-anxiety drug. These are the scores. Because different individuals are in different groups, this is a between subjects design, and therefore would be appropriate for an independent t-test. These are the scores: placebo &lt;- c(15, 16, 19, 19, 17, 20, 18, 14, 18, 20, 20, 20, 13, 11, 16, 19, 19, 16, 10) drug &lt;- c(15, 15, 16, 13, 11, 19, 17, 17, 11, 14, 10, 18, 19, 14, 13, 16, 16, 17, 14, 10, 14) We can put these into a dataframe: # put into dataframe - long format df &lt;- data.frame(anxiety = c(placebo, drug), group = c(rep(&quot;placebo&quot;, length(placebo)), rep(&quot;drug&quot;, length(drug)) ) ) head(df) ## anxiety group ## 1 15 placebo ## 2 16 placebo ## 3 19 placebo ## 4 19 placebo ## 5 17 placebo ## 6 20 placebo tail(df) ## anxiety group ## 35 16 drug ## 36 16 drug ## 37 17 drug ## 38 14 drug ## 39 10 drug ## 40 14 drug We can visualize these data and group differences by making a boxplot: ## Plot Data of anxiety by group ggplot(df, aes(x=group, y=anxiety, fill=group)) + geom_boxplot(outlier.shape = NA, alpha=.4) + geom_jitter(width=.1) + theme_classic() + scale_fill_manual(values=c(&quot;orange&quot;, &quot;brown&quot;)) Looking at these data, it seems that the anti-anxiety drug group does have subjects with generally lower overall anxiety scores than the placebo group. A logical next step would be to think about doing an independent t-test. To check whether our data are normally distributed, and therefore appropriate for an independent t-test, we need to perform a Shapiro-Wilk test on the data: # test if each group is approximately normally distributed shapiro.test(drug) ## ## Shapiro-Wilk normality test ## ## data: drug ## W = 0.95184, p-value = 0.3688 shapiro.test(placebo) ## ## Shapiro-Wilk normality test ## ## data: placebo ## W = 0.88372, p-value = 0.02494 As you can see from this output, this suggests that the placebo data are not normally distributed as the p-value is less than 0.05. This means that continuing with the independent t-test would be inappropriate. An alternative approach would be to do whats called a non-parametric alternative to the independent t-test. This test is called Wilcoxon Ranked Sum Test (also called a Mann-Whitney U test). Essentially this test ranks the data from both groups and examines whether one group has significantly more of the higher or lower ranks. Practically, what it is testing is whether there is a difference in the medians of the populations that the two samples came from. To run this test in R, we just use the function wilcox.test() and include our two vectors of data: wilcox.test(placebo, drug) # 2-tailed ## Warning in wilcox.test.default(placebo, drug): cannot compute exact p-value with ties ## ## Wilcoxon rank sum test with continuity correction ## ## data: placebo and drug ## W = 286, p-value = 0.0191 ## alternative hypothesis: true location shift is not equal to 0 The default is to run a 2-tailed test. You are given a test-statistic \\(W\\) and a p-value. Here, \\(p=0.0191\\) which indicates that our two samples have significantly different medians. With this test, you will get a warning about not being able to compute an exact p-value with ties. This isnt really important and can be ignored. In fact, you can turn it off by using the following: wilcox.test(placebo, drug, exact=F) # this gets rid of obnoxious warnings, but not a big deal ## ## Wilcoxon rank sum test with continuity correction ## ## data: placebo and drug ## W = 286, p-value = 0.0191 ## alternative hypothesis: true location shift is not equal to 0 We can also get a 95% confidence interval of the difference in population medians between the two groups by including conf.int=T: wilcox.test(placebo, drug, exact=F, conf.int = T) ## ## Wilcoxon rank sum test with continuity correction ## ## data: placebo and drug ## W = 286, p-value = 0.0191 ## alternative hypothesis: true location shift is not equal to 0 ## 95 percent confidence interval: ## 3.072682e-05 4.000088e+00 ## sample estimates: ## difference in location ## 2.000002 This tells us that the lower bound of the confidence interval is 0.000031 and the upper bound is 4.0. If your data are in a dataframe, you can run the test if your data are in long format like this: head(df) ## anxiety group ## 1 15 placebo ## 2 16 placebo ## 3 19 placebo ## 4 19 placebo ## 5 17 placebo ## 6 20 placebo wilcox.test(anxiety ~ group, data=df, exact=F, conf.int=T) ## ## Wilcoxon rank sum test with continuity correction ## ## data: anxiety by group ## W = 113, p-value = 0.0191 ## alternative hypothesis: true location shift is not equal to 0 ## 95 percent confidence interval: ## -4.000088e+00 -3.072682e-05 ## sample estimates: ## difference in location ## -2.000002 There is also a one-tailed version of this test, which you can do in the same way as you do for the t-test: wilcox.test(placebo, drug, exact=F, alternative = &quot;greater&quot;) # 1-tailed ## ## Wilcoxon rank sum test with continuity correction ## ## data: placebo and drug ## W = 286, p-value = 0.00955 ## alternative hypothesis: true location shift is greater than 0 Finally, as with t-tests, we should also report effect sizes for Wilcoxon Ranked Sum tests. This is because just determining there is a significant difference in population medians, does not tell us enough about how large this difference is. There are many different effect size measures that exist, all of which effectively evaluate the degree to which one group has higher ranks of data than the other group. Without going into formulaic details, the easiest way to calculate this effect size is to use the function wilcoxonR() from the package rcompanion. This function needs your data to be in long-data format. library(rcompanion) wilcoxonR(x = df$anxiety, g = df$group, ci = T) ## r lower.ci upper.ci ## 1 -0.373 -0.632 -0.0788 We can ignore the sign here. What we notice is that the effect size is 0.37, with a possible range of 0.05 to 0.63. With this effect size measure, medium effects are above 0.3, and large effects are over 0.5. 11.12 Non-parametric Alternatives to the Two Sample t-tests Likewise if Paired Data are not normal, you should not perform a paired t-test. Instead, you can use a Wilcoxon Signed Rank Test. This is basically the non-parametric version of the one-sample t-test. It essentially assesses how likely your difference scores are to come from a population whose population median is not equal to 0. In this example, we look at the bloodwork dataset. The two columns called immuncount and immuncount2 refer to before and after measurements of some immune cells in blood. df1 &lt;- read_csv(&quot;data/bloodwork.csv&quot;) %&gt;% select(ids, immuncount, immuncount2) head(df1) ## # A tibble: 6 x 3 ## ids immuncount immuncount2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 0.993 0.921 ## 2 GBH 1.18 1.19 ## 3 EDH 4.34 3.21 ## 4 AAA 2.56 4.01 ## 5 AJF 6.45 9.13 ## 6 FJC 3.97 2.85 If we examine whether the data are approximately normal, we find out that they are definitely not approximately normally distributed as the Shapiro-Wilk test on each give p-values far below 0.05: shapiro.test(df1$immuncount) ## ## Shapiro-Wilk normality test ## ## data: df1$immuncount ## W = 0.9028, p-value = 0.00984 shapiro.test(df1$immuncount2) ## ## Shapiro-Wilk normality test ## ## data: df1$immuncount2 ## W = 0.80749, p-value = 9.006e-05 We can visualize the differences between time point 1 and time point 2 using a scatterplot: # scatterplot ggplot(df1, aes(x = immuncount, y=immuncount2 )) + geom_point() + theme_classic() + geom_abline(intercept =0 , slope = 1) Looking at this, it looks like lots of low values are close to the line, some middle values are below the line and some are a long way above the line. We can next test if the population median of the difference scores is equal to 0 or not. We use the wilcox.test function but use the paired=T argument to ensure we do a paired test. ## Wilcoxon Signed Rank Test wilcox.test(df1$immuncount, df1$immuncount2, paired=T, conf.int = T) #with CIs ## ## Wilcoxon signed rank exact test ## ## data: df1$immuncount and df1$immuncount2 ## V = 202, p-value = 0.5425 ## alternative hypothesis: true location shift is not equal to 0 ## 95 percent confidence interval: ## -1.1615 0.1365 ## sample estimates: ## (pseudo)median ## -0.1345 This gives us confidence intervals of the population median in differences cores of -0.135[-1.162, 0.137] which includes 0 and therefore suggests that there is no difference in medians between the paired groups. The p-value confirms this being \\(p=0.54\\). The \\(V\\) value is the test-statistic. Should you need a 1-tailed test you can do it this way - this time we use alternative=\"less\" as were seeing if time point 1 has a lower median that time point 2 across individuals: wilcox.test(df1$immuncount, df1$immuncount2, paired=T, alternative = &quot;less&quot;) # 1-tailed ## ## Wilcoxon signed rank exact test ## ## data: df1$immuncount and df1$immuncount2 ## V = 202, p-value = 0.2713 ## alternative hypothesis: true location shift is less than 0 Again, we have an effect size measure for this test. This can be done using the wilcoxonPairedR() function from the rcompanion package. The downside of this function is that it requires the data to be ordered specifically. The top half of the data are the values for group1. The bottom half of the data are the values for group2. Then the id variable needs to be in the same order for both groups. df2 &lt;- df1 %&gt;% pivot_longer(2:3) %&gt;% arrange(name,ids) head(df2) ## # A tibble: 6 x 3 ## ids name value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 AAA immuncount 2.56 ## 2 ACC immuncount 4.66 ## 3 AGC immuncount 1.44 ## 4 AJF immuncount 6.45 ## 5 BED immuncount 1.18 ## 6 BFB immuncount 0.724 wilcoxonPairedR(x = df2$value, g = df2$name, ci = T) ## r lower.ci upper.ci ## 1 -0.114 -0.445 0.287 What we can see from this output is that the estimate of the effect size is -0.11[-0.45, 0.27]. This suggests that the likely effect size is 0.11 but 0 is also likely to be the effect size as it is inside the confidence interval. This is further evidence that there is no real effect of changes in scores from time point 1 to time point 2 in these data. "],["correlation.html", "Chapter 12 Correlation 12.1 Pearson Correlation 12.2 Cross-products 12.3 Conducting a Pearson Correlation Test 12.4 Assumptions of Pearsons Correlation 12.5 Confidence Intervals for r 12.6 Partial Correlations 12.7 Non-parametric Correlations 12.8 Point-Biserial Correlation", " Chapter 12 Correlation Correlation is the simplest measure of association between two continuous variables. There are a number of different types of correlation measures that will explore in this chapter. All of these measures attempt to define how changes in one variable are associated with changes in another variable. 12.1 Pearson Correlation Pearsons correlation is measured by \\(r\\) and ranges between -1 and +1. +1 indicates that the variables X and Y are maximally positively correlated, such that as values of X increase so do values of Y. -1 indicates a completely negative correlation such that as values of X increase, values of Y decrease. A value of 0 indicates that there is no overall relationship. The below image shows a positive correlation of \\(r=0.6\\), a zero correlation of \\(r=0\\) and a negative correlation of \\(r=-0.6\\) for 20 datapoints in each scatterplot. The below image shows scatterplots, each with a sample size of 30. The trendline is to help demonstrate how correlations of different magnitudes look in terms of their association. Correlations 12.2 Cross-products The formula for calculating the Pearsons correlation coefficient for a sample is: \\(\\Large r = \\frac{\\sum_{}^{} z_{x}z_{y}}{n - 1}\\) When we have a population, we can use the formula: \\(\\Large \\rho = \\frac{\\sum_{}^{} z_{x}z_{y}}{N}\\) Notice that for a population we use a different notation for the Pearsons correlation coefficient. Essentially, the steps are to convert all the X and Y scores into their respective z-scores. Then you multiply these two values together to get the cross-product. After summing up all the cross-products for each data point, we divide this number by n-1 if were dealing with a sample (we usually are), or N if were dealing with a population. The sum of the cross-products will therefore be largely positive if positive z-scores are multiple together or if negative z-scores are multiplied together. The sum of the cross-products will be largely negative if negative z-scores are multiplied with positive z-scores. The following example should help make this clearer. Look at the following data, its scatterplot and the correlation coefficient. They show that we have a positive correlation of r=0.84. Lets break it down how we got that value. library(tidyverse) x &lt;- c(1.1, 1.5, 2.1, 3.5, 3.6, 3.5, 2.6, 5.6, 4.4, 3.9) y &lt;- c(2.8, 2.9, 1.6, 5.5, 4.7, 8.1, 3.3, 7.7, 7.1, 5.8) df &lt;- data.frame(x, y) df ## x y ## 1 1.1 2.8 ## 2 1.5 2.9 ## 3 2.1 1.6 ## 4 3.5 5.5 ## 5 3.6 4.7 ## 6 3.5 8.1 ## 7 2.6 3.3 ## 8 5.6 7.7 ## 9 4.4 7.1 ## 10 3.9 5.8 ggplot(df, aes(x = x, y = y)) + geom_point(size=2) cor(x,y) ## [1] 0.8418262 First, lets calculate the means and standard deviation (using sd so a sample standard deviation) of x and y. We need to get these values so we can calculate the z-scores of each. # step 1: Get the mean and sd of x and y mean(x) ## [1] 3.18 sd(x) ## [1] 1.370158 mean(y) ## [1] 4.95 sd(y) ## [1] 2.259916 Now, we can calculate the z-scores, remembering that the formula for that is: \\(\\Large z = \\frac{x - \\overline{x}}{s_{x}}\\) # step 2. Calculate z-scores of x, and z-scores of y. df$zx &lt;- (x - mean(x)) / sd(x) # z scores of x df$zy &lt;- (y - mean(y)) / sd(y) # z scores of y df ## x y zx zy ## 1 1.1 2.8 -1.5180729 -0.9513626 ## 2 1.5 2.9 -1.2261358 -0.9071132 ## 3 2.1 1.6 -0.7882302 -1.4823557 ## 4 3.5 5.5 0.2335497 0.2433718 ## 5 3.6 4.7 0.3065340 -0.1106236 ## 6 3.5 8.1 0.2335497 1.3938569 ## 7 2.6 3.3 -0.4233088 -0.7301155 ## 8 5.6 7.7 1.7662195 1.2168592 ## 9 4.4 7.1 0.8904082 0.9513626 ## 10 3.9 5.8 0.5254868 0.3761201 Following this, we simply multiple the z-scores of x and y against each other for every data point: # step 3. Calculate the cross-product: zx * zy df$zxzy &lt;- df$zx * df$zy df ## x y zx zy zxzy ## 1 1.1 2.8 -1.5180729 -0.9513626 1.44423785 ## 2 1.5 2.9 -1.2261358 -0.9071132 1.11224399 ## 3 2.1 1.6 -0.7882302 -1.4823557 1.16843751 ## 4 3.5 5.5 0.2335497 0.2433718 0.05683941 ## 5 3.6 4.7 0.3065340 -0.1106236 -0.03390988 ## 6 3.5 8.1 0.2335497 1.3938569 0.32553483 ## 7 2.6 3.3 -0.4233088 -0.7301155 0.30906432 ## 8 5.6 7.7 1.7662195 1.2168592 2.14924036 ## 9 4.4 7.1 0.8904082 0.9513626 0.84710104 ## 10 3.9 5.8 0.5254868 0.3761201 0.19764615 We now have all of our cross-products. Notice why the majority are positive. This is because we have multiplied positive \\(z_{x}\\) with positive \\(z_{y}\\) or we multiplied negative \\(z_{x}\\) with negative \\(z_{y}\\). This happens because datapoints that tend to be above the mean for x are also above the mean for y, and points that are below the mean of x are also below the mean of y. We can add this up to get the sum of the cross-products. That is the \\(\\sum_{}^{} z_{x}z_{y}\\) in the formula. # step 4. Sum up the cross products. sum(df$zxzy) # 7.58 ## [1] 7.576436 We now divide that by n-1 as we have a sample, to get the correlation coefficient r. That gives us an estimation of the average cross-product. # step 5- calculate &#39;r&#39; by dividing by n-1. (for a sample) sum(df$zxzy) / 9 # our n was 10, so n-1 = 9 ## [1] 0.8418262 sum(df$zxzy) / (nrow(df) - 1) # nrow(df) is more generalizable ## [1] 0.8418262 # r=0.84 Just as a quick second example, here is a work through calculating a negative correlation. Notice the \\(z_{x}\\) and \\(z_{y}\\) scores that are multiplied together. They are largely opposite in terms of signs. This is what leads to a negative sum of cross-products and the negative correlation. Why? Because data points that are above the mean for x are generally below the mean in terms of y and visa-versa. ### Example 2. Negative Correlation. x &lt;- c(1.1, 1.5, 2.1, 3.5, 3.6, 3.5, 2.6, 5.6, 4.4, 3.9) y &lt;- c(10.4, 10.0, 8.4, 8.5, 8.4, 6.3, 7.1, 6.2, 8.1, 10.0) df &lt;- data.frame(x, y) ggplot(df, aes(x = x, y = y)) + geom_point(size=2) cor(df$x,df$y) ## [1] -0.6112965 Here is the code, truncated for space: # Calculate z-scores for each x and each y df$zx &lt;- (x - mean(x)) / sd(x) df$zy &lt;- (y - mean(y)) / sd(y) # Calculate the cross-product: zx * zy df$zxzy &lt;- df$zx * df$zy # let&#39;s look at the dataframe # notice the cross products: df ## x y zx zy zxzy ## 1 1.1 10.4 -1.5180729 1.37762597 -2.09133671 ## 2 1.5 10.0 -1.2261358 1.11012578 -1.36116500 ## 3 2.1 8.4 -0.7882302 0.04012503 -0.03162776 ## 4 3.5 8.5 0.2335497 0.10700008 0.02498983 ## 5 3.6 8.4 0.3065340 0.04012503 0.01229968 ## 6 3.5 6.3 0.2335497 -1.36425096 -0.31862038 ## 7 2.6 7.1 -0.4233088 -0.82925058 0.35102907 ## 8 5.6 6.2 1.7662195 -1.43112601 -2.52768263 ## 9 4.4 8.1 0.8904082 -0.16050011 -0.14291061 ## 10 3.9 10.0 0.5254868 1.11012578 0.58335643 # Sum up the cross products and Calculate &#39;r&#39; by dividing by N-1. sum(df$zxzy) / (nrow(df) - 1) ## [1] -0.6112965 cor(df$x,df$y) ## [1] -0.6112965 12.3 Conducting a Pearson Correlation Test Although cor() gives you the correlation between two continuous variables, to actually run a significance test, you need to use cor.test(). Lets use some BlueJay data to do this. Well just use data on male birds. library(tidyverse) jays &lt;- read_csv(&quot;data/BlueJays.csv&quot;) jayM &lt;- jays %&gt;% filter(KnownSex == &quot;M&quot;) # we&#39;ll just look at Males nrow(jayM) # 63 observations ## [1] 63 head(jayM) ## # A tibble: 6 x 9 ## BirdID KnownSex BillDepth BillWidth BillLength Head Mass Skull Sex ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0000-00000 M 8.26 9.21 25.9 56.6 73.3 30.7 1 ## 2 1142-05901 M 8.54 8.76 25.0 56.4 75.1 31.4 1 ## 3 1142-05905 M 8.39 8.78 26.1 57.3 70.2 31.2 1 ## 4 1142-05909 M 8.71 9.84 25.5 57.3 74.9 31.8 1 ## 5 1142-05912 M 8.74 9.28 25.4 57.1 75.1 31.8 1 ## 6 1142-05914 M 8.72 9.94 30 60.7 78.1 30.7 1 Lets say youre interested in examining whether there is an association between Body Mass and Head Size. First well make a scatterplot between the Mass and Head columns. Well also investigate the correlation using cor(). ggplot(jayM, aes(x=Mass, y=Head)) + geom_point(shape = 21, colour = &quot;navy&quot;, fill = &quot;dodgerblue&quot;) + stat_smooth(method=&quot;lm&quot;, se=F) cor(jayM$Mass, jayM$Head) # r = 0.58, a strong positive correlation. ## [1] 0.5773562 To run the significance test, we do the following: cor.test(jayM$Head, jayM$Mass) ## ## Pearson&#39;s product-moment correlation ## ## data: jayM$Head and jayM$Mass ## t = 5.5228, df = 61, p-value = 7.282e-07 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3846090 0.7218601 ## sample estimates: ## cor ## 0.5773562 This gives us a lot of information. Firstly, at the bottom it repeats the correlation coefficient cor. At the top, it gives us the value of t which is essentially how surprising it is for us to get the correlation we did assuming we were drawing our sample from a population where there is no correlation. Associated with this t value is the degrees of freedom which is equal to n-2, so in this case that is 63-2 = 61. The p-value is also given. If we are using alpha=0.05 as our significance level, then we can reject the hypothesis that there is no overall correlation in the population between Body Mass and Head size if p&lt;0.05. The default for cor.test() is to do a two-tailed test. This is testing whether your observed correlation r is different from r=0 in either the positive or negative direction. This default version also gives us the confidence interval for the correlation coefficient. Essentially, this gives us the interval in which we have a 95% confidence that the true population r lies (remember we just have data from one sample that theoretically comes from a population). Its also possible however that you had an a priori prediction about the direction of the effect. For instance, you may have predicted that Body Mass would be positively correlated with Head Size. In this case, you could do a one-tailed correlation test, where your alternative hypothesis is that there is a positive correlation and the null is that the correlation coefficient is equal to 0 or less than 0. To do one-tailed tests you need to add the alternative argument. # testing if there is a positive correlation cor.test(jayM$Head, jayM$Mass, alternative = &quot;greater&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: jayM$Head and jayM$Mass ## t = 5.5228, df = 61, p-value = 3.641e-07 ## alternative hypothesis: true correlation is greater than 0 ## 95 percent confidence interval: ## 0.4187194 1.0000000 ## sample estimates: ## cor ## 0.5773562 # testing if there is a negative correlation cor.test(jayM$Head, jayM$Mass, alternative = &quot;less&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: jayM$Head and jayM$Mass ## t = 5.5228, df = 61, p-value = 1 ## alternative hypothesis: true correlation is less than 0 ## 95 percent confidence interval: ## -1.0000000 0.7017994 ## sample estimates: ## cor ## 0.5773562 12.3.1 Significance Testing a Pearson Correlation In the above section we demonstrated how to do a one-tailed and two-tailed significance test in R. Lets discuss a little more the output that is produced from cor.test() and what its relevance is. Most importantly, we need to discuss the observed \\(t\\) statistic and the degrees of freedom. As already described above, the degrees of freedom are equal to \\(n-2\\). The \\(t\\) value represents how likely it was to get the sample correlation that we got in our sample if the true population correlation was actually \\(\\rho=0\\). There are several ways to calculate \\(t\\), but here is a nice shortcut formula: \\(\\Large t = r\\sqrt{\\frac{n-2}{1-r^2}}\\) Youll also see this same formula written like this: \\(\\Large t = \\frac{r}{\\sqrt{\\frac{1-r^2}{n-2}}}\\) So, as an example, if we calculated a correlation coefficient of \\(r=0.42\\) from a sample size of 19, then we could calculate our observed \\(t\\) value as follows: r &lt;- 0.42 n &lt;- 19 tval &lt;- r * sqrt( (n-2) / (1 - r^2) ) tval ## [1] 1.908163 We calculate the observed \\(t\\) value to be \\(t = 1.91\\). We can visualize our sampling distribution of possible correlations as a \\(t\\)-distribution with degrees of freedom equal to 17 (19-2). Like with other t-distributions, we can calculate how likely we were to get our observed value of \\(t\\) by calculating the area under the curve in the tails. For a positive \\(t\\)-value we want to know the area under the curve for values greater than our \\(t\\)-value. That would tell us how likely we were to get that value. We do this in the same way as with t-tests, using pt(). We put in our \\(t\\)-value, and our degrees of freedom. Because pt() returns the area under the curve to the left of our \\(t\\)-value, we need to do 1 - pt() to get the area to the right: 1 - pt(tval, df=n-2) ## [1] 0.03670244 Therefore, our p-value here is \\(p=0.04\\). But, be careful. This is only the p-value for a one-tailed test. If we are conducting a two-tailed significance test, then we need to account for samples that are as extremely negative as well as extremely positive as our observed value. We must double the p-value to account for the area under both tails. 2 * (1 - pt(tval, df=n-2)) ## [1] 0.07340487 Our p-value is \\(p=0.073\\) for a two-tailed test. Below is a sample of data that actually has a correlation of \\(r=0.42\\) and a sample size of 19. x &lt;- c(-0.73, 0.89, 1.36, -1.30, 1.07, 0.76, -1.32, -1.31, 0.46, 2.09, 0.36, -0.70, -1.34, -1.99, -0.39, -0.51, -0.92, 1.37, 2.31) y &lt;-c(0.69, 0.29, -0.52, -0.05, -1.08, 0.37, -0.31, -0.33, 0.76, 1.37, 0.91, 0.19, 0.10, -2.00, -1.72, 0.53, -0.52, -0.20, 0.63) dfxy &lt;- data.frame(x=x, y=y) ggplot(dfxy, aes(x=x, y=y)) + geom_point(size=2, color = &quot;#123abc&quot;, alpha=.7)+ stat_smooth(method=&quot;lm&quot;, se=F) + theme_classic() Lets look at the output if we conduct a two-tailed significance test on our sample \\(r\\): cor.test(x,y) ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = 1.9081, df = 17, p-value = 0.07341 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.0422865 0.7341500 ## sample estimates: ## cor ## 0.4199895 We get our correlation of \\(r=0.42\\), degrees of freedom = 17, and observed \\(t\\)-value of \\(t=1.91\\). We also see the p-value of \\(p=0.07\\). If we were to do a one-tailed test (assuming we had an a priori prediction as to the direction of the correlation): cor.test(x,y, alternative = &quot;greater&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = 1.9081, df = 17, p-value = 0.03671 ## alternative hypothesis: true correlation is greater than 0 ## 95 percent confidence interval: ## 0.03644971 1.00000000 ## sample estimates: ## cor ## 0.4199895 Now we see that we get the same \\(t\\)- and p-values, but the probability is half as with the two-tailed test. Additionally the R output of cor.test() gives us a 95% confidence interval around our sample Pearson correlation r. Remember, r is just an estimate of the true population correlation coefficient \\(\\rho\\). Therefore, this 95% confidence interval represents a measure of how certain we are in this estimate. We should use the 95% confidence interval that is outputted when we do a two-tail test. We discuss this in more detail below. 12.4 Assumptions of Pearsons Correlation The Pearson Correlation Coefficient requires your data to be approximately normally distributed. To do this we have various options how to test for normality. Firstly, we could do a Shapiro-Wilk test, which formally determines whether our data are normal. This is done using shapiro.test(), where we assume our data are from a normal population if the resulting p-value is above 0.05. If the p-value is below 0.05 then we have evidence to reject that our data come from a normal population. With our data above, this would look like this when running the test on each variable: shapiro.test(jayM$Mass) # P &gt; 0.05, therefore cannot reject null that data is not normal ## ## Shapiro-Wilk normality test ## ## data: jayM$Mass ## W = 0.97222, p-value = 0.1647 shapiro.test(jayM$Head) # P &gt; 0.05, therefore cannot reject null that data is not normal ## ## Shapiro-Wilk normality test ## ## data: jayM$Head ## W = 0.96521, p-value = 0.07189 We can also make a QQ-plot for each variable. Essentially what we require from this plot is for the majority of our data to fall on the straight line - especially the datapoints in the middle. Some deviation at the tails is ok. This plot orders our data and plots the observed data against values on the x-axis that we would expect to get if our data was truly from a normal population. qqnorm(jayM$Mass) qqline(jayM$Mass, col = &quot;steelblue&quot;, lwd = 2) qqnorm(jayM$Head) qqline(jayM$Head, col = &quot;steelblue&quot;, lwd = 2) Both of these QQ plots are ok, and indicate normality, as does our Shapiro-Wilk tests. Therefore we would be ok to use a Pearson Correlation test with these data. What should you do though if either of your continuous variables are not approximately normally distributed? In that case, there are other correlation coefficients and associated significance tests that you could run instead. We describe these in more detail in Section 12.7. 12.5 Confidence Intervals for r When we run a correlation on our data we get one correlation coefficient \\(r\\) from our sample. Ideally we would be able to give a confidence interval around this correlation coefficient to describe how certain or uncertain we are in the correlation coefficient. This is possible to do, but its a bit long-winded. Lets look at how it is done. The main thing to realize is that our sample of data that we calculate our observed correlation coefficient from is technically just one sample that we could have got from a population. We could have picked a slightly different sample, and got a slightly different correlation. Lets work through this using an example. Say we have a population of 25,000 subjects (datapoints) and the true population correlation coefficient is \\(\\rho=0.75\\) (When were dealing with a population we use \\(\\rho\\) for the Pearsons correlation coefficient rather than \\(r\\).). This is our population. We have 25,000 rows of x and y data: dfr &lt;- read_csv(&quot;data/popcor.csv&quot;) head(dfr) ## # A tibble: 6 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 6.45 4.89 ## 2 5.31 3.86 ## 3 4.68 6.31 ## 4 7.29 6.30 ## 5 5.70 4.58 ## 6 6.72 5.52 tail(dfr) ## # A tibble: 6 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5.88 4.26 ## 2 4.01 3.17 ## 3 5.06 5.47 ## 4 7.00 6.27 ## 5 8.60 8.18 ## 6 3.84 4.12 nrow(dfr) ## [1] 25000 We could plot this to examine the population data: ggplot(dfr, aes(x=x,y=y))+ geom_point(alpha=.05) + theme_classic() + stat_smooth(method=&#39;lm&#39;,se=F) + ggtitle(&quot;Population of 25,000 r=0.75&quot;) We can check the correlation for the population and we find that \\(\\rho=0.75\\). cor(dfr$x, dfr$y) ## [1] 0.75 If we were interested in trying to find the correlation between x and y but were unable to measure all 25,000 individuals in the population, we might pick one sample to test. Imagine we pick a sample size of \\(n=15\\). Lets pick 15 datapoints at random from our population and find out the correlation coefficient \\(r\\) of this sample: set.seed(1) # so we get the same results # this code selects 15 rows at random samp1 &lt;- dfr[sample(1:nrow(dfr),15, T),] samp1 ## # A tibble: 15 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4.71 3.69 ## 2 4.97 4.73 ## 3 5.99 4.79 ## 4 5.56 3.78 ## 5 5.12 5.58 ## 6 5.91 6.82 ## 7 4.51 5.40 ## 8 5.50 6.36 ## 9 4.85 6.28 ## 10 4.39 4.52 ## 11 5.10 4.37 ## 12 4.56 4.38 ## 13 3.83 4.34 ## 14 6.92 7.26 ## 15 5.38 4.70 We can make a quick plot of our sample of 15, and calculate the sample correlation: ggplot(samp1, aes(x=x, y=y)) + geom_point(size=2) + theme_classic() + stat_smooth(method=&#39;lm&#39;,se=F) cor(samp1$x, samp1$y) ## [1] 0.5706988 In our sample we get \\(r=0.80\\) which is a little bit higher than our population correlation. What if we take a few more samples of size 15? What would we get for those sample correlations? Lets do it: samp2 &lt;- dfr[sample(1:nrow(dfr),15, T),] cor(samp2$x, samp2$y) ## [1] 0.732974 samp3 &lt;- dfr[sample(1:nrow(dfr),15, T),] cor(samp3$x, samp3$y) ## [1] 0.6396087 samp4 &lt;- dfr[sample(1:nrow(dfr),15, T),] cor(samp4$x, samp4$y) ## [1] 0.7624823 samp5 &lt;- dfr[sample(1:nrow(dfr),15, T),] cor(samp5$x, samp5$y) ## [1] 0.4712707 The next three samples have correlations that are also higher than the population correlation. The fifth sample we collected is much lower with the sample correlation being \\(r=0.55\\). As you might have worked out by now, if we keep doing this over and over again (thousands of times) we would end up with a sampling distribution of correlation coefficients. Lets do that: #this code is to get 50,000 correlation coefficients from samples of n=15 results &lt;- vector(&#39;list&#39;,50000) for(i in 1:50000){ samp &lt;- dfr[sample(1:nrow(dfr),15, T),] results[[i]] &lt;- cor(samp$x, samp$y) } dfr.results &lt;- data.frame(r = unlist(results)) head(dfr.results) ## r ## 1 0.7350283 ## 2 0.8708844 ## 3 0.8753898 ## 4 0.7909757 ## 5 0.7408920 ## 6 0.7744925 Because we now have a sampling distribution of correlation coefficients, we can plot this in a histogram. ggplot(dfr.results, aes(x=r)) + geom_histogram(aes(y = ..density..), color = &quot;black&quot;, fill = &quot;purple&quot;, alpha=.4, binwidth = .01) + geom_density(alpha = 0.7, fill = &quot;mistyrose&quot;) + theme_classic() + xlab(&quot;Sample correlation r&quot;) + ggtitle(&quot;Sampling Distribution of Correlation Coefficients&quot;)+ geom_vline(xintercept=0.75, lty=1, lwd=1, color=&quot;red&quot;) What is the first thing that you notice about this sampling distribution? Hopefully it is that it is not symmetrical. It is highly skewed. This is because correlation coefficients are bounded between -1 and +1. We do not get an approximately normally distributed sampling distribution (with the one exception being when the population correlation \\(\\rho=0\\). The red line represents our original population coefficient of \\(\\rho=0.75\\). There is however a trick that we can employ to make this sampling distribution approximately normal. We can do something called Fisher Transform our sample correlations in the sampling distribution. So for every value of \\(r\\) in our sampling distribution, we can apply the following formula: \\(\\Large z&#39; = 0.5 \\times \\log(\\frac{1 + r}{1 - r})\\) \\(z&#39;\\) is referred to as the Fisher Transformed \\(r\\) value. So, if we had a sample correlation of \\(r=0.56\\), that would equate to a \\(z&#39;\\) value of \\(z&#39;=0.63\\): 0.5 * log( (1+0.56) / (1-0.56) ) ## [1] 0.6328332 Alternatively, a correlation of \\(r=0.75\\) would have a Fisher transformed score of \\(z&#39;=0.97\\): 0.5 * log( (1+0.75) / (1-0.75) ) ## [1] 0.9729551 Notably, a correlation of \\(r=0.0\\) has a Fisher transformed score of \\(z&#39;=0.00\\): 0.5 * log( (1+0) / (1-0) ) ## [1] 0 If we apply this formula to all the \\(r\\) values in our sampling distribution and replot the distribution, it now looks like this: #put Fisher transformed scores into a column called &#39;zt&#39; dfr.results$zt &lt;- 0.5 * log( (1+dfr.results$r) / (1-dfr.results$r) ) head(dfr.results) ## r zt ## 1 0.7350283 0.9395780 ## 2 0.8708844 1.3367291 ## 3 0.8753898 1.3556909 ## 4 0.7909757 1.0740326 ## 5 0.7408920 0.9524541 ## 6 0.7744925 1.0314583 ggplot(dfr.results, aes(x=zt)) + geom_histogram(aes(y = ..density..), color = &quot;black&quot;, fill = &quot;purple&quot;, alpha=.4, binwidth = .05) + geom_density(alpha = 0.7, fill = &quot;mistyrose&quot;) + theme_classic() + xlab(&quot;Fisher Transformed z&#39;&quot;) + ggtitle(&quot;Sampling Distribution of Correlation Coefficients&quot;)+ geom_vline(xintercept=0.97, lty=1, lwd=1, color=&quot;red&quot;) This distribution is approximately symmetrical. The red line here represents the population correlation coefficient of \\(r=0.75\\) which is \\(z&#39;=0.97\\). This is the approximate mean of the distribution. The standard deviation of this transformed sampling distribution can be calculated by this formula: \\(\\Large se = \\sqrt{\\frac{1}{n-3}}\\) So, because we have a sample size of \\(n=15\\) the standard deviation of this sampling distribution is \\(se = 0.289\\): sqrt(1/12) ## [1] 0.2886751 Now we have an approximately normal sampling distribution with a standard deviation, we can start to do similar things to what we did with other sampling distributions such as calculate confidence intervals. Lets calculate a confidence interval around a sample of \\(r=0.56\\). What we assume is that the Fisher transformed score of \\(r=0.56\\) is the mean of the sampling distribution. Therefore we assume that \\(z&#39; = 0.63\\) is the mean of the sampling distribution. Because we want to have 95% of the distribution inside the confidence interval, then we want to know which values are 1.96 standard deviations away from the mean. This is because were assuming the transformed sampling distribution to be approximately normal, and in a standard normal curve 95% of the data is \\(\\pm 1.96\\) standard deviations of the mean (see section 11.2. The formula is: \\(CI_{95\\%} = z&#39; + (1.96 \\times se)\\) In our example, 1.96 standard deviations away from the mean is: 0.63 + (1.96 * 0.289) ## [1] 1.19644 0.63 - (1.96 * 0.289) ## [1] 0.06356 We can visualize this as follows: Our confidence interval in terms of Fisher Transformed z values is \\(z&#39; = 0.63[0.06, 1.20]\\). One thing we can immediately draw from that is that a correlation of \\(r=0\\) is unlikely to be the true population correlation. This is because when \\(r=0\\), the transformed value is also equal to 0 and \\(z&#39;=0\\) is not within the 95% confidence interval. Also, it is notable that our true population correlation coefficient of $ rho=0.75$ which equates to \\(z&#39;=0.97\\) is included within the correlation coefficient. As with other 95% confidence interval measures, what this really means is that in 95% of our samples we will capture the true population correlation coefficient when we create confidence intervals around our sample correlation coefficient. Although we can directly determine if a population correlation of \\(\\rho=0\\) is within our confidence interval or not, leaving the confidence interval in Fisher transformed \\(z&#39;\\) values isnt that interpretable. Therefore, you can transform \\(z&#39;\\) values back to \\(r\\) values using the following formula: \\(\\Large r = \\frac{\\exp(2z&#39;)-1}{1 + \\exp(2z&#39;)}\\) So for each bit of our confidence interval, transforming the \\(z&#39;\\) values back to \\(r\\) values we get the following: (exp(2 * 0.63) - 1) / ( 1 + exp(2 * 0.63)) ## [1] 0.5580522 (exp(2 * 0.06) - 1) / ( 1 + exp(2 * 0.06)) ## [1] 0.0599281 (exp(2 * 1.20) - 1) / ( 1 + exp(2 * 1.20)) ## [1] 0.8336546 Our 95% confidence interval in terms of \\(r\\) is therefore \\(r = 0.56[0.06, 0.83]\\) which is very wide! but doesnt include \\(r=0\\). If we were to increase our sample size then our confidence intervals would get tighter - and our individual estimate of the correlation coefficient would get close to the true population correlation coefficient \\(\\rho\\). Correlation Confidence Interval Example Because the above is a little bit tricky, lets look at a second example. In a study, researchers found a correlation of \\(r= -0.654\\) based on 34 observations. What is the 95% confidence interval of this correlation ? First, convert the \\(r\\) value to a Fisher Transformed \\(z&#39;\\) value and assume that is the mean of the symmetrical sampling distribution, and we get \\(z&#39; = -0.78\\): 0.5 * log( (1 + -0.654) / (1 - -0.654) ) ## [1] -0.7822566 Second, we know the standard deviation of this symmetrical sampling distribution is equal to \\(se = \\sqrt{\\frac{1}{n-3}}\\), so its equal to 0.1796: sqrt(1 / 31) ## [1] 0.1796053 Third, we want to know the values of \\(z&#39;\\) that are 1.96 times the sampling distribution standard deviation from the mean, to get the lower and upper bounds of our confidence intervals: -0.7822566 + (1.96 * 0.1796053) ## [1] -0.4302302 -0.7822566 - (1.96 * 0.1796053) ## [1] -1.134283 In terms of Fisher transformed values, our 95% confidence interval is therefore \\(z&#39; = -0.78[-1.13, -0.43]\\). Notice that 0 is not inside the confidence interval, suggesting that the population correlation coefficient is not equal to 0. We can also convert all of these back to \\(r\\) values: (exp(2 * -0.78) - 1) / ( 1 + exp(2 * -0.78)) ## [1] -0.6527067 (exp(2 * -1.13) - 1) / ( 1 + exp(2 * -1.13)) ## [1] -0.8110193 (exp(2 * -0.43) - 1) / ( 1 + exp(2 * -0.43)) ## [1] -0.4053213 So our 95% Confidence interval is \\(r = -0.65[-0.41, -0.81]\\). Notice with the larger sample size, we get a much tighter confidence interval. 12.6 Partial Correlations An important consideration when running a correlation is the third variable problem. In brief, this is when we have the situation that a purported association between X and Y is actually being driven by both of their association with a third variable Z. It is important to consider that a correlation may only exist because both variables are correlated with something else. Alternatively, we may wish to get an adjusted correlation between X and Y based on their relationship with Z. This is essentially the theory behind partial correlations. If we have measured the correlation between X and Y (which well call \\(r_{xy}\\)) as well as each of their correlations with Z (which well call \\(r_{xz}\\) and \\(r_{yz}\\) respectively) then we can calculate this adjusted partial correlation between X and Y which well call \\(r_{xy.z}\\). To do this, we use this horrible looking formula: \\(\\Large r_{xy.z} = \\frac{r_{xy} - (r_{xz}\\times r_{yz}) } {\\sqrt{1-r_{xz}^2} \\times \\sqrt{1-r_{yz}^2}}\\) In reality though, its pretty easy to plug the correct values for \\(r\\) or \\(r^2\\) into this formula and get the result required. Lets use an example. In these data we have 50 participants who logged the number of hours that they engaged in a task. We also have a column that shows their score in that task, and a column that records how tired they were. Higher tiredness scores means that they were more tired. (Weve changed this from the column name in the original video to make it clear!) gs &lt;- read_csv(&quot;data/gamescore.csv&quot;) colnames(gs)[4]&lt;-&quot;tiredness&quot; # change this column name to make it more clear nrow(gs) ## [1] 50 head(gs) ## # A tibble: 6 x 4 ## name hours score tiredness ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Ebert, Julia 11.6 95.2 4.5 ## 2 Vasquez, Horacio 8.5 109. 6.1 ## 3 Yoakum, Anthony 8 110. 6.6 ## 4 Kroha, Abagail 10.7 95.4 7.3 ## 5 Baray Perez, Daysi 3 81.2 7.7 ## 6 Mcalister, Katherine 8.5 124. 7.2 Say we predicted that the more hours played would lead to a higher score in the task. We might make our scatterplot and then run a one-tailed Pearsons correlation significance test: # scatterplot ggplot(gs, aes(x=hours, y=score)) + geom_point(color=&quot;navy&quot;) + stat_smooth(method=&quot;lm&quot;,se=F) + theme_classic() cor.test(gs$hours, gs$score, alternative = &quot;greater&quot;) # p=0.049 ## ## Pearson&#39;s product-moment correlation ## ## data: gs$hours and gs$score ## t = 1.6887, df = 48, p-value = 0.04888 ## alternative hypothesis: true correlation is greater than 0 ## 95 percent confidence interval: ## 0.001470868 1.000000000 ## sample estimates: ## cor ## 0.2368152 We can see from this plot that we have a correlation of r=0.24 between hours and score, which is a low to moderate correlation. Our one-tailed significance test also tells us that we have a significant difference of p=0.049 that this is a significantly positive correlation, although the effect is smallish. What if we considered each of these variables relationship with tiredness ? We can calculate the Pearson correlation for tiredness against score and for hours against tiredness, in addition to our original correlation of hours against score. cor(gs$hours, gs$score) # r = 0.24 ## [1] 0.2368152 cor(gs$tiredness, gs$score) # r = -0.21 ## [1] -0.2099467 cor(gs$hours, gs$tiredness) # r = -0.29 ## [1] -0.2867374 As you can see, tiredness is negatively correlated with score, meaning individuals who report themselves as more tired scored lower in the task. Hours was also negatively correlated with tiredness, meaning individuals who were more tired spent fewer hours on the task. Its therefore possible that an individuals tiredness could affect both the number of hours that an individual engages in the task, as well as their overall performance. These relationships may affect the overall observed relationship between hours and score. If we were to do this in R, wed use the pcor.test() function from the ppcor package. We just need to tell it what our original X and Y are as well as our third variable Z. library(ppcor) pcor.test(x=gs$hours, y=gs$score, z=gs$tiredness) ## estimate p.value statistic n gp Method ## 1 0.1885594 0.1944534 1.316311 50 1 pearson This output tells us that the adjust correlation, i.e. the partial correlation for \\(r_{xy}\\) is 0.19. There is a p-value associated with that correlation of p=0.194 which indicates that the correlation is no longer significant. This means that the relationship between test score and hours is no longer significant after you take into account that they are both related to tiredness. We can also look at this by hand using the formula above: r.xy &lt;- cor(gs$hours, gs$score) # r = 0.24 r.xz &lt;- cor(gs$tiredness, gs$score) # r = -0.21 r.yz &lt;- cor(gs$hours, gs$tiredness) # r = -0.29 numerator &lt;- r.xy - (r.xz * r.yz) denominator &lt;- (sqrt(1 - r.xz^2) * (sqrt(1 - r.yz^2)) ) numerator/denominator ## [1] 0.1885594 We get the same result! 12.7 Non-parametric Correlations When at least on of our variables are not normal, then we need to consider alternative approaches to the Pearson correlation for assessing correlations. Lets take this example, where we are interested in seeing if theres an association between saturated fat and cholesterol levels across a bunch of different cheeses: library(tidyverse) cheese &lt;- read_csv(&quot;data/cheese.csv&quot;) head(cheese) ## # A tibble: 6 x 9 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 blue 18.7 0.8 7.78 21.4 2.34 75 0 353 ## 2 brick 18.8 0.784 8.60 23.2 2.79 94 0 371 ## 3 brie 17.4 0.826 8.01 20.8 0.45 100 0 334 ## 4 camembert 15.3 0.724 7.02 19.8 0.46 72 0 300 ## 5 caraway 18.6 0.83 8.28 25.2 3.06 93 0 376 ## 6 cheddar 21.1 0.942 9.39 24.9 1.28 105 0 403 # let&#39;s make a scatterplot of saturated fat against cholesterol ggplot(cheese, aes(x = sat_fat, y = chol)) + geom_point() + stat_smooth(method=&quot;lm&quot;,se=F) It looks like there is a pretty obvious relationship, but lets check the normality of each variable before progressing. Firstly the Shapiro-Wilk tests suggest that our data do not come from a normal distribution: shapiro.test(cheese$sat_fat) # P &lt; 0.05, therefore data may not be normal ## ## Shapiro-Wilk normality test ## ## data: cheese$sat_fat ## W = 0.85494, p-value = 6.28e-07 shapiro.test(cheese$chol) # P &lt; 0.05, therefore data may not be normal ## ## Shapiro-Wilk normality test ## ## data: cheese$chol ## W = 0.90099, p-value = 2.985e-05 Secondly, we have quite dramatic deviation from the straight line of our datapoints in our QQ plots. This indicates that our data are likely skewed. qqnorm(cheese$sat_fat) qqline(cheese$sat_fat, col = &quot;steelblue&quot;, lwd = 2) qqnorm(cheese$chol) qqline(cheese$chol, col = &quot;steelblue&quot;, lwd = 2) We could be thorough and check this by plotting histograms of our data: library(gridExtra) p1 &lt;- ggplot(cheese, aes(x=sat_fat)) + geom_histogram(color=&quot;black&quot;, fill=&quot;lightseagreen&quot;) p2 &lt;- ggplot(cheese, aes(x=chol)) + geom_histogram(color=&quot;black&quot;, fill=&quot;lightseagreen&quot;) grid.arrange(p1,p2,nrow=1) Because our data do not appear to be normal, we cannot do a Pearson correlation coefficient. We should instead use a non-parametric correlation method. There are several of these to choose from. We dont plan to go into the details here of how these methods determine their correlation coefficients or conduct significance test. In brief, these methods generally rank order the datapoints along the x and y axes and then determine how ordered these ranks are with respect to each other. Probably the most commonly used non-parametric correlation test is called the Spearman Rank Correlation test. To run this, we can use cor() to get the correlation or cor.test() to run the significance test in the same way we did the Pearson test. However, the difference here is that we specify method=\"spearman\" at the end. cor(cheese$sat_fat, cheese$chol, method = &quot;spearman&quot;) ## [1] 0.8677042 cor.test(cheese$sat_fat, cheese$chol, method = &quot;spearman&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: cheese$sat_fat and cheese$chol ## S = 8575.9, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.8677042 The correlation coefficient here is 0.87 and is termed rho instead of r. With the significance test, you get a test-statistic S which relates to how well ordered the ranked data are. Also provided is a p-value. As with the Pearson, the default is a 2-tailed test, testing whether the observed correlation could have come from a population with a correlation of 0. If the p-value is below 0.05 (using alpha = 0.05 as our criterion), then that is reasonable evidence that there is a significant correlation. You may also notice with Spearman Rank correlations that you are forever getting warnings about computing p-values with ties. Dont worry at all about this - although this is an issue with the test and how it calculates the p-value, it isnt of any real practical concern. If you were interested in conducting a one-tailed correlation test, you could do that in the same way as you did for the Pearson. For instance, if you predicted that cholesterol and saturated fat would have a positive correlation, you could do the following to do a one-tailed test: cor.test(cheese$sat_fat, cheese$chol, method = &quot;spearman&quot;, alternative = &quot;greater&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: cheese$sat_fat and cheese$chol ## S = 8575.9, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is greater than 0 ## sample estimates: ## rho ## 0.8677042 You may also notice that the output of the Spearman Rank test does not give confidence intervals for the value of rho. This is unfortunate and is one of the drawbacks of doing a non-parametric correlation. Finally, there are several other types of non-parametric correlations you could choose from if you didnt want to do a Spearman Rank correlation. We personally recommend using a method called Kendalls Tau B correlation, which can be done like this: cor.test(cheese$sat_fat, cheese$chol, method = &quot;kendall&quot;) ## ## Kendall&#39;s rank correlation tau ## ## data: cheese$sat_fat and cheese$chol ## z = 8.8085, p-value &lt; 2.2e-16 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## 0.7102531 This output gives you a tau value which is the correlation coefficient, and a p-value which you can interpret in the same way as the other tests. Ranked Data If at least one of your variables of your data are rank (ordinal) data, then you should use non-parametric correlations. In the following example, the data show the dominance rank, age, body size and testosterone levels for a group of 18 animals. Lower numbers of the ranks, indicate a higher ranking animal. An animal with rank 1 means that it is the most dominant individual. Perhaps with such data you may be interested in seeing if there was an association between dominance rank and testosterone levels. Because your dominance rank measure is ordinal (a rank), then you should pick a non-parametric correlation. test &lt;- read_csv(&quot;data/testosterone.csv&quot;) head(test) ## # A tibble: 6 x 4 ## drank age size testosterone ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 13 183 4.8 ## 2 7 9 155 3.9 ## 3 7 5.5 144 3.8 ## 4 1 11.5 201 6.4 ## 5 12 3.5 125 1.8 ## 6 4 10 166 4.3 ggplot(test, aes(x = drank, y = testosterone)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se=F) + xlab(&quot;Dominance Rank&quot;) + ylab(&quot;Testosterone Level&quot;) cor(test$drank, test$testosterone, method = &quot;spearman&quot;) # rho = -0.91 ## [1] -0.9083378 If you had the a priori prediction, that more dominant animals would have higher testosterone, then you could do a one-tailed test. This would mean that you expect there to be a negative correlation - as the rank number gets higher, the levels of testosterone would fall. In this case, youd use alternative = \"less\". cor.test(test$drank, test$testosterone, method = &quot;spearman&quot;, alternative = &quot;less&quot;) # 1- tailed ## ## Spearman&#39;s rank correlation rho ## ## data: test$drank and test$testosterone ## S = 1849.2, p-value = 9.367e-08 ## alternative hypothesis: true rho is less than 0 ## sample estimates: ## rho ## -0.9083378 Non-parametric Partial Correlation If you look back to section 12.6 calculate the partial correlation for a Pearson correlation. We also used the function pcor.test() from the ppcor package to do this for us. We can actually use the same formula or function to do partial correlations for non-parametric correlations. For example, lets examine the Spearman correlations between anxiety, exam score and revision. These are the data: exams &lt;- read_csv(&quot;data/exams1.csv&quot;) nrow(exams) ## [1] 102 head(exams) ## # A tibble: 6 x 5 ## code revise exam anxiety gender ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 4 40 86.3 Male ## 2 2 11 65 88.7 Female ## 3 3 27 80 70.2 Male ## 4 4 53 80 61.3 Male ## 5 5 4 40 89.5 Male ## 6 6 22 70 60.5 Female Using the Shapiro-Wilk test we note that our data are not approximately normal: # note these data are not approximately normal: # all p&lt;0.05, therefore reject null that they are approximately normal shapiro.test(exams$revise) ## ## Shapiro-Wilk normality test ## ## data: exams$revise ## W = 0.81608, p-value = 6.088e-10 shapiro.test(exams$exam) ## ## Shapiro-Wilk normality test ## ## data: exams$exam ## W = 0.95595, p-value = 0.001841 shapiro.test(exams$anxiety) ## ## Shapiro-Wilk normality test ## ## data: exams$anxiety ## W = 0.85646, p-value = 1.624e-08 We can investigate the individual correlations by plotting the scatterplots and calculating the Spearman correlations: cor(exams$revise, exams$exam, method = &quot;spearman&quot;) ## [1] 0.3330192 cor(exams$revise, exams$anxiety, method = &quot;spearman&quot;) ## [1] -0.6107712 cor(exams$anxiety, exams$exam, method= &quot;spearman&quot;) ## [1] -0.3887703 If we were interested in the correlation between revision time and exam performance controlling for anxiety, we would need to do a partial correlation. We can just use the ppcor.test() function: pcor.test(x=exams$revise, y=exams$exam, z=exams$anxiety, method = &quot;spearman&quot;) ## estimate p.value statistic n gp Method ## 1 0.1310034 0.1916154 1.314798 102 1 spearman Again, we see that the relationship between these two variables is reduced considerably to \\(\\rho=0.13\\) when we control for each of their relationships with anxiety. 12.8 Point-Biserial Correlation Throughout the entirety of this chapter weve been discussing various correlation measures between two continuous variables. This little subsection is here just to point out that you can in fact also measure the correlation between a categorical variable and a continuous variable. In the following dataset, we have data on the number of hours that various cats spend away from their house over a one week period. They are tracked via cool GPS collars, and these data make pretty pictures of how far they traveled from their homes. cats &lt;- read_csv(&quot;data/cats.csv&quot;) nrow(cats) ## [1] 60 head(cats) ## # A tibble: 6 x 3 ## time sex sex1 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 41 male 1 ## 2 40 female 0 ## 3 40 male 1 ## 4 38 male 1 ## 5 34 male 1 ## 6 46 female 0 The data have a time column that is the number of hours away from home. A sex column which is the sex of the cat, and a sex1 column which is the sex of the cat coded as a number. This final column exists to help us make the dotplot below and to help run a correlation: # dot plot ggplot(cats, aes(sex1,time)) + geom_jitter(width = .05) + scale_x_continuous(breaks=c(0,1), labels=c(&quot;female&quot;, &quot;male&quot;))+ stat_smooth(method=&#39;lm&#39;,se=F) + theme_classic() What we have in this plot are individual points representing each cat. The points are jittered to make overlapping datapoints more readable. We have also put a trendline through the data. How was this achieved? Essentially, all the females are given the value 0, and all the males are given the value 1. We then run a Pearsons correlation using the X values to be 0 or 1, and the Y values to be the time measurement. Doing this, we can simply run a Pearsons correlation test using cor.test(): cor.test(cats$sex1, cats$time) ## ## Pearson&#39;s product-moment correlation ## ## data: cats$sex1 and cats$time ## t = 3.1138, df = 58, p-value = 0.002868 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.137769 0.576936 ## sample estimates: ## cor ## 0.3784542 These results suggests that there is a positive relationship between cat sex and time spent away from home, with male cats spending more time away than female cats. Looking at the dotplot again, it looks like the female data are possibly bimodal. As the data are in long format, we can filter out the male and female data: catsF_time &lt;- cats %&gt;% filter(sex==&quot;female&quot;) %&gt;% .$time catsM_time &lt;- cats %&gt;% filter(sex==&quot;male&quot;) %&gt;% .$time shapiro.test(catsF_time) ## ## Shapiro-Wilk normality test ## ## data: catsF_time ## W = 0.81663, p-value = 8.505e-05 shapiro.test(catsM_time) ## ## Shapiro-Wilk normality test ## ## data: catsM_time ## W = 0.97521, p-value = 0.7246 Clearly our suspicions about the female data are true, they do not appear to come from an approximately normal distribution. In that case, we could run a point-biserial Spearman correlation. This is just the same procedure, but instead we apply the Spearman test: cor.test(cats$time, cats$sex1, method=&quot;spearman&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: cats$time and cats$sex1 ## S = 23048, p-value = 0.004774 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.3596003 Just as a note - there are probably better ways of running such a point-biserial correlation than using cor.test(), but what weve shown here is probably sufficient to get the idea across that you can convert two groups to 0s and 1s and then run a correlation to test for group differences. "],["linear-regression.html", "Chapter 13 Linear Regression 13.1 Introduction to Linear Regression 13.2 a and b 13.3 Residuals 13.4 Standard Error of the Estimate 13.5 Goodness of Fit Test - F-ratio 13.6 Assumptions of Linear Regression 13.7 Examining individual predictor estimates", " Chapter 13 Linear Regression With linear regression, we are attempting to further our understanding of the relationship between two continuous variables. In particular, we try to predict the values of the outcome variable based on the values of the predictor variable. In simple linear regression we only include one predictor variable. That is the type of regression that we will discuss in this chapter. 13.1 Introduction to Linear Regression The regression method that we are going to start with is called Ordinary Least Squares Regression. Hopefully the reason why its called least squares will be come obvious, although were not too sure why its called ordinary. Lets illustrate the question at hand with some data: library(tidyverse) # Import Data df &lt;- read_csv(&quot;data/parenthood.csv&quot;) nrow(df) ## [1] 100 head(df) ## # A tibble: 6 x 4 ## dan.sleep baby.sleep dan.grump day ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7.59 10.2 56 1 ## 2 7.91 11.7 60 2 ## 3 5.14 7.92 82 3 ## 4 7.71 9.61 55 4 ## 5 6.68 9.75 67 5 ## 6 5.99 5.04 72 6 As you can see, what we have are four columns of data. The first column shows the amount of sleep that Dan got in an evening. The second column relates to the amount of sleep a baby got. The third column is a rating of Dans grumpiness. The last column is a day identifier. Each row represents a different day. Say were interested in seeing whether there was an association between Dans sleep and her grumpiness. We could examine this using a scatterplot: # Scatterplot ggplot(df, aes(x = dan.sleep, y = dan.grump)) + geom_point() + stat_smooth(method=&#39;lm&#39;, se=F) It looks like these variables are clearly associated, with higher levels of sleep being related to lower levels of grumpiness. To get a measure of how big this relationship is, we could run a correlation test. cor.test(df$dan.sleep, df$dan.grump) ## ## Pearson&#39;s product-moment correlation ## ## data: df$dan.sleep and df$dan.grump ## t = -20.854, df = 98, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.9340614 -0.8594714 ## sample estimates: ## cor ## -0.903384 This shows that the variables are highly correlated with r=-0.90. Its also a highly significant relationship. Using stat_smooth() we also applied a best fitting trendline through the data. This line was actually calculated to be in the position that it is in using regression. The line also has the equation: \\(y&#39; = a + xb\\) You may also see this written in other ways such as: \\(y&#39; = \\beta_{0} + x\\beta_{1}\\) but well stick with \\(y&#39; = a + xb\\) 13.2 a and b In the equation for a regression line, \\(y&#39; = a + xb\\) \\(y&#39;\\) is equal to the predicted value of \\(y\\). Essentially if you go from any value of \\(x\\) and go up to the trendline, and then across to the y-axis, that is your predicted value of y. The trendline represents the predicted values of \\(y\\) for all values of \\(x\\). In regression terms, we often refer to \\(x\\) as the predictor variable and \\(y\\) as the outcome variable. The value of \\(b\\) in the equation represents the slope of the regression line. If its a positive number then it means that the regression line is going upwards (akin to a positive correlation, where \\(y\\) increases as \\(x\\) increases.) If its a negative number, then it means that the regression line is going downwards (akin to a negative correlation, where \\(y\\) decreases as \\(x\\) increases). The value of \\(b\\) can also be considered to be how much \\(y\\) changes for every 1 unit increase in \\(x\\). So a value of \\(b = -1.4\\) would indicate that as \\(x\\) increases by 1, \\(y\\) will decrease by 1.4. The value of \\(a\\) represents the y-intercept. This is the value of y' that you would get if you extended the regression line to cross at \\(x=0\\). We can illustrate that in the graph below. We extended the trendline (dotted black line) from its ends until it passes through where \\(x=0\\) (the dotted red line). Where it crosses this point is at \\(y=125.96\\) which is depicted by the orange dotted line. This makes the y-intercept for this trendline equal to 125.96. # Scatterplot ggplot(df, aes(x = dan.sleep, y = dan.grump)) + geom_point() + stat_smooth(method=&#39;lm&#39;, se=F) + xlim(-1,10)+ ylim(40,130)+ geom_abline(intercept =125.956 , slope = -8.937, lty=2)+ geom_vline(xintercept=0, color=&#39;red&#39;, lty=2) + geom_segment(x=-1.6,xend=0,y=125.96,yend=125.96, lty=2, color=&#39;orange&#39;) In reality, we do not ever extend the regression lines like this. In fact, a regression line by definition only fits the range of datapoints along the x-axis that we have - it should not extend beyond those points. This is more a theoretical construct to help us understand where the line is on the graph. To think a bit more about a and b lets compare these regression lines and their respective equations. In the top left image, you can see three parallel regression lines. These all have the same slope value (b = 0.704). How they differ is in their y-intercept. The different values of a indicate that they are at different heights. In the top right, weve extended the regression lines in red back to where the x-axis is 0. Where the lines cross here (x=0) is the value of each y-intercept i.e. a. In the bottom right image, you can see that all the plots have the same y-intercept value (8.32). How they differ is in their slope b. Two are positive values of b, with the larger value having a steeper slope. The negative value of b (-0.622) has a slope going downwards. In the bottom right we extend these trendlines back to where x=0, and you can see that all have the same y-intercept (x=0, y=8.32). 13.2.1 How to calculate a and b in R To run a regression in R, we use the lm() function. It looks like the following: mod1 &lt;- lm(dan.grump ~ dan.sleep, data=df) # build regression model The first thing after the bracket is the outcome variable which is dan.grump. Then a tilde (~) and then the predictor variable which is dan.sleep. Finally, we tell lm what dataset were using. The best way to read that statement is: \"dan.grump 'is predicted by' dan.sleep\". Were also saving the regression model as mod1 because theres tons of information that comes along with the regression. To have a look at what a and b are, we can just look at our saved object mod1. mod1 ## ## Call: ## lm(formula = dan.grump ~ dan.sleep, data = df) ## ## Coefficients: ## (Intercept) dan.sleep ## 125.956 -8.937 Here the value underneath Intercept (125.956) refers to the y-intercept a. The value underneath dan.sleep, the predictor variable, is our value of b the slope. So this would mean that the regression line for these data would be: \\(y&#39; = 125.956 + -8.937b\\) We can also get these values directly by running the following code: mod1$coefficients ## (Intercept) dan.sleep ## 125.956292 -8.936756 Now, we should also mention one other thing about these values. These regression coefficients are estimates. We have one sample of 100 subjects from which we estimated the true population values of a and b. The true population values of a and b are parameters. 13.2.2 How to calculate a and b by hand To think a bit more about these a and b values, we could look at how these values are actually calculated. First, we calculate b. The formula for this requires knowing the sample standard deviation of the X and Y variables, as well as their Pearson correlation. \\(\\Large b = r\\frac{s_{Y}}{s_{X}}\\) So for our example, wed calculate b like this: r &lt;- cor(df$dan.sleep, df$dan.grump) sy &lt;- sd(df$dan.grump) sx &lt;- sd(df$dan.sleep) b &lt;- r * (sy/sx) b ## [1] -8.936756 Next, we can calculate a using the formula: \\(a = \\overline{Y} - b\\overline{X}\\) . This requires us to know the sample mean of X and Y. So for our example, we calculate a like this: my &lt;- mean(df$dan.grump) mx &lt;- mean(df$dan.sleep) a &lt;- my - (b * mx) a ## [1] 125.9563 Therefore, we have an equation for our trendline which is y' = 125.96 + -8.94x, which means that for every 1 unit increase of sleep, Dans grumpiness decreases by 8.94. 13.3 Residuals Once we fit a trendline to the data its clear that not all the datapoints fit to the line. In fact, almost none of the datapoints are on the line! This is because the trendline is our prediction of the value of y based on the value of x. Each datapoint actually is either larger or smaller in terms of y than the regression line. Sometimes its a bit bigger or smaller, other times it might be a lot bigger or smaller. Occasionally, the predicted value of y might be on the regression line. Because our trendline isnt a perfect fit for the data, the formula for the regression line could technically be written as: \\(y&#39; = a + bx + \\epsilon\\) \\(\\epsilon\\) refers to the error, or how far each datapoint is from the predicted value. In fact, the difference of each data point from the predicted value is called a raw residual or ordinary residual. We calculate the size of the residual for each datapoint by the following formula: \\(residual = y - y&#39;\\) This essentially is the difference of each data point from the predicted value. 13.3.1 How to calculate the residuals Using our regression line equation of y' = 125.96 + -8.94x, we can manually calculate the raw residuals. Firstly, we calculate the predicted values of y \\(y&#39;\\) for each value of \\(x\\). We can put these back into our original dataframe. X &lt;- df$dan.sleep # the predictor Y &lt;- df$dan.grump # the outcome Y.pred &lt;- 125.96 + (-8.94 * X) Y.pred ## [1] 58.1054 55.2446 80.0084 57.0326 66.2408 72.4094 52.7414 61.6814 59.8040 67.1348 67.9394 69.9062 ## [13] 72.7670 66.5090 68.6546 69.3698 69.6380 50.2382 61.5026 58.6418 54.4400 60.2510 64.6316 55.6916 ## [25] 82.5116 73.4822 50.8640 64.0058 61.5026 63.4694 52.9202 55.7810 69.9062 48.5396 81.4388 70.6214 ## [37] 68.6546 82.6904 63.1118 57.4796 58.8206 55.1552 53.3672 59.1782 54.5294 77.3264 53.0096 57.8372 ## [49] 73.4822 45.5000 51.6686 65.9726 59.5358 73.2140 49.7912 72.0518 60.7874 60.5192 64.4528 70.3532 ## [61] 63.9164 63.2906 61.5920 69.6380 48.0032 56.0492 53.1884 60.9662 66.0620 58.4630 59.9828 56.8538 ## [73] 78.3992 55.6916 69.1910 62.3966 77.2370 56.2280 62.2178 51.3110 64.0058 62.7542 48.5396 80.4554 ## [85] 82.0646 63.1118 63.2012 57.3902 53.0990 73.3928 74.8232 66.4196 64.7210 76.1642 79.8296 78.4886 ## [97] 56.4962 77.8628 63.2012 68.2970 df$Y.pred &lt;- Y.pred head(df) ## # A tibble: 6 x 5 ## dan.sleep baby.sleep dan.grump day Y.pred ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7.59 10.2 56 1 58.1 ## 2 7.91 11.7 60 2 55.2 ## 3 5.14 7.92 82 3 80.0 ## 4 7.71 9.61 55 4 57.0 ## 5 6.68 9.75 67 5 66.2 ## 6 5.99 5.04 72 6 72.4 Next, getting the raw residual is simply a matter of taking each observed value of \\(y\\) and subtracting the predicted value of \\(y\\). # so to get the residual, y - y&#39; df$dan.grump - Y.pred ## [1] -2.1054 4.7554 1.9916 -2.0326 0.7592 -0.4094 0.2586 -1.6814 0.1960 3.8652 4.0606 ## [12] -4.9062 1.2330 0.4910 -2.6546 -0.3698 3.3620 1.7618 -0.5026 -5.6418 -0.4400 2.7490 ## [23] 9.3684 0.3084 -0.5116 -1.4822 8.1360 1.9942 -1.5026 3.5306 -8.9202 -2.7810 6.0938 ## [34] -7.5396 4.5612 -10.6214 -5.6546 6.3096 -2.1118 -0.4796 0.1794 4.8448 -5.3672 -6.1782 ## [45] -4.5294 -5.3264 3.9904 2.1628 -3.4822 0.5000 6.3314 2.0274 -1.5358 -2.2140 2.2088 ## [56] 1.9482 -1.7874 -1.5192 2.5472 -3.3532 -2.9164 0.7094 -0.5920 -8.6380 5.9968 5.9508 ## [67] -1.1884 3.0338 -1.0620 6.5370 -2.9828 2.1462 0.6008 -2.6916 -2.1910 -1.3966 4.7630 ## [78] 11.7720 4.7822 2.6890 -11.0058 -0.7542 1.4604 -0.4554 8.9354 -1.1118 0.7988 -0.3902 ## [89] 0.9010 -1.3928 3.1768 -3.4196 -5.7210 -2.1642 -3.8296 0.5114 -5.4962 4.1372 -8.2012 ## [100] 5.7030 df$residuals &lt;- df$dan.grump - Y.pred head(df) ## # A tibble: 6 x 6 ## dan.sleep baby.sleep dan.grump day Y.pred residuals ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7.59 10.2 56 1 58.1 -2.11 ## 2 7.91 11.7 60 2 55.2 4.76 ## 3 5.14 7.92 82 3 80.0 1.99 ## 4 7.71 9.61 55 4 57.0 -2.03 ## 5 6.68 9.75 67 5 66.2 0.759 ## 6 5.99 5.04 72 6 72.4 -0.409 13.3.2 Visualizing the Residuals Now we have a raw residual for all 100 datapoints in our data. The following plot is our same scatterplot, but this time weve also added a little red line connecting each observed datapoint to the regression line. The size of each of these red lines represents the residuals. p1 &lt;- ggplot(df, aes(x = dan.sleep, y = dan.grump)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_segment(aes(xend = dan.sleep, yend = dan.grump-residuals), alpha = .5, color=&#39;red&#39;) + theme_classic() + ggtitle(&quot;OLSR best fit trendline&quot;) p1 ## `geom_smooth()` using formula &#39;y ~ x&#39; 13.3.3 Comparing our trendline to other trendlines An important question to consider is why did we end up with our trendline and not some other trendline? The answer is that ours is the best fit, but what does that really mean? In short, it means that the best-fit regression line is the one that has the smallest squared residuals. The squared residuals are calculated by squaring every residual and then summing these all up. Lets look at this by looking at one possible trendline that we could have used. The one that well choose is a trendline that goes horizontally through the data with a y value that is the mean of Y. mean(df$dan.grump) ## [1] 63.71 So, the mean of the Y variable dan.grump is 63.71. Lets put a trendline through our data that is horizontal at 63.71. The equation for this line would be: \\(y&#39; = 63.71 + 0x\\) Lets visualize this: ggplot(df, aes(x = dan.sleep, y = dan.grump)) + geom_point() + geom_hline(yintercept = mean(df$dan.grump), color=&#39;blue&#39;) It doesnt look like using the mean value of Y is that good a predictor of each datapoint. We could actually visualize how good.bad it is by calculating the residual of each datapoint from this new trendline. This time each residual is equal to: \\(residual = y - \\overline{y}\\) Lets calculate these residuals, and then graph them on this scatterplot: # we can work out what the &#39;residuals&#39; would be for this trendline: df$Ymean &lt;- mean(df$dan.grump) df$resid_Ymean &lt;- df$dan.grump - df$Ymean #residual from Ymean head(df) ## # A tibble: 6 x 8 ## dan.sleep baby.sleep dan.grump day Y.pred residuals Ymean resid_Ymean ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7.59 10.2 56 1 58.1 -2.11 63.7 -7.71 ## 2 7.91 11.7 60 2 55.2 4.76 63.7 -3.71 ## 3 5.14 7.92 82 3 80.0 1.99 63.7 18.3 ## 4 7.71 9.61 55 4 57.0 -2.03 63.7 -8.71 ## 5 6.68 9.75 67 5 66.2 0.759 63.7 3.29 ## 6 5.99 5.04 72 6 72.4 -0.409 63.7 8.29 ## visualize this: p2 &lt;- ggplot(df, aes(x = dan.sleep, y = dan.grump)) + geom_point() + geom_hline(yintercept = mean(df$dan.grump), color=&#39;blue&#39;) + geom_segment(aes(xend = dan.sleep, yend = dan.grump-resid_Ymean), alpha = .5, color=&#39;red&#39;) + # geom_point(aes(y = dan.grump+residuals), shape = 1) + theme_classic() + ggtitle(&quot;Residuals to the mean Y&quot;) p2 If we compare both graphs scatterplots side by side, its pretty clear that our best-fit trendline is doing a much better job of predicting each datapoints Y value. The horizontal trendline at the mean of Y looks pretty bad for the majority of datapoints - its residuals are much bigger. library(gridExtra) grid.arrange(p1,p2,nrow=1) In fact, lets actually quantify the difference in residuals between these two trendlines. Because we have both positive and negative residuals, if we just added them together wed end up with 0. In statistics, one common way to make numbers positive is to square them. As we saw with standard deviation, this also has the advantage of emphasizing large values. What we do to compare our residuals, is to therefore square them. We call the residuals from our trendline the raw residuals. We call the residuals from the horizontal line the total residuals. df$residuals2 &lt;- df$residuals ^ 2 # raw residuals df$resid_Ymean2 &lt;- df$resid_Ymean ^ 2 # total residuals head(df[c(1,3,6,8,9,10)]) # just showing the relevant columns ## # A tibble: 6 x 6 ## dan.sleep dan.grump residuals resid_Ymean residuals2 resid_Ymean2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7.59 56 -2.11 -7.71 4.43 59.4 ## 2 7.91 60 4.76 -3.71 22.6 13.8 ## 3 5.14 82 1.99 18.3 3.97 335. ## 4 7.71 55 -2.03 -8.71 4.13 75.9 ## 5 6.68 67 0.759 3.29 0.576 10.8 ## 6 5.99 72 -0.409 8.29 0.168 68.7 You can see the squared raw residual and squared total residual for the first six datapoints. Ideally, we want the raw residuals to be as small a fraction as possible of the total residuals. If we sum up the squared raw residuals and squared total residuals, we can determine this: SS.resid &lt;- sum(df$residuals2) SS.resid #1838.722 ## [1] 1838.75 SS.tot &lt;- sum(df$resid_Ymean2) SS.tot #9998.59 ## [1] 9998.59 As you can clearly see, the summed squared residuals for our best fit regression line are much smaller than the summed squared residuals when using the mean of y as the regression line. 13.3.4 Coefficient of Determination R2 One way to make these summed squares of residuals numbers more interpretable is to convert them to \\(R^{2}\\). The logic goes as following. If the trendline is absolutely useless at predicting the y values, then the trendline would have residuals as high as the total residuals. If the trendline is perfect at predicting the y values, then the residual SS total would be 0. If we look at the sum of the squares of the raw residuals as a ratio of a sum of the squared total residuals then we can work out how well our trendline fits. We calculate this using the formula, \\(R^{2} = 1 - \\frac{SS_{raw}}{SS_{total}}\\) in the following way: 1 - (SS.resid/SS.tot) # 0.816 (this is R2) ## [1] 0.816099 So for our data, \\(R^{2} = 0.816\\). This means that our regression model (and trendline) is a very good fit to the data. \\(R^{2}\\) ranges from 0 to 1. If its value was 1, then that would indicate that the best-fit trendline perfectly fits the data with no raw residuals. If its value was 0, then that would mean that the best-fit trendline is no better at fitting the data than the horizontal line at the mean of Y. Values of \\(R^{2}\\) that get closer to 1 indicate that the model is doing a better job at estimating each value of Y. We say that the model has a better fit to the data. There is a way to directly get the value of \\(R^{2}\\) in R. You may remember earlier in this chapter that we ran our linear model using the function lm(). We saved the output of this regression model as the object mod1. You can use summary() to get lots of information about the regression model. summary(mod1) # the R2 matches ## ## Call: ## lm(formula = dan.grump ~ dan.sleep, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.025 -2.213 -0.399 2.681 11.750 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 125.9563 3.0161 41.76 &lt;2e-16 *** ## dan.sleep -8.9368 0.4285 -20.85 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.332 on 98 degrees of freedom ## Multiple R-squared: 0.8161, Adjusted R-squared: 0.8142 ## F-statistic: 434.9 on 1 and 98 DF, p-value: &lt; 2.2e-16 The thing to focus on from this output right now, is the value called Multiple R-squared. This gives us the same value of \\(R^{2}\\) that we calculated by hand: \\(R^{2}=0.816\\). The other value of \\(R^{2}\\) is called Adjusted R-squared. This is relevant when you are conducting multiple regression with more than one predictor in the model. We only have one predictor in the model (dan.sleep) so we wont talk more about Adjusted R squared here. Finally, there is a shortcut way in which we can calculate \\(R^{2}\\). It is simply to square the Pearsons correlation coefficient: \\(R^{2} = r^{2}\\). r &lt;- cor(df$dan.sleep, df$dan.grump) r ## [1] -0.903384 r^2 # same R2 as above ## [1] 0.8161027 13.4 Standard Error of the Estimate \\(R^{2}\\) is one value that gives us a sense of how well our regression model is doing. Another method to assess model fit is to examine the Standard Error of the Estimate. Unfortunately, this value has a number of names. Youll see it referred to as the Standard Error of the Regression, Residual Standard Error, Regression Standard Error, \\(S\\), or \\(\\sigma_{est}\\). We prefer to call it the Standard Error of the Estimate or \\(\\sigma_{est}\\). This is calculated as follows: \\(\\sigma_{est} = \\sqrt{\\frac{\\Sigma (y - y&#39;)^{2}}{n-2}}\\) The \\(\\Sigma (y - y&#39;)^{2}\\) part of this equation is the Sum of the raw residuals squared that we already calculated when calculating \\(R^{2}\\). We can therefore calculate \\(\\sigma_{est}\\) quite straightforwardly in R manually. s_est &lt;- sqrt(SS.resid / 98) # n=100 s_est ## [1] 4.3316 Therefore \\(\\sigma_{est} = 4.332\\) for our model. Its actually possible to see this value in the output of the summary of the model in R. Here, its called the residual standard error: summary(mod1) ## ## Call: ## lm(formula = dan.grump ~ dan.sleep, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.025 -2.213 -0.399 2.681 11.750 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 125.9563 3.0161 41.76 &lt;2e-16 *** ## dan.sleep -8.9368 0.4285 -20.85 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.332 on 98 degrees of freedom ## Multiple R-squared: 0.8161, Adjusted R-squared: 0.8142 ## F-statistic: 434.9 on 1 and 98 DF, p-value: &lt; 2.2e-16 13.4.1 What to do with the Standard Error of the Estimate ? What we are generally looking for with \\(\\sigma_{est}\\) is a number as small as possible. This is because what essentially it is a measure of is an approximate estimate of the average raw residual. The question is, how small is small? This is difficult to answer. One reason is because \\(\\sigma_{est}\\) is actually in the original units of the Y-axis (the outcome variable). Therefore its not as simple as saying that it should be close to 0, because the Y-axis may be in units that are very large. In other ways, having this value in the original units of Y can be quite helpful as it is easy to envisage what the size of the average residual is. However, a good rule of thumb is that approximately 95% of the observations (raw datapoints) should fall within plus or minus 2 times the standard error of the estimates from the regression line. We can illustrate this with the following plot: ggplot(df, aes(x = dan.sleep, y = dan.grump)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() + geom_abline(intercept = 125.956+s_est+s_est, slope = -8.937, color = &#39;red&#39;, lty=2)+ geom_abline(intercept = 125.956-s_est-s_est, slope = -8.937, color = &#39;red&#39;, lty=2) + ggtitle(&quot;Regression with 2 x Standard Error of the Estimate&quot;) This is our original scatterplot again. The blue line is still the best fitting trendline. The two dashed red lines are two times \\(\\sigma_{est}\\) above and below the blue line respectively. That is they are 2 * 4.332 = 8.644 units of grumpiness above or below the trendline. If we count up the number of datapoints that our outside of the dotted red lines, we can see that there are six datapoints out of our 100 datapoints that are just outside, although some of these are very close indeed to the red line. This is probably ok as were only expecting 95% of datapoints on average to be inside the red lines. 13.5 Goodness of Fit Test - F-ratio \\(R^{2}\\) and \\(\\sigma_{est}\\) are two calculations that help us determine if our regression model (and trendline) is indeed a good fit to the data. A more formal method is to run a statistical test called the Goodness of Fit Test. To do this we calculate an F-ratio that essentially examines how well our trendline (and model) fit the data compared to the null model which is the model where we use the mean of Y as our prediction. As a reminder, these scatterplots show the difference in performance of our fitted model (left) compared to the null model (right). grid.arrange(p1,p2,nrow=1) ## `geom_smooth()` using formula &#39;y ~ x&#39; The F-ratio is essentially a method of determining the proportion of residual variance compared to total variance. We can calculate it using the following formula: \\(F = \\frac{SSM/d.f.SSM}{SSR/d.f.SSR}\\) Here, SSM refers to the sum of squares for the model (the model sum of squares). This is equal to: \\(SSM = SST - SSR\\) That is, its the difference between the total sum of squares (the sum of the squared residuals for the null model) minus the residual sum of squares (the sum of the squared residuals for the fitted model). The d.f.SSM refers to the degrees of freedom for the model sum of squares, which is equal to the number of predictors in the model. We only have one predictor (dan.sleep), so that means the degrees of freedom are 1. The d.f.SSR refers to the degrees of freedom for the raw residuals. This is equal to the number of observations minus the number of predictors minus 1. Therefore it is equal to 100 - 1 - 1 = 98 as we had 100 datapoints (or observations). Lets calculate this in R. Firstly, well calculate the model sum of squares: SS.mod &lt;- SS.tot - SS.resid SS.mod #8159.868 ## [1] 8159.84 Next we divide the model sum of squares and the residual sum of squares by their respective degrees of freedom to get the mean sum of squares for each. \\(F\\) is then calculated by dividing the former by the latter: MS.resid &lt;- SS.resid / 98 MS.resid #18.76 ## [1] 18.76276 MS.mod &lt;- SS.mod / 1 MS.mod #8159.868 ## [1] 8159.84 Fval &lt;- MS.mod / MS.resid Fval #434.9 ## [1] 434.8955 So \\(F = 434.9\\) which is a large value. Larger values indicate that we were less likely to get a difference between the sum of squares for our fitted and null models by chance alone. Essentially, the observed value of F is compared to the sampling distribution for F if the null hypothesis is true. The null is that our model (trendline) is no better than random in fitting the datapoints Whether our \\(F\\) value is sufficiently large can be looked up in an F-table (not recommended), or you can simply let R do the work for you. If you use summary() on your saved regression model, it will give you the \\(F\\) value as well as a p-value to go along with it. summary(mod1) ## ## Call: ## lm(formula = dan.grump ~ dan.sleep, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.025 -2.213 -0.399 2.681 11.750 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 125.9563 3.0161 41.76 &lt;2e-16 *** ## dan.sleep -8.9368 0.4285 -20.85 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.332 on 98 degrees of freedom ## Multiple R-squared: 0.8161, Adjusted R-squared: 0.8142 ## F-statistic: 434.9 on 1 and 98 DF, p-value: &lt; 2.2e-16 You can see from the model output that the F-statistic is given on the bottom row along with the degrees of freedom for the model (1) and the residuals (98). The p-value here is 0.0000000000000022 which is basically 0. Typically, if the p-value is less than 0.05 we will say that our fitted model is a better fit to the data than the null model. 13.6 Assumptions of Linear Regression Even if your model is a good fit to the data, there are still several things you need to check before progressing to examining whether your predictor is significantly (a better term is probably meaningfully) predicting the outcome variable. The last series of things we need to do is to check whether we have violated the assumptions of linear regression. In short, here are some of the assumptions that we need to adhere to: Normality - Specifically the residuals are normally distributed. Linearity - We need to be examining linear relationships between predictors and outcomes Homogeneity of Variance (homoscedasticity) Uncorrelated Predictors (only relevant if doing more than one predictor) No overly influential datapoints Lets discuss each of these in turn. 13.6.1 Normality of Residuals One assumption of linear regression is that our residuals are approximately normally distributed. We can check this in several ways. But first, we should probably own up and mention that there are several types of residuals. The residuals we have been dealing with from the regression model have been the raw residuals. We got these by simply subtracting the predicted value of y from the observed value of y \\(residual = y - y&#39;\\). However, statisticians like to modify these residuals. One modification they make is to turn these residuals into what are called standardized residuals. These are the raw residuals divided by the standard deviation of the residuals. You can directly access the standardized residuals in R by using the rstandard() function with your model output: df$std_resids &lt;- rstandard(mod1) head(df[c(1,3,6,11)]) # just showing the relevant columns ## # A tibble: 6 x 4 ## dan.sleep dan.grump residuals std_resids ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7.59 56 -2.11 -0.494 ## 2 7.91 60 4.76 1.10 ## 3 5.14 82 1.99 0.467 ## 4 7.71 55 -2.03 -0.478 ## 5 6.68 67 0.759 0.172 ## 6 5.99 72 -0.409 -0.0991 We can demonstrate that the standardized residuals really are highly correlated with the raw residuals by plotting them on a scatterplot: ggplot(df, aes(x=residuals, y=std_resids)) + geom_point() + theme_classic()+ xlab(&quot;Raw Residuals&quot;)+ ylab(&quot;Standardized Residuals&quot;) They are effectively the same - just transformed. One advantage of standardized residuals is that they can help us look for unusual datapoints with large residuals. While raw residuals are always in the original units of the y-axis, standardized residuals are, well, standardized. Standardized residuals that are greater than 2 or less than -2 are quite large, those that are larger than 3 or less than 3 are very large indeed and should be looked at in more detail. Lets get back to checking the normality of these residuals. First up, we could plot a histogram and see if we think its approximately normal: #a) histogram plot ggplot(df, aes(x=std_resids)) + geom_histogram(color=&#39;white&#39;) # possibly ok This looks possibly ok. Its not too skewed, but it can be hard from a histogram with just 100 datapoints to get a sense of the shape. A second approach would be to use a Shapiro-Wilk test which more formally test whether the data are approximately normally distributed: #b) Shapiro-Wilk test shapiro.test(df$std_resids) # shapiro test says normal. ## ## Shapiro-Wilk normality test ## ## data: df$std_resids ## W = 0.99201, p-value = 0.8221 Given that the p-value here is above our cut-off of 0.05, this suggests that we have no evidence to reject the hypothesis that our data came from a normal distribution. An easier way of saying this is, our residuals are likely approximately normally distributed. The other method we can employ to check normality is to use a QQ plot: #c) QQ plot qqnorm(df$std_resids) qqline(df$std_resids, col = &quot;steelblue&quot;, lwd = 2) # it&#39;s ok A discussion of precisely what these QQ plots are is beyond the scope here. However, in general terms, what we are plotting is the residual against the theoretical value of each residual that we would expect if our data were normally distributed. In practical terms, what were looking for is that the bulk of our data fit along the blue line. Its ok to have a bit of wobble at the extremes - that just means that our data distribution probably has slightly fat tails. Its also possible to generate a version of the above plot quickly, directly from the saved linear model object. plot( x = mod1, which = 2 ) # fast way of getting same plot 13.6.2 2. Linearity  The second major assumption of linear regression is that the relationship between our predictor and outcome is linear! So, data that look like the following would not have a linear relationship. comparing a and b One simple approach is to examine the scatterplot of your predictor (X) and outcome (Y) variables. We already did this, and our data looked pretty linear! Another approach is to examine the relationship between your observed Y values and the predicted Y values \\(y&#39;\\). This should also be a linear relationship. Although we calculated the \\(y&#39;\\) values earlier using the formula for the regression line, we can actually grab them directly from the model object with the fitted.values() command. df$Y.fitted &lt;- fitted.values(mod1) # plot this relationship ggplot(df, aes(x = Y.fitted, y = dan.grump)) + geom_point() + stat_smooth(method=&#39;lm&#39;, se=F) As you can see from this plot, our fitted (predicted) values of Y on the x-axis have a strong linear relationship with our observed values of Y on the y-axis. 13.6.3 3. Homogeneity of Variance / Homoscedasticity The third assumption that we need to check is homoscedasticity (also sometimes referred to as homogeneity of variance). What this really means is that the model should be equally good at predicting Ys across all values of X. Our regression model shouldnt be better at predicting values of Y for e.g. small values of X but not for large values of X. Practically, what this means, is that the size of the residuals should be equal across all values of X. If we plot the values of X (or the predicted/fitted values of Y) on the x-axis against the residuals (in this case standardized residuals) on the Y-axis, then there should be no overall pattern. We should be equally likely to get small or large residuals for any value of X (or predicted/fitted value of Y). This would mean that any trendline on this graph should be a horizontal line. # if true, this should be a straight line ggplot(df, aes(x = Y.fitted, y = std_resids)) + geom_point() + stat_smooth(method=&#39;lm&#39;, se=F) As you can see, our plot is basically a random scatterplot and there is no overall pattern. That is good, it means we have homoscedasticity. If we did not have homoscedasticity, then wed see a pattern in this scatterplot - such as the residuals getting larger or smaller for larger fitted (predicted) values of Y. You can access a version of this plot directly from the model object like this: plot(mod1, which = 3) There are some more formal tests of homoscedasticity but we dont need to worry about them. 13.6.4 No Colinearity This assumption of linear regression only applies when we have more than one predictor in the model. In our model, we do only have one predictor (dan.sleep) so we dont need to worry about it. If we had added in another variable into the model, e.g. the amount of hours of baby sleep, then wed have a second predictor. That means, were trying to predict dan.grump based on both dan.sleep and baby.sleep. In this case, dan.sleep and baby.sleep should not be correlated with each other. 13.6.5 Unusual Datapoints There are a number of ways that datapoints could be unusual. We will discuss data points that are: outliers have high leverage have high influence Generally linear regression models should not be overly affected by individual data points. Usually the category that we most need to be concerned about are points with high influence. 13.6.5.1 i. Outliers Outliers are datapoints that are typically highly unusual in terms of outcome Y but not in terms of the predictor X. These will be datapoints that have high residuals. Lets look at an example dataset. We have a predictor x and an outcome y. With y1 we have the same outcome variables, but have removed the value for one datapoint: x &lt;- c(4.1,4.2,5,5.5,6,6.1,6.15,6.4,7,7.2,7.7,8,9.7) y &lt;- c(3.9,4.3,6,5.9,6.1,11.9,6.3,5.8,7.9,6.4,7.8,8,9.1) y1 &lt;- c(3.9,4.3,6,5.9,6.1,NA,6.3,5.8,7.9,6.4,7.8,8,9.1) ddf &lt;- data.frame(x,y,y1) ddf ## x y y1 ## 1 4.10 3.9 3.9 ## 2 4.20 4.3 4.3 ## 3 5.00 6.0 6.0 ## 4 5.50 5.9 5.9 ## 5 6.00 6.1 6.1 ## 6 6.10 11.9 NA ## 7 6.15 6.3 6.3 ## 8 6.40 5.8 5.8 ## 9 7.00 7.9 7.9 ## 10 7.20 6.4 6.4 ## 11 7.70 7.8 7.8 ## 12 8.00 8.0 8.0 ## 13 9.70 9.1 9.1 And here, were plotting the best fitting regression line in blue. The dotted red line represents the best fitting trendline that we would have if we removed the datapoints that has the y value of 11.9. The dashed black line shows the distance from this datapoint to the trendline we would have if it was removed. ## (Intercept) x ## 0.8247787 0.8785270 As you can see from this small example, outliers are datapoints that have very large residuals from the trendline. They have an unusually large Y. Notice though that the slope of the trendline hasnt change too much at all. It is slightly shifted down after you remove that outlier from the calculations, but overall the coefficient of b is similar to before. This type of outlier is not necessarily a big deal. 13.6.5.2 ii. High Leverage Datapoints that have high leverage are those that have a high influence on the regression lines trajectory, but dont necessarily affect the angle of the slope. They are typically unusual in terms of their X value, but not necessarily in terms of Y, meaning that they dont have to have a high residual. We can measure the leverage of datapoints using the function hatvalues() on the model object. For the scatterplot above, the hat values of each datapoint can be calculated as follows: mod.out &lt;- lm(y~x, data = ddf) ddf$hat_val &lt;- hatvalues(mod.out) ddf ## x y y1 hat_val ## 1 4.10 3.9 3.9 0.25535302 ## 2 4.20 4.3 4.3 0.24009985 ## 3 5.00 6.0 6.0 0.14260536 ## 4 5.50 5.9 5.9 0.10381722 ## 5 6.00 6.1 6.1 0.08206442 ## 6 6.10 11.9 NA 0.07975810 ## 7 6.15 6.3 6.3 0.07886047 ## 8 6.40 5.8 5.8 0.07692761 ## 9 7.00 7.9 7.9 0.08966480 ## 10 7.20 6.4 6.4 0.09936183 ## 11 7.70 7.8 7.8 0.13552914 ## 12 8.00 8.0 8.0 0.16540649 ## 13 9.70 9.1 9.1 0.45055168 As you can see, the 6th datapoint was the outlier but it does not have a large leverage as its not overly influencing the trajectory of the regression line. Datapoint 13 on the other hand does have a higher leverage. Roughly speaking, a large hatvalue is one which is 2-3 times the average hat value. We can check the mean of the hat values like this: mean(hatvalues(mod.out)) ## [1] 0.1538462 So clearly, the 13th datapoint does have a high leverage. 13.6.5.3 iii. High Influence The third type of unusual datapoint are ones that you need to be most wary of. These are datapoints that have high influence. Essentially a high influence datapoint is a high leverage datapoint that is also an outlier. Lets look at these example data. We are looking at a predictor variable x against an outcome variable y2. y3 is the same data as y2 with one datapoints value removed. x &lt;- c(4.1,4.2,5,5.5,6,6.15,6.4,7,7.2,7.7,8,9.7) y2 &lt;- c(3.9,4.3,6,5.9,6.1,6.3,5.8,7.9,6.4,7.8,8,6.1) y3 &lt;- c(3.9,4.3,6,5.9,6.1,6.3,5.8,7.9,6.4,7.8,8,NA) ddf1 &lt;- data.frame(x,y2,y3) ddf1 ## x y2 y3 ## 1 4.10 3.9 3.9 ## 2 4.20 4.3 4.3 ## 3 5.00 6.0 6.0 ## 4 5.50 5.9 5.9 ## 5 6.00 6.1 6.1 ## 6 6.15 6.3 6.3 ## 7 6.40 5.8 5.8 ## 8 7.00 7.9 7.9 ## 9 7.20 6.4 6.4 ## 10 7.70 7.8 7.8 ## 11 8.00 8.0 8.0 ## 12 9.70 6.1 NA Lets plot these data - Im sparing you the code for this plot: What you can see here is the scatterplot of x against y2. The blue trendline is the best fit line to the regression of x against y2. The red dotted line is the regression line when we remove the datapoint at x=9.7. This datapoint has both high leverage and is an outlier (has an overly large residual). As a consequence of this, when you remove this datapoint it significantly shifts the angle of the slope of the regression line. That means that the value of b changes quite considerably. Lets add the hat values (measuring leverage) to the data. We can also add what is called Cook's Distance which is a measure of the influence of each datapoint. mod.out2 &lt;- lm(y2~x, data = ddf1) ddf1$hat_val &lt;- hatvalues(mod.out2) ddf1$cooks_d &lt;-cooks.distance(mod.out2) ddf1 ## x y2 y3 hat_val cooks_d ## 1 4.10 3.9 3.9 0.26609280 0.2940686663 ## 2 4.20 4.3 4.3 0.25062833 0.1201644267 ## 3 5.00 6.0 6.0 0.15151904 0.0347793264 ## 4 5.50 5.9 5.9 0.11178988 0.0026090653 ## 5 6.00 6.1 6.1 0.08914853 0.0007585906 ## 6 6.15 6.3 6.3 0.08568825 0.0029898530 ## 7 6.40 5.8 5.8 0.08333867 0.0085341275 ## 8 7.00 7.9 7.9 0.09512926 0.1169635836 ## 9 7.20 6.4 6.4 0.10452756 0.0038328585 ## 10 7.70 7.8 7.8 0.13998476 0.0808111497 ## 11 8.00 8.0 8.0 0.16946124 0.1138881520 ## 12 9.70 6.1 NA 0.45269169 2.8757588661 As you can see, datapoint 12 (x=9.7) has high leverage and its also an outlier. Consequently it has a very large Cooks Distance. Typically a Cooks distance &gt; 1 requires more consideration about how to proceed with your model. You can also quickly get a plot of the Cooks Distances of all datapoints from a regression model like this: plot(mod.out2, which = 4) 13.6.5.4 Checking Influence: Lets look for influential datapoints in our Sleep vs Grumpiness Data. At the beginning of this section, we saved the linear model as mod. We can look at the Cooks Distances with the cooks.distance() function, or we could just plot them: plot(mod1, which = 4) This plot shows each of our 100 datapoints in the order they appear in the original dataframe along the x-axis. Clearly, looking at the y-axis, all our datapoints have Cooks distances of far below 1, so we are fine to assume that we have no overly influential datapoints. 13.7 Examining individual predictor estimates After checking if our model is a good fit and making sure that we did not violate any of the assumptions of the linear regression, we can finally move forward with what we came here to do. That is, we can check whether our predictor is meaningfully useful in predicting our outcome variable. This means, is our observed value of b sufficiently different from 0? To do this, we use two strategies. First, we can generate a 95% confidence interval around our estimate of b. That is the method that we prefer. Secondly, you can run a significance test. 13.7.1 95% confidence interval of b. First, lets do this the easy way. Then well work out how we got the 95% confidence interval. You can simply find this by running the confint() function on our regression model object: # easy way - gives us the confidence interval: coefficients(mod1) ## (Intercept) dan.sleep ## 125.956292 -8.936756 confint(object = mod1, level = .95) ## 2.5 % 97.5 % ## (Intercept) 119.971000 131.94158 ## dan.sleep -9.787161 -8.08635 This tells us that the lower bound of the 95% confidence interval of b is -9.79, and the upper bound is -8.09, with our estimate of b being -8.94. This means that if we were to sample 100 days randomly over and over again and measure Dans sleep and grumpiness on these days, in 95% of the samples we collect we would have the true population parameter value of b. In lay terms, we can suggest that there is approximately a 95% likelihood that this true population value of b is between -9.79 and -8.09. The only way to really get the true population value would have been to measure Dans sleep and grumpiness for every day that Dan has been alive. In big picture terms, what we can draw from this 95% confidence interval is that the relationship between sleep and grumpiness is highly negative and clearly strong. There really seems to be close to no chance that 0 could be the value of b - it is not inside the confidence interval. In fact, we could generate a ridiculously confidence confidence interval, such as a 99.9999% confidence interval: confint(object = mod1, level = .999999) ## 0.00005 % 99.99995 % ## (Intercept) 110.21037 141.702208 ## dan.sleep -11.17398 -6.699536  and 0 is still nowhere near being included. How is this 95% confidence interval calculated? To do this we need to think about those sampling distributions again, and the standard error of b. 13.7.2 Standard Error of b As weve discussed earlier, our one single estimate of b is just one possible estimate. We estimated this value based on our one single sample of 100 days. However, if we had sampled a different sample of 100 days, we would have got a slightly (or maybe greatly) different estimate of b. If we repeated this procedure thousands of times, then wed have a sampling distribution of bs. This sampling distribution will be t-distribution shaped and has degrees of freedom = number of observations - the number of predictors (1) - 1. Therefore it is t-distribution shaped with d.f.=98. If we know the standard deviation of this sampling distribution (which is known as the standard error of b - \\(s_{b}\\)), then we can create confidence intervals. However, it turns out that calculating \\(s_{b}\\) is a bit annoying. Fortunately, there is a quick way to find it out by looking at the summary of the model output: summary(mod1) ## ## Call: ## lm(formula = dan.grump ~ dan.sleep, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.025 -2.213 -0.399 2.681 11.750 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 125.9563 3.0161 41.76 &lt;2e-16 *** ## dan.sleep -8.9368 0.4285 -20.85 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.332 on 98 degrees of freedom ## Multiple R-squared: 0.8161, Adjusted R-squared: 0.8142 ## F-statistic: 434.9 on 1 and 98 DF, p-value: &lt; 2.2e-16 We can see here that \\(s_{b} = 0.4285\\) If youre interested, then this is the formula to calculate \\(s_{b}\\): \\(s_{b} = \\frac{\\sigma_{est}}{\\sqrt{\\Sigma (x - \\overline{x})}}\\) Heres how we calculate it by hand according to this formula: x_dif &lt;- df$dan.sleep - mean(df$dan.sleep) # difference of each x from mean of x ssx &lt;- sqrt(sum(x_dif^2)) # square root of the sum of these differences squared sb &lt;- s_est/ssx #s_est was calculated earlier sb # this is the standard error of b ## [1] 0.4285351 13.7.3 Calculating 95% confidence interval of b by hand We can use the following formula to calculate the 95% CI for b: \\(CI_{95} = b +/- t * s_{b}\\) As with all confidence intervals, what we do is to presume that our estimate of b is the mean of the sampling distribution. Then knowing that the distribution is t-shaped with d.f.=98, we need to find the value of t that will leave 2.5% of the distribution in each tail. We can look that up in R using qt: tval &lt;- qt(.975, df = 98) tval ## [1] 1.984467 Weve now calculated everything we need to for the Confidence Interval b + (tval * sb) #upper bound ## [1] -8.086342 b - (tval * sb) # lower bound ## [1] -9.78717 As we can see, these match up with the output when using confint(): confint(mod1) ## 2.5 % 97.5 % ## (Intercept) 119.971000 131.94158 ## dan.sleep -9.787161 -8.08635 13.7.4 Signifcance Testing b We can also use this sampling distribution to apply a significance test. Our null hypothesis will be that the population value of b is 0. Our alternative hypothesis will be that b is not equal to 0. \\(H_{0}: b=0\\) \\(H_{1}: b\\neq0\\) Given we know the standard deviation of this sampling distribution, \\(s_{b}\\), we can calculate how far away our observed sample value of \\(b\\) is in terms of how many standard deviations from the mean of 0 it is. We call this value a t-statistic and it is calculated like this: \\(t = \\frac{b}{s_{b}}\\) Once we have calculated our t-statistic, then we can determine given the shape of the distribution and the degrees of freedom, what proportion of the distribution is more extreme than our observed value. tobs &lt;- b/sb tobs #-20.854 ## [1] -20.8542 Our observed value is therefore \\(t = -20.854\\). Lets look at this graphically: comparing a and b This shows the t-distribution for d.f. = 98. It looks very strange because weve extended the axes to -21 and +21. We did this so that we could include a dotted blue line for our observed t-value of -20.854. Notice that by the time the t-distribution of d.f.=98 gets to close to a value of t=-3 then there is almost nothing left in the tail of the distribution. Just for completeness, we can calculate what proportion of times we observe a t-value of more extreme (i.e. less than) -20.854 using pt(). pt(-20.854, df=98) ## [1] 4.094577e-38 This essentially gives us a one-tailed p-value of p = 0.00000000000000000000000000000000000004094577, which is very small. To get a 2-tailed p-value we just double this value. Essentially what we can conclude here is that our one value of b is extremely unlikely to have come from a population where b=0, and thus we reject our null hypothesis and accept the alternative. The preceding information was provided to help you think about whats really going on when we calculate the t-statistic. However, it turns out there is actually a shortcut way of calculating t using n and the Pearsons correlation coefficient r. Its the following formula: \\(t = \\frac{r \\times \\sqrt{n-2}}{\\sqrt{1 - r^{2}}}\\) r &lt;- cor(df$dan.sleep, df$dan.grump) n &lt;- nrow(df) (r * sqrt(n-2)) / (sqrt(1-r^2)) # -20.85 ## [1] -20.8544 As you can see from the above code - it works! "],["permutation-testing.html", "Chapter 14 Permutation Testing 14.1 t-test Permutation 14.2 Correlation Coefficient Permutation Tests 14.3 Permutation test for a Paired t-test 14.4 Permutation tests in Packages", " Chapter 14 Permutation Testing Permutation tests are a type of randomization test. The theoretical difference between permutation tests and inferential tests is that with permutation tests we build the sampling distribution from the observed data, rather than inferring or assuming that a sampling distribution exist. In practice, what a permutation test does is to take your observed data and then shuffle (or permute) part of it. After each shuffle, some aspect of the data is recalculated. That could be for instance the correlation coefficient, or it could be a difference in means between two groups. The data then get randomly reshuffled again, and the test-statistic is recalculated again. This goes on for thousands of times - for as many shuffles are deemed acceptable. This is usually a minimum of 1,000 but typically at least 10,000 shuffles are done. After all the permutations (shuffles) are performed, a distribution of the statistic of interest is generated from the permutations. This is compared to the original observed statistics (e.g. correlation coefficient, difference in group means) to see if the observed value is unusually large compared to the permuted data. If this seems a little confusing, hopefully seeing it in action will help 14.1 t-test Permutation Lets look at our two independent samples of exam scores: library(tidyverse) anastasia &lt;- c(65, 74, 73, 83, 76, 65, 86, 70, 80, 55, 78, 78, 90, 77, 68) bernadette &lt;- c(72, 66, 71, 66, 76, 69, 79, 73, 62, 69, 68, 60, 73, 68, 67, 74, 56, 74) # put into a dataframe: dd &lt;- data.frame(values = c(anastasia, bernadette), group = c(rep(&quot;Anastasia&quot;,15), rep(&quot;Bernadette&quot;, 18)) ) dd ## values group ## 1 65 Anastasia ## 2 74 Anastasia ## 3 73 Anastasia ## 4 83 Anastasia ## 5 76 Anastasia ## 6 65 Anastasia ## 7 86 Anastasia ## 8 70 Anastasia ## 9 80 Anastasia ## 10 55 Anastasia ## 11 78 Anastasia ## 12 78 Anastasia ## 13 90 Anastasia ## 14 77 Anastasia ## 15 68 Anastasia ## 16 72 Bernadette ## 17 66 Bernadette ## 18 71 Bernadette ## 19 66 Bernadette ## 20 76 Bernadette ## 21 69 Bernadette ## 22 79 Bernadette ## 23 73 Bernadette ## 24 62 Bernadette ## 25 69 Bernadette ## 26 68 Bernadette ## 27 60 Bernadette ## 28 73 Bernadette ## 29 68 Bernadette ## 30 67 Bernadette ## 31 74 Bernadette ## 32 56 Bernadette ## 33 74 Bernadette We can plot these data as boxplots to get a sense of the within group variation as well as the observed differences between the groups: ggplot(dd, aes(x = group, y = values, fill = group)) + geom_boxplot(alpha=.3, outlier.shape = NA) + geom_jitter(width=.1, size=2) + theme_classic() + scale_fill_manual(values = c(&quot;firebrick&quot;, &quot;dodgerblue&quot;)) Now, from our two independent samples, we can directly observe what the difference in sample means is. This is just calculated by subtracting one sample mean from the other: meandif &lt;- mean(anastasia) - mean(bernadette) # 5.48 meandif ## [1] 5.477778 So, from our samples, we observed a difference in grades of 5.48 between the groups. Typically, we would run an independent t-test to test whether these two samples came from theoretical populations that differ in their means: t.test(anastasia, bernadette, var.equal = T) ## ## Two Sample t-test ## ## data: anastasia and bernadette ## t = 2.1154, df = 31, p-value = 0.04253 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.1965873 10.7589683 ## sample estimates: ## mean of x mean of y ## 74.53333 69.05556 This Students t-test (notice var.equal=T) suggests that this is a significant difference, meaning that the groups do differ in their population means. However, this test relies on several assumptions (see section 11.7). Instead, we could apply a permutation test that is free of assumptions. Essentially what we are going to do is ask how surprising it was to get a difference of 5.48 given our real data. Put another way, if we shuffled the data into different groups of 15 and 18 (the respective sample sizes of Anastasia and Bernadette), would we get a difference in sample means of greater or lower than 5.48? If we did this thousands of times, how many times would we get differences in sample means above 5.48? Lets apply this theory to just one permutation. First, we combine all the data: set.seed(1) # just to keep the random number generator the same for all of us allscores &lt;- c(anastasia, bernadette) allscores ## [1] 65 74 73 83 76 65 86 70 80 55 78 78 90 77 68 72 66 71 66 76 69 79 73 62 69 68 60 73 68 67 74 56 74 Next, we shuffle them into new groups of 15 and 18.: x &lt;- split(sample(allscores), rep(1:2, c(15,18))) x ## $`1` ## [1] 83 86 65 74 68 73 78 77 71 66 74 69 79 55 68 ## ## $`2` ## [1] 76 56 80 68 76 62 72 78 90 69 67 70 65 73 74 73 60 66 We have two brand new samples that contain all of the scores from our original data, but theyve just been shuffled around. We could look at what the difference in sample means is between these two new samples: x[[1]] # this is our shuffled sample of size 15 ## [1] 83 86 65 74 68 73 78 77 71 66 74 69 79 55 68 x[[2]] # this is our shuffled sample of size 18 ## [1] 76 56 80 68 76 62 72 78 90 69 67 70 65 73 74 73 60 66 mean(x[[1]]) # mean of the new sample of size 15 ## [1] 72.4 mean(x[[2]]) # mean of the new sample of size 18 ## [1] 70.83333 # what&#39;s the difference in their means? mean(x[[1]]) - mean(x[[2]]) ## [1] 1.566667 The difference in sample means is 1.81, which is a lot smaller than our original difference in sample means. Lets do this same process 10,000 times! Dont worry too much about the details of the code. What we are doing is the above process, just putting it in a loop and asking it to do it 10,000 times. We save all the results in an object called results. results&lt;-vector(&#39;list&#39;,10000) for(i in 1:10000){ x &lt;- split(sample(allscores), rep(1:2, c(15,18))) results[[i]]&lt;-mean(x[[1]]) - mean(x[[2]]) } head(unlist(results)) # these are all our mean differences from 10,000 shuffles of the data. We&#39;re just looking at the first 6. ## [1] -4.0555556 4.7444444 -2.3444444 -0.3888889 1.6888889 -6.7444444 We can actually make a histogram showing the distribution of these differences in sample means. df &lt;- data.frame(difs = unlist(results)) ggplot(df, aes(x=difs)) + geom_histogram(color=&quot;black&quot;, fill=&quot;green&quot;, alpha=.4) + geom_vline(color=&quot;navy&quot;,lwd=1,lty=2,xintercept = 5.48) + theme_classic()+ ggtitle(&quot;Mean Differences from \\n 10000 Permutations of Raw Data&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. This histogram shows that for some of our 10,000 shuffles, we actually got some differences between our two samples of higher than 5.48 (the dotted blue line), but the vast majority of shuffles led to samples that had mean differences lower than 5.48. In fact, several shuffles led to samples where the sample of size 18 (Bernadette in the original data) had a sample mean that was higher than the sample of size 15 (Anastasia in the original data). We can directly calculate how many times out of 10,000 shuffles we got a difference in sample means that was greater than 5.48 sum(unlist(results) &gt; 5.48) # 202 times out of 10000 ## [1] 202 To convert this to a p-value, we simply divide this value by the number of shuffles we ran - which was 10,000. sum(unlist(results) &gt; 5.48) /10000 # which is 0.0202 proportion of the time ## [1] 0.0202 So our p-value is p=0.0215 which is similar to a one-tailed p-value. If we wished to have a 2-tailed p-value we would simply multiply this value by 2: # 2-tailed value 2 * (sum(unlist(results) &gt; 5.48) /10000) ## [1] 0.0404 Example 2: Lets take a look at a second example. Here, we have various subjects rating their anxiety levels. They do this after either taking a new anxiolytic drug or a placebo. The subjects in each group are independent of each other. The placebo group has 19 subjects and the drug group has 21 subjects. The data: placebo &lt;- c(15, 16, 19, 19, 17, 20, 18, 14, 18, 20, 20, 20, 13, 11, 16, 19, 19, 16, 10) drug &lt;- c(15, 15, 16, 13, 11, 19, 17, 17, 11, 14, 10, 18, 19, 14, 13, 16, 16, 17, 14, 10, 14) length(placebo) #19 ## [1] 19 length(drug) #21 ## [1] 21 If we were interested in doing a Students t-test, we might want to check whether the data are approximately normal. We could perform Shapiro-Wilk tests to do this: shapiro.test(drug) # approximately normal as p&gt;.05 ## ## Shapiro-Wilk normality test ## ## data: drug ## W = 0.95184, p-value = 0.3688 shapiro.test(placebo) # not enough evidence to be normal as p&lt;.05 ## ## Shapiro-Wilk normality test ## ## data: placebo ## W = 0.88372, p-value = 0.02494 From this we find that the placebo group is not approximately normally distributed (p value of the Shapiro-Wilk test is &lt;.05). We could do a non-parametric test such as Wilcoxon Ranked Sum test (see xxx.xxx), but an alternative strategy is to perform a permutation test. Lets first plot the data, and then look at our observed difference in anxiety scores between our two independent samples: # put into dataframe - long format ddf &lt;- data.frame(anxiety = c(placebo, drug), group = c(rep(&quot;placebo&quot;, length(placebo)), rep(&quot;drug&quot;, length(drug)) ) ) head(ddf) ## anxiety group ## 1 15 placebo ## 2 16 placebo ## 3 19 placebo ## 4 19 placebo ## 5 17 placebo ## 6 20 placebo #boxplots ggplot(ddf, aes(x=group, y=anxiety, fill=group)) + geom_boxplot(outlier.shape = NA, alpha=.4) + geom_jitter(width=.1) + theme_classic() + scale_fill_manual(values=c(&quot;orange&quot;, &quot;brown&quot;)) mean(placebo) - mean(drug) #2.128 ## [1] 2.12782 So our observed difference in sample means is 2.128. In the permutation test, what well do is shuffle all the scores randomly between the two groups, creating new samples of the same size (19 and 21). Then well see what difference in sample means we get from those shuffled groups. Well also do this 10,000 times. allvalues &lt;- c(placebo, drug) results&lt;-vector(&#39;list&#39;,10000) for(i in 1:10000){ x &lt;- split(sample(allvalues), rep(1:2, c(19,21))) results[[i]]&lt;-mean(x[[1]]) - mean(x[[2]]) } head(unlist(results)) # these are the first six of all our mean differences from 10,000 shuffles of the data. ## [1] 1.1253133 0.7243108 -0.5789474 -0.6791980 0.5238095 1.7268170 Lets plot the distribution of these data to see what proportion of times our shuffled groups got samples that were greater than 2.128. df0 &lt;- data.frame(difs = unlist(results)) ggplot(df0, aes(x=difs)) + geom_histogram(color=&quot;black&quot;, fill=&quot;pink&quot;, alpha=.4) + geom_vline(color=&quot;navy&quot;,lwd=1,lty=2,xintercept = 2.128) + theme_classic()+ ggtitle(&quot;Mean Differences from \\n 10000 Permutations of Raw Data&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. It looks like very few times did we get two samples that had differences in sample means that were greater than 2.128. We can calculate exactly how many times, and express this as the proportion of times we got a difference in sample means greater than 2.128: sum(unlist(results) &gt; 2.128) # 109 times out of 10000 ## [1] 116 sum(unlist(results) &gt; 2.128) /10000 # which is 0.0109 proportion of the time ## [1] 0.0116 So, in this case we can say that the probability of getting a difference in sample means between the drug and placebo groups that was larger than our observed difference of 2.128 was p = 0.0109. This is very strong evidence that the observed difference is significantly greater than wed expect by chance. 14.2 Correlation Coefficient Permutation Tests You can apply the logic of permutation tests to almost any statistical test. Lets look at an example for Pearson correlations. In these data, we are looking at 15 subjects who are completing a task. We measured the time they spent on the task and their high scores. library(tidyverse) df &lt;- read_csv(&quot;data/timescore.csv&quot;) ## Rows: 15 Columns: 3 ## -- Column specification ----------------------------------------------------------------------------------- ## Delimiter: &quot;,&quot; ## chr (1): subject ## dbl (2): time, score ## ## i Use `spec()` to retrieve the full column specification for this data. ## i Specify the column types or set `show_col_types = FALSE` to quiet this message. head(df) ## # A tibble: 6 x 3 ## subject time score ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1A 5.5 3 ## 2 2B 2.4 6.9 ## 3 3C 8.8 17.9 ## 4 4D 7 10.5 ## 5 5E 9.3 12.2 ## 6 6F 2.5 3.5 If we make a scatterplot of the data, we can see that those who spent longer on the task tended to get higher scores: # scatterplot ggplot(df, aes(x = time, y = score)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se=F) ## `geom_smooth()` using formula &#39;y ~ x&#39; Using a standard approach, we could find the correlation of these two variables and run a significance test using cor.test(). We can see that there is a moderate Pearsons r of r=0.55 which is statistically significant (p=0.031). # regular significance test cor.test(df$time,df$score) #r=0.55, p=0.031 ## ## Pearson&#39;s product-moment correlation ## ## data: df$time and df$score ## t = 2.4258, df = 13, p-value = 0.03057 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.0643515 0.8324385 ## sample estimates: ## cor ## 0.5582129 We could take an alternative tack, and decide to do a permutation test. The idea here is again, how surprising is it to get a correlation of 0.55 with these data? Were there other ways of ordering the x and y variables to get higher correlation coefficients? Lets look at our y axis variable, the score: set.seed(1) # just doing this so all our results look same df$score # actual data in order ## [1] 3.0 6.9 17.9 10.5 12.2 3.5 11.0 7.6 8.4 13.4 10.1 9.0 10.1 17.7 6.8 This is the original order of the data. If we use sample() we can shuffle the data: sample(df$score) # actual data but order shuffled ## [1] 8.4 10.5 11.0 3.0 6.9 10.1 10.1 17.9 7.6 9.0 12.2 3.5 6.8 13.4 17.7 Lets shuffle the score again, but this time store it in the original dataframe: df$shuffle1 &lt;- sample(df$score) #create a new column with shuffled data df ## # A tibble: 15 x 4 ## subject time score shuffle1 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1A 5.5 3 13.4 ## 2 2B 2.4 6.9 11 ## 3 3C 8.8 17.9 8.4 ## 4 4D 7 10.5 12.2 ## 5 5E 9.3 12.2 9 ## 6 6F 2.5 3.5 10.1 ## 7 7G 4.8 11 6.8 ## 8 8H 4.1 7.6 3.5 ## 9 9I 5 8.4 10.1 ## 10 10J 2.9 13.4 17.7 ## 11 11K 6.4 10.1 6.9 ## 12 12L 7.7 9 7.6 ## 13 13M 9.3 10.1 10.5 ## 14 14N 8.3 17.7 3 ## 15 15O 5.1 6.8 17.9 If we plot this shuffled y (score) against the original x (time), we now get this scatterplot, which basically shows no relationship: # this is what that new column looks like: ggplot(df, aes(x = time, y = shuffle1)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se=F) ## `geom_smooth()` using formula &#39;y ~ x&#39; And the correlation for this new scatterplot is really close to 0! r = 0.0005: cor.test(df$time, df$shuffle1) # now relationship is a bit negative ## ## Pearson&#39;s product-moment correlation ## ## data: df$time and df$shuffle1 ## t = -1.2077, df = 13, p-value = 0.2487 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.7137421 0.2324852 ## sample estimates: ## cor ## -0.317602 We could shuffle the score variable even more times, and directly calculate the r value against the time variable for each shuffle using cor(). # we can do this many times cor(df$time, sample(df$score)) ## [1] 0.1600023 cor(df$time, sample(df$score)) ## [1] -0.1238816 cor(df$time, sample(df$score)) ## [1] 0.3472331 cor(df$time, sample(df$score)) ## [1] 0.04201647 As you can see, the more shuffles we do, we get varied values of r. What we really should do is perform 10,000 (or another really high number) shuffles of the score variable and re-calculate r against the time variable for all 10,000 of these shuffles. Dont worry about the code below, but thats exactly what were doing. Were saving the r values from the 10,000 shuffles in the object called results. results &lt;- vector(&#39;list&#39;,10000) for(i in 1:10000){ results[[i]] &lt;- cor(df$time, sample(df$score)) } head(unlist(results)) # this are the correlations for the first 6 of 10,000 shuffles ## [1] -0.05905503 0.14874908 -0.25029004 0.10780960 -0.03206121 -0.49661730 We can plot the results in a histogram, and also put a vertical line at 0.56 which was our original observed correlation between time and score from the raw unshuffled data. results.df &lt;- data.frame(x = unlist(results)) ggplot(results.df, aes(x)) + geom_histogram(color=&quot;darkgreen&quot;,fill=&quot;lightseagreen&quot;) + geom_vline(xintercept = 0.56, lwd=1, lty=2) + xlab(&quot;r&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. As you can see, there were a few shuffles (or permutations) that we got an r value of greater than 0.56, but not that many. In fact, we can directly calculate how many: sum(unlist(results) &gt; 0.56) #163 were greater. ## [1] 130 It turns out that 163 times out of 10,000 shuffles we got a r value of greater than 0.56. WE could calculate this as a proportion by dividing by 10,000: sum(unlist(results) &gt; 0.56) / 10000 #0.0163 ## [1] 0.013 We can use this value as our p-value. Because it is relatively low, we could argue that we were very unlikely by chance alone to have got a r value of 0.56 from our data. This suggests that the correlation between time and score is significant. The advantages of running a permutation test is that it is free of the assumptions of normality for the Pearsons r correlation significance test. Its also a cool method, and pretty intuitive. 14.3 Permutation test for a Paired t-test We can apply the same principle of permutation to the paired t-test. Remember, essentially the paired t-test is focused on performing a one-sample t-test on the difference in scores between the paired data - testing whether the mean of the differences could potentially come from a population with \\(\\mu=0\\). Lets look at the following data that record scores for the same individual over two time points - before and after. # take a look at these before and after scores ba &lt;- read_csv(&quot;data/beforeafter1.csv&quot;) head(ba) ## # A tibble: 6 x 3 ## id before after ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mc 5.5 5.3 ## 2 ma 5.7 5.3 ## 3 co 4.4 3.3 ## 4 kj 3.4 3.1 ## 5 ln 5.3 5.3 ## 6 oe 5.2 5.1 We could plot these data using a scatterplot to examine the overall trend of how scores change from before to after: # make a scatterplot with the x being &#39;before&#39; and y being &#39;after&#39; ggplot(ba, aes(x=before, y=after)) + geom_point() + theme_classic()+ geom_abline(intercept =0 , slope = 1) + xlim(2,8)+ ylim(2,8) As most of these points are below the diagonal line, this seems to suggest that the scores for the before data seem to be lower on the whole than the scores for the above data. Typically, we would run a paired t-test with such data to examine if there was a difference: t.test(ba$before, ba$after, paired=T) ## ## Paired t-test ## ## data: ba$before and ba$after ## t = 2.6667, df = 10, p-value = 0.02363 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.1315583 1.4684417 ## sample estimates: ## mean of the differences ## 0.8 This suggests that there is a significant difference p&lt;.05 with the 95% confidence interval of the true difference in means being between 0.13 and 1.47. However, the paired t-test assumes that the data are from an approximately normal distribution. In particular, that the differences scores (the difference between the before and after scores for each individual) are normally distributed. We can check that using a Shapiro-Wilk test: # create a difference column for the difference between before and after ba$difference &lt;- ba$before - ba$after # run a Shapiro test on the difference column shapiro.test(ba$difference) ## ## Shapiro-Wilk normality test ## ## data: ba$difference ## W = 0.82621, p-value = 0.02081 With the p-value here being p&lt;.05, this suggests that our data are not normally distributed. One option would be to do a non-parametric Wilcoxon-signed rank test (see section 11.12). Alternatively, we could do a permutation test. Lets look at our data again, and focus on the difference column. ba ## # A tibble: 11 x 4 ## id before after difference ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mc 5.5 5.3 0.200 ## 2 ma 5.7 5.3 0.400 ## 3 co 4.4 3.3 1.10 ## 4 kj 3.4 3.1 0.300 ## 5 ln 5.3 5.3 0 ## 6 oe 5.2 5.1 0.100 ## 7 mb 3.4 3 0.4 ## 8 dc 7.5 5 2.5 ## 9 dg 3.4 2.1 1.3 ## 10 mj 6.6 3.9 2.7 ## 11 kb 5 5.2 -0.200 Our observed mean for the differences scores is 0.8. mean(ba$difference) ## [1] 0.8 How likely were we to get this mean difference if our before and after conditions were randomized? For example, for individual mj, their before score was 6.6 and after was 3.9 leading to a difference of 2.7. But what if their before and after were switched? Then the difference score would be -2.7. What we would like to do, is to randomly flip the before and after columns for each individual and recalculate the difference scores. Each time we do this, we will calculate the mean of the difference scores. A programmatic shortcut to doing this is to multiple each difference score randomly by either +1 or -1. Here is the first shuffle we could perform: set.seed(1) shuffle1 &lt;- ba$difference * sample(c(-1,1), 11, replace = T) shuffle1 ## [1] -0.2 0.4 -1.1 -0.3 0.0 -0.1 -0.4 -2.5 1.3 2.7 0.2 mean(shuffle1) ## [1] -8.07361e-17 In this example, the before and after scores were randomly flipped for individuals mc, ma, mj and kb. Lets do a second shuffle: shuffle2 &lt;- ba$difference * sample(c(-1,1), 11, replace = T) shuffle2 ## [1] -0.2 -0.4 -1.1 -0.3 0.0 0.1 0.4 2.5 -1.3 -2.7 0.2 mean(shuffle2) ## [1] -0.2545455 In this example, the before and after scores were randomly flipped for individuals mc, co, kj, oe, mb, dg and mj. In both shuffles the mean of the difference scores was less than our observed mean of 0.8. We can put this into a loop to do it 10,000 times: results &lt;- vector(&#39;list&#39;,10000) for(i in 1:10000){ results[[i]] &lt;- mean(ba$difference * sample(c(-1,1), 11, replace = T)) } And we can plot these results as a histogram: df1 &lt;- data.frame(difs = unlist(results)) ggplot(df1, aes(x=difs)) + geom_histogram(color=&quot;black&quot;, fill=&quot;pink&quot;, alpha=.4, binwidth = .05) + geom_vline(color=&quot;navy&quot;,lwd=1,lty=2,xintercept = .8) + theme_classic()+ ggtitle(&quot;Mean Differences from \\n 10000 Permutations of Raw Data&quot;) We can also calculate the number of times out of 10,000 that we observed a mean difference higher than the mean of 0.8 in our original data, which is only in 19 shuffles out fo 10,000: sum(unlist(results)&gt;0.8) ## [1] 16 We divide this number by 10,000 to get our p-value: sum(unlist(results)&gt;0.8) / 10000 ## [1] 0.0016 This suggests that we have a highly significant p=0.002 difference between our before and after data within subjects. 14.4 Permutation tests in Packages Above we wrote script from scratch to perform our permutation tests. In many ways, this is our preferred approach as it is more customizable. However, in some packages there are some permutation tests already available as functions. One example is the independence_test from the package coin that will do a permutation t-test for between subjects. The code for this is below (this requires dataframes to be in the long format): library(coin) head(ddf) ## anxiety group ## 1 15 placebo ## 2 16 placebo ## 3 19 placebo ## 4 19 placebo ## 5 17 placebo ## 6 20 placebo # for some reason, the coin package makes you make the &#39;group&#39; column/variable a factor ddf$group &lt;- factor(ddf$group) independence_test(anxiety ~ group, data = ddf, alternative = &quot;less&quot;) ## ## Asymptotic General Independence Test ## ## data: anxiety by group (drug, placebo) ## Z = -2.1998, p-value = 0.01391 ## alternative hypothesis: less As you can see, this gives a roughly similar result to our own permutation script. You can also do a 2-tailed version: #2-tailed permutation test independence_test(anxiety ~ group, data = ddf) ## ## Asymptotic General Independence Test ## ## data: anxiety by group (drug, placebo) ## Z = -2.1998, p-value = 0.02782 ## alternative hypothesis: two.sided "],["analyzing-categorical-data.html", "Chapter 15 Analyzing Categorical Data", " Chapter 15 Analyzing Categorical Data This chapter will explore analyzing categorical data using Chi-Squared and Fisher-Exact tests. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
