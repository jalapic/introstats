<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 Linear Regression | PSY317L &amp; PSY120R Textbook</title>
  <meta name="description" content="Chapter 12 Linear Regression | PSY317L &amp; PSY120R Textbook" />
  <meta name="generator" content="bookdown 0.23 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 Linear Regression | PSY317L &amp; PSY120R Textbook" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Linear Regression | PSY317L &amp; PSY120R Textbook" />
  
  
  

<meta name="author" content="James P. Curley &amp; Tyler M. Milewski" />


<meta name="date" content="2021-08-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="correlation.html"/>
<link rel="next" href="permutation-testing.html"/>
<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intro to Statistics & R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Welcome to PSY317 / PSY120R !</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#what-this-book-includes-and-what-it-doesnt"><i class="fa fa-check"></i><b>1.1</b> What this book includes and what it doesn’t</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#how-to-use-this-guide"><i class="fa fa-check"></i><b>1.2</b> How to use this guide</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>1.3</b> Acknowledgements</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i><b>1.4</b> References</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#other-places-to-find-help-about-r-and-statistics"><i class="fa fa-check"></i><b>1.5</b> Other places to find help about R and Statistics</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="basic-syntax.html"><a href="basic-syntax.html"><i class="fa fa-check"></i><b>2</b> Basic Syntax</a>
<ul>
<li class="chapter" data-level="2.1" data-path="basic-syntax.html"><a href="basic-syntax.html#simple-mathematical-syntax"><i class="fa fa-check"></i><b>2.1</b> Simple mathematical syntax</a></li>
<li class="chapter" data-level="2.2" data-path="basic-syntax.html"><a href="basic-syntax.html#assignment"><i class="fa fa-check"></i><b>2.2</b> Assignment</a></li>
<li class="chapter" data-level="2.3" data-path="basic-syntax.html"><a href="basic-syntax.html#vectors"><i class="fa fa-check"></i><b>2.3</b> Vectors</a></li>
<li class="chapter" data-level="2.4" data-path="basic-syntax.html"><a href="basic-syntax.html#characters"><i class="fa fa-check"></i><b>2.4</b> Characters</a></li>
<li class="chapter" data-level="2.5" data-path="basic-syntax.html"><a href="basic-syntax.html#naming-of-objects"><i class="fa fa-check"></i><b>2.5</b> Naming of objects</a></li>
<li class="chapter" data-level="2.6" data-path="basic-syntax.html"><a href="basic-syntax.html#logical-operators"><i class="fa fa-check"></i><b>2.6</b> Logical Operators</a></li>
<li class="chapter" data-level="2.7" data-path="basic-syntax.html"><a href="basic-syntax.html#some-things-that-are-useful-to-know."><i class="fa fa-check"></i><b>2.7</b> Some things that are useful to know.</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="basic-syntax.html"><a href="basic-syntax.html#tab-is-your-friend"><i class="fa fa-check"></i><b>2.7.1</b> Tab is your friend</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="basic-syntax.html"><a href="basic-syntax.html#error-messages"><i class="fa fa-check"></i><b>2.8</b> Error Messages</a></li>
<li class="chapter" data-level="2.9" data-path="basic-syntax.html"><a href="basic-syntax.html#functions"><i class="fa fa-check"></i><b>2.9</b> Functions</a></li>
<li class="chapter" data-level="2.10" data-path="basic-syntax.html"><a href="basic-syntax.html#chaining-syntax"><i class="fa fa-check"></i><b>2.10</b> Chaining Syntax</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html"><i class="fa fa-check"></i><b>3</b> Introduction to Data Carpentry</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#data-types"><i class="fa fa-check"></i><b>3.1</b> Data Types</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#categorical-data"><i class="fa fa-check"></i><b>3.1.1</b> Categorical Data</a></li>
<li class="chapter" data-level="3.1.2" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#numerical-data-discrete-vs.-continuous"><i class="fa fa-check"></i><b>3.1.2</b> Numerical Data (Discrete vs. Continuous)</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#importing-data"><i class="fa fa-check"></i><b>3.2</b> Importing Data</a></li>
<li class="chapter" data-level="3.3" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#introduction-to-dataframes"><i class="fa fa-check"></i><b>3.3</b> Introduction to Dataframes</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#dataframe-basics"><i class="fa fa-check"></i><b>3.3.1</b> Dataframe basics</a></li>
<li class="chapter" data-level="3.3.2" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#indexing-dataframes."><i class="fa fa-check"></i><b>3.3.2</b> Indexing dataframes.</a></li>
<li class="chapter" data-level="3.3.3" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#adding-and-removing-columns"><i class="fa fa-check"></i><b>3.3.3</b> Adding and removing columns</a></li>
<li class="chapter" data-level="3.3.4" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#structure-of-datasets"><i class="fa fa-check"></i><b>3.3.4</b> Structure of Datasets</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#manually-creating-a-dataframe"><i class="fa fa-check"></i><b>3.4</b> Manually creating a Dataframe</a></li>
<li class="chapter" data-level="3.5" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#tidyverse"><i class="fa fa-check"></i><b>3.5</b> tidyverse</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#table"><i class="fa fa-check"></i><b>3.5.1</b> table()</a></li>
<li class="chapter" data-level="3.5.2" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#filter---subsetting-data"><i class="fa fa-check"></i><b>3.5.2</b> filter() - Subsetting Data</a></li>
<li class="chapter" data-level="3.5.3" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#select---selecting-specific-columns"><i class="fa fa-check"></i><b>3.5.3</b> select() - Selecting specific columns</a></li>
<li class="chapter" data-level="3.5.4" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#mutate---creating-new-columns"><i class="fa fa-check"></i><b>3.5.4</b> mutate() - Creating new columns</a></li>
<li class="chapter" data-level="3.5.5" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#arrange---sort-data-columns"><i class="fa fa-check"></i><b>3.5.5</b> arrange() - Sort Data Columns</a></li>
<li class="chapter" data-level="3.5.6" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#chaining-together"><i class="fa fa-check"></i><b>3.5.6</b> Chaining together</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#wide-versus-long-data"><i class="fa fa-check"></i><b>3.6</b> Wide versus Long Data</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#wide-to-long"><i class="fa fa-check"></i><b>3.6.1</b> Wide to Long</a></li>
<li class="chapter" data-level="3.6.2" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#long-to-wide"><i class="fa fa-check"></i><b>3.6.2</b> Long to Wide</a></li>
<li class="chapter" data-level="3.6.3" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#real-data-example."><i class="fa fa-check"></i><b>3.6.3</b> Real Data Example.</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#joins"><i class="fa fa-check"></i><b>3.7</b> Joins</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data-visualization.html"><a href="data-visualization.html"><i class="fa fa-check"></i><b>4</b> Data Visualization</a>
<ul>
<li class="chapter" data-level="4.1" data-path="data-visualization.html"><a href="data-visualization.html#introduction-to-ggplot2"><i class="fa fa-check"></i><b>4.1</b> Introduction to ggplot2</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="data-visualization.html"><a href="data-visualization.html#assigning-plots"><i class="fa fa-check"></i><b>4.1.1</b> Assigning plots</a></li>
<li class="chapter" data-level="4.1.2" data-path="data-visualization.html"><a href="data-visualization.html#titles-and-axes-titles"><i class="fa fa-check"></i><b>4.1.2</b> Titles and Axes Titles</a></li>
<li class="chapter" data-level="4.1.3" data-path="data-visualization.html"><a href="data-visualization.html#colors-shapes-and-sizes"><i class="fa fa-check"></i><b>4.1.3</b> Colors, Shapes and Sizes</a></li>
<li class="chapter" data-level="4.1.4" data-path="data-visualization.html"><a href="data-visualization.html#themes"><i class="fa fa-check"></i><b>4.1.4</b> Themes</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="data-visualization.html"><a href="data-visualization.html#histograms"><i class="fa fa-check"></i><b>4.2</b> Histograms</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="data-visualization.html"><a href="data-visualization.html#histograms-with-ggplot2"><i class="fa fa-check"></i><b>4.2.1</b> Histograms with ggplot2</a></li>
<li class="chapter" data-level="4.2.2" data-path="data-visualization.html"><a href="data-visualization.html#density-curves"><i class="fa fa-check"></i><b>4.2.2</b> Density Curves</a></li>
<li class="chapter" data-level="4.2.3" data-path="data-visualization.html"><a href="data-visualization.html#comparing-distributions"><i class="fa fa-check"></i><b>4.2.3</b> Comparing Distributions</a></li>
<li class="chapter" data-level="4.2.4" data-path="data-visualization.html"><a href="data-visualization.html#stem-and-leaf-plots"><i class="fa fa-check"></i><b>4.2.4</b> Stem-and-Leaf Plots</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="data-visualization.html"><a href="data-visualization.html#scatterplots"><i class="fa fa-check"></i><b>4.3</b> Scatterplots</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="data-visualization.html"><a href="data-visualization.html#bubble-charts"><i class="fa fa-check"></i><b>4.3.1</b> Bubble Charts</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="data-visualization.html"><a href="data-visualization.html#line-graphs"><i class="fa fa-check"></i><b>4.4</b> Line Graphs</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="data-visualization.html"><a href="data-visualization.html#multiple-line-graphs"><i class="fa fa-check"></i><b>4.4.1</b> Multiple Line Graphs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="data-visualization.html"><a href="data-visualization.html#comparing-distributions-across-groups"><i class="fa fa-check"></i><b>4.5</b> Comparing Distributions across Groups</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="data-visualization.html"><a href="data-visualization.html#strip-plots"><i class="fa fa-check"></i><b>4.5.1</b> Strip Plots</a></li>
<li class="chapter" data-level="4.5.2" data-path="data-visualization.html"><a href="data-visualization.html#boxplots"><i class="fa fa-check"></i><b>4.5.2</b> Boxplots</a></li>
<li class="chapter" data-level="4.5.3" data-path="data-visualization.html"><a href="data-visualization.html#violin-plots"><i class="fa fa-check"></i><b>4.5.3</b> Violin Plots</a></li>
<li class="chapter" data-level="4.5.4" data-path="data-visualization.html"><a href="data-visualization.html#stacked-boxplots"><i class="fa fa-check"></i><b>4.5.4</b> Stacked Boxplots</a></li>
<li class="chapter" data-level="4.5.5" data-path="data-visualization.html"><a href="data-visualization.html#ridgeline-plots"><i class="fa fa-check"></i><b>4.5.5</b> Ridgeline Plots</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="data-visualization.html"><a href="data-visualization.html#bar-graphs"><i class="fa fa-check"></i><b>4.6</b> Bar Graphs</a></li>
<li class="chapter" data-level="4.7" data-path="data-visualization.html"><a href="data-visualization.html#small-multiples"><i class="fa fa-check"></i><b>4.7</b> Small Multiples</a></li>
<li class="chapter" data-level="4.8" data-path="data-visualization.html"><a href="data-visualization.html#saving-and-exporting-ggplot2-graphs"><i class="fa fa-check"></i><b>4.8</b> Saving and Exporting ggplot2 graphs</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="descriptives.html"><a href="descriptives.html"><i class="fa fa-check"></i><b>5</b> Descriptives</a>
<ul>
<li class="chapter" data-level="5.1" data-path="descriptives.html"><a href="descriptives.html#sample-vs-population"><i class="fa fa-check"></i><b>5.1</b> Sample vs Population</a></li>
<li class="chapter" data-level="5.2" data-path="descriptives.html"><a href="descriptives.html#sample-and-population-size"><i class="fa fa-check"></i><b>5.2</b> Sample and Population Size</a></li>
<li class="chapter" data-level="5.3" data-path="descriptives.html"><a href="descriptives.html#central-tendency"><i class="fa fa-check"></i><b>5.3</b> Central Tendency</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="descriptives.html"><a href="descriptives.html#mode"><i class="fa fa-check"></i><b>5.3.1</b> Mode</a></li>
<li class="chapter" data-level="5.3.2" data-path="descriptives.html"><a href="descriptives.html#median"><i class="fa fa-check"></i><b>5.3.2</b> Median</a></li>
<li class="chapter" data-level="5.3.3" data-path="descriptives.html"><a href="descriptives.html#mean"><i class="fa fa-check"></i><b>5.3.3</b> Mean</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="descriptives.html"><a href="descriptives.html#variation"><i class="fa fa-check"></i><b>5.4</b> Variation</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="descriptives.html"><a href="descriptives.html#range"><i class="fa fa-check"></i><b>5.4.1</b> Range</a></li>
<li class="chapter" data-level="5.4.2" data-path="descriptives.html"><a href="descriptives.html#interquartile-range"><i class="fa fa-check"></i><b>5.4.2</b> Interquartile Range</a></li>
<li class="chapter" data-level="5.4.3" data-path="descriptives.html"><a href="descriptives.html#average-deviation"><i class="fa fa-check"></i><b>5.4.3</b> Average Deviation</a></li>
<li class="chapter" data-level="5.4.4" data-path="descriptives.html"><a href="descriptives.html#standard-deviation"><i class="fa fa-check"></i><b>5.4.4</b> Standard Deviation</a></li>
<li class="chapter" data-level="5.4.5" data-path="descriptives.html"><a href="descriptives.html#variance"><i class="fa fa-check"></i><b>5.4.5</b> Variance</a></li>
<li class="chapter" data-level="5.4.6" data-path="descriptives.html"><a href="descriptives.html#average-versus-standard-deviation"><i class="fa fa-check"></i><b>5.4.6</b> Average versus Standard Deviation</a></li>
<li class="chapter" data-level="5.4.7" data-path="descriptives.html"><a href="descriptives.html#sample-standard-deviation"><i class="fa fa-check"></i><b>5.4.7</b> Sample Standard Deviation</a></li>
<li class="chapter" data-level="5.4.8" data-path="descriptives.html"><a href="descriptives.html#sample-versus-population-standard-deviation"><i class="fa fa-check"></i><b>5.4.8</b> Sample versus Population Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="descriptives.html"><a href="descriptives.html#descriptive-statistics-in-r"><i class="fa fa-check"></i><b>5.5</b> Descriptive Statistics in R</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="descriptives.html"><a href="descriptives.html#dealing-with-missing-data"><i class="fa fa-check"></i><b>5.5.1</b> Dealing with Missing Data</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="descriptives.html"><a href="descriptives.html#descriptives-for-datasets"><i class="fa fa-check"></i><b>5.6</b> Descriptives for Datasets</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="descriptives.html"><a href="descriptives.html#descriptives-for-groups"><i class="fa fa-check"></i><b>5.6.1</b> Descriptives for Groups</a></li>
<li class="chapter" data-level="5.6.2" data-path="descriptives.html"><a href="descriptives.html#counts-by-group"><i class="fa fa-check"></i><b>5.6.2</b> Counts by Group</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>6</b> Distributions</a>
<ul>
<li class="chapter" data-level="6.0.1" data-path="distributions.html"><a href="distributions.html#uniform-distribution"><i class="fa fa-check"></i><b>6.0.1</b> Uniform Distribution</a></li>
<li class="chapter" data-level="6.0.2" data-path="distributions.html"><a href="distributions.html#bimodal-distribution"><i class="fa fa-check"></i><b>6.0.2</b> Bimodal Distribution</a></li>
<li class="chapter" data-level="6.0.3" data-path="distributions.html"><a href="distributions.html#normal-distribution"><i class="fa fa-check"></i><b>6.0.3</b> Normal Distribution</a></li>
<li class="chapter" data-level="6.0.4" data-path="distributions.html"><a href="distributions.html#standard-normal-distribution"><i class="fa fa-check"></i><b>6.0.4</b> Standard Normal Distribution</a></li>
<li class="chapter" data-level="6.0.5" data-path="distributions.html"><a href="distributions.html#skewness-and-kurtosis"><i class="fa fa-check"></i><b>6.0.5</b> Skewness and Kurtosis</a></li>
<li class="chapter" data-level="6.1" data-path="distributions.html"><a href="distributions.html#z-scores"><i class="fa fa-check"></i><b>6.1</b> Z-scores</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="distributions.html"><a href="distributions.html#z-scores-in-samples."><i class="fa fa-check"></i><b>6.1.1</b> z-scores in samples.</a></li>
<li class="chapter" data-level="6.1.2" data-path="distributions.html"><a href="distributions.html#using-z-scores-to-determine-probabilities"><i class="fa fa-check"></i><b>6.1.2</b> Using z-scores to determine probabilities</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="distributions.html"><a href="distributions.html#what-is-a-sampling-distribution"><i class="fa fa-check"></i><b>6.2</b> What is a Sampling Distribution ?</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="distributions.html"><a href="distributions.html#sample-size-and-the-sampling-distribution"><i class="fa fa-check"></i><b>6.2.1</b> Sample Size and the Sampling Distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="distributions.html"><a href="distributions.html#central-limit-theorem"><i class="fa fa-check"></i><b>6.3</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="6.4" data-path="distributions.html"><a href="distributions.html#sampling-distribution-problems"><i class="fa fa-check"></i><b>6.4</b> Sampling distribution problems</a></li>
<li class="chapter" data-level="6.5" data-path="distributions.html"><a href="distributions.html#the-t-distribution"><i class="fa fa-check"></i><b>6.5</b> The t-distribution</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="confidence-intervals.html"><a href="confidence-intervals.html"><i class="fa fa-check"></i><b>7</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="7.1" data-path="confidence-intervals.html"><a href="confidence-intervals.html#sample-means-as-estimates."><i class="fa fa-check"></i><b>7.1</b> Sample means as estimates.</a></li>
<li class="chapter" data-level="7.2" data-path="confidence-intervals.html"><a href="confidence-intervals.html#calculating-a-confidence-interval-with-z-distribution"><i class="fa fa-check"></i><b>7.2</b> Calculating a confidence interval with z-distribution</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="confidence-intervals.html"><a href="confidence-intervals.html#other-confidence-intervals-ranges"><i class="fa fa-check"></i><b>7.2.1</b> Other Confidence Intervals ranges</a></li>
<li class="chapter" data-level="7.2.2" data-path="confidence-intervals.html"><a href="confidence-intervals.html#confidence-intervals-and-sample-size"><i class="fa fa-check"></i><b>7.2.2</b> Confidence Intervals and Sample Size</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="confidence-intervals.html"><a href="confidence-intervals.html#confidence-intervals-with-t-distribution"><i class="fa fa-check"></i><b>7.3</b> Confidence Intervals with t-distribution</a></li>
<li class="chapter" data-level="7.4" data-path="confidence-intervals.html"><a href="confidence-intervals.html#calculating-a-t-distribution-confidence-interval"><i class="fa fa-check"></i><b>7.4</b> Calculating a t-distribution Confidence Interval</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="confidence-intervals.html"><a href="confidence-intervals.html#t-distribution-cis-and-sample-size."><i class="fa fa-check"></i><b>7.4.1</b> t-distribution CIs and sample size.</a></li>
<li class="chapter" data-level="7.4.2" data-path="confidence-intervals.html"><a href="confidence-intervals.html#other-confidence-intervals-ranges-for-t-distribution"><i class="fa fa-check"></i><b>7.4.2</b> Other Confidence Intervals ranges for t-distribution</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="confidence-intervals.html"><a href="confidence-intervals.html#comparing-cis-using-the-z--and-t-distributions"><i class="fa fa-check"></i><b>7.5</b> Comparing CIs using the z- and t-distributions</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>8</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="8.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#two-tailed-and-one-tailed-tests"><i class="fa fa-check"></i><b>8.1</b> Two-tailed and One-tailed tests</a></li>
<li class="chapter" data-level="8.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#examples-of-1--and-2-tailed-tests"><i class="fa fa-check"></i><b>8.2</b> Examples of 1- and 2-tailed tests</a></li>
<li class="chapter" data-level="8.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#significance-levels-and-p-values"><i class="fa fa-check"></i><b>8.3</b> Significance Levels and p-values</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html"><i class="fa fa-check"></i><b>9</b> One Sample Inferential Statistics</a>
<ul>
<li class="chapter" data-level="9.1" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#one-sample-z-tests"><i class="fa fa-check"></i><b>9.1</b> One-sample z-tests</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#sampling-distribution-recap"><i class="fa fa-check"></i><b>9.1.1</b> Sampling Distribution Recap</a></li>
<li class="chapter" data-level="9.1.2" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#calculating-p-values-for-z-test"><i class="fa fa-check"></i><b>9.1.2</b> Calculating p-values for z-test</a></li>
<li class="chapter" data-level="9.1.3" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#using-critical-values"><i class="fa fa-check"></i><b>9.1.3</b> Using critical values</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#one-sample-t-tests"><i class="fa fa-check"></i><b>9.2</b> One-sample t-tests</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#critical-values-for-the-one-sample-t-test"><i class="fa fa-check"></i><b>9.2.1</b> Critical values for the one-sample t-test</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#conducting-one-sample-t-tests-in-r"><i class="fa fa-check"></i><b>9.3</b> Conducting one-sample t-tests in R</a></li>
<li class="chapter" data-level="9.4" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#assumptions-of-the-one-sample-t-test"><i class="fa fa-check"></i><b>9.4</b> Assumptions of the one-sample t-test</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html"><i class="fa fa-check"></i><b>10</b> Two Sample Inferential Statistics</a>
<ul>
<li class="chapter" data-level="10.1" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#independent-samples-t-test"><i class="fa fa-check"></i><b>10.1</b> Independent Samples t-test</a></li>
<li class="chapter" data-level="10.2" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#sampling-distribution-of-the-difference-in-sample-means"><i class="fa fa-check"></i><b>10.2</b> Sampling Distribution of the Difference in Sample Means</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#visualizing-the-sampling-distribution"><i class="fa fa-check"></i><b>10.2.1</b> Visualizing the Sampling Distribution</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#pooled-standard-deviation"><i class="fa fa-check"></i><b>10.3</b> Pooled Standard Deviation</a></li>
<li class="chapter" data-level="10.4" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#theory-behind-students-t-test"><i class="fa fa-check"></i><b>10.4</b> Theory behind Student’s t-test</a></li>
<li class="chapter" data-level="10.5" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#confidence-interval-for-difference-in-means"><i class="fa fa-check"></i><b>10.5</b> Confidence Interval for Difference in Means</a></li>
<li class="chapter" data-level="10.6" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#conducting-the-student-t-test-in-r"><i class="fa fa-check"></i><b>10.6</b> Conducting the Student t-test in R</a></li>
<li class="chapter" data-level="10.7" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#assumptions-of-the-independent-t-test"><i class="fa fa-check"></i><b>10.7</b> Assumptions of the Independent t-test</a></li>
<li class="chapter" data-level="10.8" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#welchs-t-test"><i class="fa fa-check"></i><b>10.8</b> Welch’s t-test</a></li>
<li class="chapter" data-level="10.9" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#effect-size-for-independent-two-sample-t-tests"><i class="fa fa-check"></i><b>10.9</b> Effect Size for Independent two sample t-tests:</a></li>
<li class="chapter" data-level="10.10" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#paired-t-tests"><i class="fa fa-check"></i><b>10.10</b> Paired t-tests</a>
<ul>
<li class="chapter" data-level="10.10.1" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#the-paired-t-test-is-a-one-sample-t-test"><i class="fa fa-check"></i><b>10.10.1</b> The paired t-test is a one-sample t-test</a></li>
<li class="chapter" data-level="10.10.2" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#one-tailed-paired-t-tests"><i class="fa fa-check"></i><b>10.10.2</b> One-tailed paired t-tests</a></li>
<li class="chapter" data-level="10.10.3" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#calculating-effect-sizes"><i class="fa fa-check"></i><b>10.10.3</b> Calculating effect sizes</a></li>
</ul></li>
<li class="chapter" data-level="10.11" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#non-parametric-alternatives-for-independent-t-tests"><i class="fa fa-check"></i><b>10.11</b> Non-parametric Alternatives for Independent t-tests</a></li>
<li class="chapter" data-level="10.12" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#non-parametric-alternatives-to-the-two-sample-t-tests"><i class="fa fa-check"></i><b>10.12</b> Non-parametric Alternatives to the Two Sample t-tests</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="correlation.html"><a href="correlation.html"><i class="fa fa-check"></i><b>11</b> Correlation</a>
<ul>
<li class="chapter" data-level="11.1" data-path="correlation.html"><a href="correlation.html#pearson-correlation"><i class="fa fa-check"></i><b>11.1</b> Pearson Correlation</a></li>
<li class="chapter" data-level="11.2" data-path="correlation.html"><a href="correlation.html#cross-products"><i class="fa fa-check"></i><b>11.2</b> Cross-products</a></li>
<li class="chapter" data-level="11.3" data-path="correlation.html"><a href="correlation.html#conducting-a-pearson-correlation-test"><i class="fa fa-check"></i><b>11.3</b> Conducting a Pearson Correlation Test</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="correlation.html"><a href="correlation.html#significance-testing-a-pearson-correlation"><i class="fa fa-check"></i><b>11.3.1</b> Significance Testing a Pearson Correlation</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="correlation.html"><a href="correlation.html#assumptions-of-pearsons-correlation"><i class="fa fa-check"></i><b>11.4</b> Assumptions of Pearson’s Correlation</a></li>
<li class="chapter" data-level="11.5" data-path="correlation.html"><a href="correlation.html#confidence-intervals-for-r"><i class="fa fa-check"></i><b>11.5</b> Confidence Intervals for r</a></li>
<li class="chapter" data-level="11.6" data-path="correlation.html"><a href="correlation.html#partial-correlations"><i class="fa fa-check"></i><b>11.6</b> Partial Correlations</a></li>
<li class="chapter" data-level="11.7" data-path="correlation.html"><a href="correlation.html#non-parametric-correlations"><i class="fa fa-check"></i><b>11.7</b> Non-parametric Correlations</a></li>
<li class="chapter" data-level="11.8" data-path="correlation.html"><a href="correlation.html#point-biserial-correlation"><i class="fa fa-check"></i><b>11.8</b> Point-Biserial Correlation</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>12</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="linear-regression.html"><a href="linear-regression.html#introduction-to-linear-regression"><i class="fa fa-check"></i><b>12.1</b> Introduction to Linear Regression</a></li>
<li class="chapter" data-level="12.2" data-path="linear-regression.html"><a href="linear-regression.html#a-and-b"><i class="fa fa-check"></i><b>12.2</b> a and b</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="linear-regression.html"><a href="linear-regression.html#how-to-calculate-a-and-b-in-r"><i class="fa fa-check"></i><b>12.2.1</b> How to calculate a and b in R</a></li>
<li class="chapter" data-level="12.2.2" data-path="linear-regression.html"><a href="linear-regression.html#how-to-calculate-a-and-b-by-hand"><i class="fa fa-check"></i><b>12.2.2</b> How to calculate a and b ‘by hand’</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="linear-regression.html"><a href="linear-regression.html#residuals"><i class="fa fa-check"></i><b>12.3</b> Residuals</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="linear-regression.html"><a href="linear-regression.html#how-to-calculate-the-residuals"><i class="fa fa-check"></i><b>12.3.1</b> How to calculate the residuals</a></li>
<li class="chapter" data-level="12.3.2" data-path="linear-regression.html"><a href="linear-regression.html#visualizing-the-residuals"><i class="fa fa-check"></i><b>12.3.2</b> Visualizing the Residuals</a></li>
<li class="chapter" data-level="12.3.3" data-path="linear-regression.html"><a href="linear-regression.html#comparing-our-trendline-to-other-trendlines"><i class="fa fa-check"></i><b>12.3.3</b> Comparing our trendline to other trendlines</a></li>
<li class="chapter" data-level="12.3.4" data-path="linear-regression.html"><a href="linear-regression.html#coefficient-of-determination-r2"><i class="fa fa-check"></i><b>12.3.4</b> Coefficient of Determination R2</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="linear-regression.html"><a href="linear-regression.html#standard-error-of-the-estimate"><i class="fa fa-check"></i><b>12.4</b> Standard Error of the Estimate</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="linear-regression.html"><a href="linear-regression.html#what-to-do-with-the-standard-error-of-the-estimate"><i class="fa fa-check"></i><b>12.4.1</b> What to do with the Standard Error of the Estimate ?</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="linear-regression.html"><a href="linear-regression.html#goodness-of-fit-test---f-ratio"><i class="fa fa-check"></i><b>12.5</b> Goodness of Fit Test - F-ratio</a></li>
<li class="chapter" data-level="12.6" data-path="linear-regression.html"><a href="linear-regression.html#assumptions-of-linear-regression"><i class="fa fa-check"></i><b>12.6</b> Assumptions of Linear Regression</a>
<ul>
<li class="chapter" data-level="12.6.1" data-path="linear-regression.html"><a href="linear-regression.html#normality-of-residuals"><i class="fa fa-check"></i><b>12.6.1</b> Normality of Residuals</a></li>
<li class="chapter" data-level="12.6.2" data-path="linear-regression.html"><a href="linear-regression.html#linearity"><i class="fa fa-check"></i><b>12.6.2</b> 2. Linearity —</a></li>
<li class="chapter" data-level="12.6.3" data-path="linear-regression.html"><a href="linear-regression.html#homogeneity-of-variance-homoscedasticity"><i class="fa fa-check"></i><b>12.6.3</b> 3. Homogeneity of Variance / Homoscedasticity</a></li>
<li class="chapter" data-level="12.6.4" data-path="linear-regression.html"><a href="linear-regression.html#no-colinearity"><i class="fa fa-check"></i><b>12.6.4</b> No Colinearity</a></li>
<li class="chapter" data-level="12.6.5" data-path="linear-regression.html"><a href="linear-regression.html#unusual-datapoints"><i class="fa fa-check"></i><b>12.6.5</b> Unusual Datapoints</a></li>
</ul></li>
<li class="chapter" data-level="12.7" data-path="linear-regression.html"><a href="linear-regression.html#examining-individual-predictor-estimates"><i class="fa fa-check"></i><b>12.7</b> Examining individual predictor estimates</a>
<ul>
<li class="chapter" data-level="12.7.1" data-path="linear-regression.html"><a href="linear-regression.html#confidence-interval-of-b."><i class="fa fa-check"></i><b>12.7.1</b> 95% confidence interval of ‘b’.</a></li>
<li class="chapter" data-level="12.7.2" data-path="linear-regression.html"><a href="linear-regression.html#standard-error-of-b"><i class="fa fa-check"></i><b>12.7.2</b> Standard Error of b</a></li>
<li class="chapter" data-level="12.7.3" data-path="linear-regression.html"><a href="linear-regression.html#calculating-95-confidence-interval-of-b-by-hand"><i class="fa fa-check"></i><b>12.7.3</b> Calculating 95% confidence interval of ‘b’ by hand</a></li>
<li class="chapter" data-level="12.7.4" data-path="linear-regression.html"><a href="linear-regression.html#signifcance-testing-b"><i class="fa fa-check"></i><b>12.7.4</b> Signifcance Testing b</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="permutation-testing.html"><a href="permutation-testing.html"><i class="fa fa-check"></i><b>13</b> Permutation Testing</a>
<ul>
<li class="chapter" data-level="13.1" data-path="permutation-testing.html"><a href="permutation-testing.html#t-test-permutation"><i class="fa fa-check"></i><b>13.1</b> t-test Permutation</a></li>
<li class="chapter" data-level="13.2" data-path="permutation-testing.html"><a href="permutation-testing.html#correlation-coefficient-permutation-tests"><i class="fa fa-check"></i><b>13.2</b> Correlation Coefficient Permutation Tests</a></li>
<li class="chapter" data-level="13.3" data-path="permutation-testing.html"><a href="permutation-testing.html#permutation-test-for-a-paired-t-test"><i class="fa fa-check"></i><b>13.3</b> Permutation test for a Paired t-test</a></li>
<li class="chapter" data-level="13.4" data-path="permutation-testing.html"><a href="permutation-testing.html#permutation-tests-in-packages"><i class="fa fa-check"></i><b>13.4</b> Permutation tests in Packages</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">PSY317L &amp; PSY120R Textbook</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression" class="section level1" number="12">
<h1><span class="header-section-number">Chapter 12</span> Linear Regression</h1>
<p>With linear regression, we are attempting to further our understanding of the relationship between two continuous variables. In particular, we try to predict the values of the outcome variable based on the values of the predictor variable. In simple linear regression we only include one predictor variable. That is the type of regression that we will discuss in this chapter.</p>
<div id="introduction-to-linear-regression" class="section level2" number="12.1">
<h2><span class="header-section-number">12.1</span> Introduction to Linear Regression</h2>
<p>The regression method that we are going to start with is called <em>Ordinary Least Squares Regression</em>. Hopefully the reason why it’s called “least squares” will be come obvious, although we’re not too sure why it’s called “ordinary”.</p>
<p>Let’s illustrate the question at hand with some data:</p>
<div class="sourceCode" id="cb1587"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1587-1"><a href="linear-regression.html#cb1587-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1587-2"><a href="linear-regression.html#cb1587-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1587-3"><a href="linear-regression.html#cb1587-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1587-4"><a href="linear-regression.html#cb1587-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Import Data</span></span>
<span id="cb1587-5"><a href="linear-regression.html#cb1587-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1587-6"><a href="linear-regression.html#cb1587-6" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;data/parenthood.csv&quot;</span>)</span>
<span id="cb1587-7"><a href="linear-regression.html#cb1587-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1587-8"><a href="linear-regression.html#cb1587-8" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(df)</span></code></pre></div>
<pre><code>## [1] 100</code></pre>
<div class="sourceCode" id="cb1589"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1589-1"><a href="linear-regression.html#cb1589-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(df)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 4
##   dan.sleep baby.sleep dan.grump   day
##       &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;
## 1      7.59      10.2         56     1
## 2      7.91      11.7         60     2
## 3      5.14       7.92        82     3
## 4      7.71       9.61        55     4
## 5      6.68       9.75        67     5
## 6      5.99       5.04        72     6</code></pre>
<p>As you can see, what we have are four columns of data. The first column shows the amount of sleep that Dan got in an evening. The second column relates to the amount of sleep a baby got. The third column is a rating of Dan’s grumpiness. The last column is a day identifier. Each row represents a different day.</p>
<p>Say we’re interested in seeing whether there was an association between Dan’s sleep and her grumpiness. We could examine this using a scatterplot:</p>
<div class="sourceCode" id="cb1591"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1591-1"><a href="linear-regression.html#cb1591-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Scatterplot</span></span>
<span id="cb1591-2"><a href="linear-regression.html#cb1591-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> dan.sleep, <span class="at">y =</span> dan.grump)) <span class="sc">+</span> </span>
<span id="cb1591-3"><a href="linear-regression.html#cb1591-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb1591-4"><a href="linear-regression.html#cb1591-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_smooth</span>(<span class="at">method=</span><span class="st">&#39;lm&#39;</span>, <span class="at">se=</span>F) </span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-644-1.png" width="672" /></p>
<p>It looks like these variables are clearly associated, with higher levels of sleep being related to lower levels of grumpiness.</p>
<p>To get a measure of how big this relationship is, we could run a correlation test.</p>
<div class="sourceCode" id="cb1592"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1592-1"><a href="linear-regression.html#cb1592-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor.test</span>(df<span class="sc">$</span>dan.sleep, df<span class="sc">$</span>dan.grump)</span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  df$dan.sleep and df$dan.grump
## t = -20.854, df = 98, p-value &lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  -0.9340614 -0.8594714
## sample estimates:
##       cor 
## -0.903384</code></pre>
<p>This shows that the variables are highly correlated with r=-0.90. It’s also a highly significant relationship.</p>
<p>Using <code>stat_smooth()</code> we also applied a best fitting trendline through the data. This line was actually calculated to be in the position that it is in using regression.</p>
<p>The line also has the equation:</p>
<p><span class="math inline">\(y&#39; = a + xb\)</span></p>
<p>You may also see this written in other ways such as:</p>
<p><span class="math inline">\(y&#39; = \beta_{0} + x\beta_{1}\)</span></p>
<p>but we’ll stick with</p>
<p><span class="math inline">\(y&#39; = a + xb\)</span></p>
</div>
<div id="a-and-b" class="section level2" number="12.2">
<h2><span class="header-section-number">12.2</span> a and b</h2>
<p>In the equation for a regression line,</p>
<p><span class="math inline">\(y&#39; = a + xb\)</span></p>
<p><span class="math inline">\(y&#39;\)</span> is equal to the predicted value of <span class="math inline">\(y\)</span>. Essentially if you go from any value of <span class="math inline">\(x\)</span> and go up to the trendline, and then across to the y-axis, that is your predicted value of y. The trendline represents the predicted values of <span class="math inline">\(y\)</span> for all values of <span class="math inline">\(x\)</span>.</p>
<p>In regression terms, we often refer to <span class="math inline">\(x\)</span> as the <strong>predictor</strong> variable and <span class="math inline">\(y\)</span> as the <strong>outcome</strong> variable.</p>
<p>The value of <span class="math inline">\(b\)</span> in the equation represents the <strong>slope</strong> of the regression line. If it’s a positive number then it means that the regression line is going upwards (akin to a positive correlation, where <span class="math inline">\(y\)</span> increases as <span class="math inline">\(x\)</span> increases.) If it’s a negative number, then it means that the regression line is going downwards (akin to a negative correlation, where <span class="math inline">\(y\)</span> decreases as <span class="math inline">\(x\)</span> increases). The value of <span class="math inline">\(b\)</span> can also be considered to be how much <span class="math inline">\(y\)</span> changes for every 1 unit increase in <span class="math inline">\(x\)</span>. So a value of <span class="math inline">\(b = -1.4\)</span> would indicate that as <span class="math inline">\(x\)</span> increases by 1, <span class="math inline">\(y\)</span> will decrease by 1.4.</p>
<p>The value of <span class="math inline">\(a\)</span> represents the <strong>y-intercept</strong>. This is the value of <code>y'</code> that you would get if you extended the regression line to cross at <span class="math inline">\(x=0\)</span>.</p>
<p>We can illustrate that in the graph below. We extended the trendline (dotted black line) from its ends until it passes through where <span class="math inline">\(x=0\)</span> (the dotted red line). Where it crosses this point is at <span class="math inline">\(y=125.96\)</span> which is depicted by the orange dotted line. This makes the y-intercept for this trendline equal to 125.96.</p>
<div class="sourceCode" id="cb1594"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1594-1"><a href="linear-regression.html#cb1594-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Scatterplot</span></span>
<span id="cb1594-2"><a href="linear-regression.html#cb1594-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> dan.sleep, <span class="at">y =</span> dan.grump)) <span class="sc">+</span> </span>
<span id="cb1594-3"><a href="linear-regression.html#cb1594-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb1594-4"><a href="linear-regression.html#cb1594-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_smooth</span>(<span class="at">method=</span><span class="st">&#39;lm&#39;</span>, <span class="at">se=</span>F) <span class="sc">+</span></span>
<span id="cb1594-5"><a href="linear-regression.html#cb1594-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">10</span>)<span class="sc">+</span> </span>
<span id="cb1594-6"><a href="linear-regression.html#cb1594-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">40</span>,<span class="dv">130</span>)<span class="sc">+</span></span>
<span id="cb1594-7"><a href="linear-regression.html#cb1594-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span><span class="fl">125.956</span> , <span class="at">slope =</span> <span class="sc">-</span><span class="fl">8.937</span>, <span class="at">lty=</span><span class="dv">2</span>)<span class="sc">+</span></span>
<span id="cb1594-8"><a href="linear-regression.html#cb1594-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span><span class="dv">0</span>, <span class="at">color=</span><span class="st">&#39;red&#39;</span>, <span class="at">lty=</span><span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb1594-9"><a href="linear-regression.html#cb1594-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">x=</span><span class="sc">-</span><span class="fl">1.6</span>,<span class="at">xend=</span><span class="dv">0</span>,<span class="at">y=</span><span class="fl">125.96</span>,<span class="at">yend=</span><span class="fl">125.96</span>, <span class="at">lty=</span><span class="dv">2</span>, <span class="at">color=</span><span class="st">&#39;orange&#39;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-646-1.png" width="672" /></p>
<p>In reality, we do not ever extend the regression lines like this. In fact, a regression line by definition only fits the range of datapoints along the x-axis that we have - it should not extend beyond those points. This is more a theoretical construct to help us understand where the line is on the graph.</p>
<p>To think a bit more about <code>a</code> and <code>b</code> let’s compare these regression lines and their respective equations.</p>
<p><img src="img/ab.png" /></p>
<p>In the top left image, you can see three parallel regression lines. These all have the same slope value (b = 0.704). How they differ is in their y-intercept. The different values of ‘a’ indicate that they are at different heights. In the top right, we’ve extended the regression lines in red back to where the x-axis is 0. Where the lines cross here (x=0) is the value of each y-intercept i.e. ‘a’.</p>
<p>In the bottom right image, you can see that all the plots have the same y-intercept value (8.32). How they differ is in their slope b. Two are positive values of b, with the larger value having a steeper slope. The negative value of b (-0.622) has a slope going downwards. In the bottom right we extend these trendlines back to where x=0, and you can see that all have the same y-intercept (x=0, y=8.32).</p>
<p><br></p>
<div id="how-to-calculate-a-and-b-in-r" class="section level3" number="12.2.1">
<h3><span class="header-section-number">12.2.1</span> How to calculate a and b in R</h3>
<p>To run a regression in R, we use the <code>lm()</code> function. It looks like the following:</p>
<div class="sourceCode" id="cb1595"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1595-1"><a href="linear-regression.html#cb1595-1" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(dan.grump <span class="sc">~</span> dan.sleep, <span class="at">data=</span>df)  <span class="co"># build regression model</span></span></code></pre></div>
<p>The first thing after the bracket is the outcome variable which is <code>dan.grump</code>. Then a tilde (~) and then the predictor variable which is <code>dan.sleep</code>. Finally, we tell <code>lm</code> what dataset we’re using. The best way to read that statement is: <code>"dan.grump 'is predicted by' dan.sleep"</code>.</p>
<p>We’re also saving the regression model as <code>mod1</code> because there’s tons of information that comes along with the regression.</p>
<p>To have a look at what <code>a</code> and <code>b</code> are, we can just look at our saved object <code>mod1</code>.</p>
<div class="sourceCode" id="cb1596"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1596-1"><a href="linear-regression.html#cb1596-1" aria-hidden="true" tabindex="-1"></a>mod1  </span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = dan.grump ~ dan.sleep, data = df)
## 
## Coefficients:
## (Intercept)    dan.sleep  
##     125.956       -8.937</code></pre>
<p>Here the value underneath “Intercept” (125.956) refers to the y-intercept <code>a</code>. The value underneath <code>dan.sleep</code>, the predictor variable, is our value of <code>b</code> the slope.</p>
<p>So this would mean that the regression line for these data would be:</p>
<p><span class="math inline">\(y&#39; = 125.956 + -8.937b\)</span></p>
<p>We can also get these values directly by running the following code:</p>
<div class="sourceCode" id="cb1598"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1598-1"><a href="linear-regression.html#cb1598-1" aria-hidden="true" tabindex="-1"></a>mod1<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>## (Intercept)   dan.sleep 
##  125.956292   -8.936756</code></pre>
<p>Now, we should also mention one other thing about these values. These regression coefficients are ‘estimates’. We have one sample of 100 subjects from which we estimated the true population values of ‘a’ and ‘b’. The true population values of ‘a’ and ‘b’ are parameters.</p>
<p><br></p>
</div>
<div id="how-to-calculate-a-and-b-by-hand" class="section level3" number="12.2.2">
<h3><span class="header-section-number">12.2.2</span> How to calculate a and b ‘by hand’</h3>
<p>To think a bit more about these ‘a’ and ‘b’ values, we could look at how these values are actually calculated.</p>
<p>First, we calculate ‘b’. The formula for this requires knowing the sample standard deviation of the X and Y variables, as well as their Pearson correlation.</p>
<p><span class="math inline">\(\Large b = r\frac{s_{Y}}{s_{X}}\)</span></p>
<p>So for our example, we’d calculate ‘b’ like this:</p>
<div class="sourceCode" id="cb1600"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1600-1"><a href="linear-regression.html#cb1600-1" aria-hidden="true" tabindex="-1"></a>r <span class="ot">&lt;-</span>  <span class="fu">cor</span>(df<span class="sc">$</span>dan.sleep, df<span class="sc">$</span>dan.grump)</span>
<span id="cb1600-2"><a href="linear-regression.html#cb1600-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1600-3"><a href="linear-regression.html#cb1600-3" aria-hidden="true" tabindex="-1"></a>sy <span class="ot">&lt;-</span> <span class="fu">sd</span>(df<span class="sc">$</span>dan.grump)</span>
<span id="cb1600-4"><a href="linear-regression.html#cb1600-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1600-5"><a href="linear-regression.html#cb1600-5" aria-hidden="true" tabindex="-1"></a>sx <span class="ot">&lt;-</span> <span class="fu">sd</span>(df<span class="sc">$</span>dan.sleep)</span>
<span id="cb1600-6"><a href="linear-regression.html#cb1600-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1600-7"><a href="linear-regression.html#cb1600-7" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> r <span class="sc">*</span> (sy<span class="sc">/</span>sx)</span>
<span id="cb1600-8"><a href="linear-regression.html#cb1600-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1600-9"><a href="linear-regression.html#cb1600-9" aria-hidden="true" tabindex="-1"></a>b</span></code></pre></div>
<pre><code>## [1] -8.936756</code></pre>
<p>Next, we can calculate ‘a’ using the formula: <span class="math inline">\(a = \overline{Y} - b\overline{X}\)</span> . This requires us to know the sample mean of X and Y.</p>
<p>So for our example, we calculate ‘a’ like this:</p>
<div class="sourceCode" id="cb1602"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1602-1"><a href="linear-regression.html#cb1602-1" aria-hidden="true" tabindex="-1"></a>my <span class="ot">&lt;-</span> <span class="fu">mean</span>(df<span class="sc">$</span>dan.grump)</span>
<span id="cb1602-2"><a href="linear-regression.html#cb1602-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1602-3"><a href="linear-regression.html#cb1602-3" aria-hidden="true" tabindex="-1"></a>mx <span class="ot">&lt;-</span> <span class="fu">mean</span>(df<span class="sc">$</span>dan.sleep)</span>
<span id="cb1602-4"><a href="linear-regression.html#cb1602-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1602-5"><a href="linear-regression.html#cb1602-5" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> my <span class="sc">-</span> (b <span class="sc">*</span> mx)</span>
<span id="cb1602-6"><a href="linear-regression.html#cb1602-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1602-7"><a href="linear-regression.html#cb1602-7" aria-hidden="true" tabindex="-1"></a>a</span></code></pre></div>
<pre><code>## [1] 125.9563</code></pre>
<p>Therefore, we have an equation for our trendline which is <code>y' = 125.96 + -8.94x</code>, which means that for every 1 unit increase of sleep, Dan’s grumpiness decreases by 8.94.</p>
</div>
</div>
<div id="residuals" class="section level2" number="12.3">
<h2><span class="header-section-number">12.3</span> Residuals</h2>
<p>Once we fit a trendline to the data it’s clear that not all the datapoints fit to the line. In fact, almost none of the datapoints are on the line! This is because the trendline is our prediction of the value of y based on the value of x. Each datapoint actually is either larger or smaller in terms of y than the regression line. Sometimes it’s a bit bigger or smaller, other times it might be a lot bigger or smaller. Occasionally, the predicted value of y might be on the regression line.</p>
<p>Because our trendline isn’t a perfect fit for the data, the formula for the regression line could technically be written as:</p>
<p><span class="math inline">\(y&#39; = a + bx + \epsilon\)</span></p>
<p><span class="math inline">\(\epsilon\)</span> refers to the error, or how far each datapoint is from the predicted value.</p>
<p>In fact, the difference of each data point from the predicted value is called a <strong>raw residual</strong> or <strong>ordinary residual</strong>. We calculate the size of the residual for each datapoint by the following formula:</p>
<p><span class="math inline">\(residual = y - y&#39;\)</span></p>
<p>This essentially is the difference of each data point from the predicted value.</p>
<div id="how-to-calculate-the-residuals" class="section level3" number="12.3.1">
<h3><span class="header-section-number">12.3.1</span> How to calculate the residuals</h3>
<p>Using our regression line equation of <code>y' = 125.96 + -8.94x</code>, we can manually calculate the raw residuals.</p>
<p>Firstly, we calculate the predicted values of y <span class="math inline">\(y&#39;\)</span> for each value of <span class="math inline">\(x\)</span>. We can put these back into our original dataframe.</p>
<div class="sourceCode" id="cb1604"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1604-1"><a href="linear-regression.html#cb1604-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> df<span class="sc">$</span>dan.sleep  <span class="co"># the predictor</span></span>
<span id="cb1604-2"><a href="linear-regression.html#cb1604-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> df<span class="sc">$</span>dan.grump  <span class="co"># the outcome</span></span>
<span id="cb1604-3"><a href="linear-regression.html#cb1604-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1604-4"><a href="linear-regression.html#cb1604-4" aria-hidden="true" tabindex="-1"></a>Y.pred <span class="ot">&lt;-</span>  <span class="fl">125.96</span>   <span class="sc">+</span>   (<span class="sc">-</span><span class="fl">8.94</span> <span class="sc">*</span> X)</span>
<span id="cb1604-5"><a href="linear-regression.html#cb1604-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1604-6"><a href="linear-regression.html#cb1604-6" aria-hidden="true" tabindex="-1"></a>Y.pred</span></code></pre></div>
<pre><code>##   [1] 58.1054 55.2446 80.0084 57.0326 66.2408 72.4094 52.7414 61.6814 59.8040 67.1348 67.9394 69.9062
##  [13] 72.7670 66.5090 68.6546 69.3698 69.6380 50.2382 61.5026 58.6418 54.4400 60.2510 64.6316 55.6916
##  [25] 82.5116 73.4822 50.8640 64.0058 61.5026 63.4694 52.9202 55.7810 69.9062 48.5396 81.4388 70.6214
##  [37] 68.6546 82.6904 63.1118 57.4796 58.8206 55.1552 53.3672 59.1782 54.5294 77.3264 53.0096 57.8372
##  [49] 73.4822 45.5000 51.6686 65.9726 59.5358 73.2140 49.7912 72.0518 60.7874 60.5192 64.4528 70.3532
##  [61] 63.9164 63.2906 61.5920 69.6380 48.0032 56.0492 53.1884 60.9662 66.0620 58.4630 59.9828 56.8538
##  [73] 78.3992 55.6916 69.1910 62.3966 77.2370 56.2280 62.2178 51.3110 64.0058 62.7542 48.5396 80.4554
##  [85] 82.0646 63.1118 63.2012 57.3902 53.0990 73.3928 74.8232 66.4196 64.7210 76.1642 79.8296 78.4886
##  [97] 56.4962 77.8628 63.2012 68.2970</code></pre>
<div class="sourceCode" id="cb1606"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1606-1"><a href="linear-regression.html#cb1606-1" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>Y.pred <span class="ot">&lt;-</span> Y.pred</span>
<span id="cb1606-2"><a href="linear-regression.html#cb1606-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1606-3"><a href="linear-regression.html#cb1606-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(df)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 5
##   dan.sleep baby.sleep dan.grump   day Y.pred
##       &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
## 1      7.59      10.2         56     1   58.1
## 2      7.91      11.7         60     2   55.2
## 3      5.14       7.92        82     3   80.0
## 4      7.71       9.61        55     4   57.0
## 5      6.68       9.75        67     5   66.2
## 6      5.99       5.04        72     6   72.4</code></pre>
<p>Next, getting the raw residual is simply a matter of taking each observed value of <span class="math inline">\(y\)</span> and subtracting the predicted value of <span class="math inline">\(y\)</span>.</p>
<div class="sourceCode" id="cb1608"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1608-1"><a href="linear-regression.html#cb1608-1" aria-hidden="true" tabindex="-1"></a><span class="co"># so to get the residual,  y - y&#39;</span></span>
<span id="cb1608-2"><a href="linear-regression.html#cb1608-2" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>dan.grump <span class="sc">-</span> Y.pred</span></code></pre></div>
<pre><code>##   [1]  -2.1054   4.7554   1.9916  -2.0326   0.7592  -0.4094   0.2586  -1.6814   0.1960   3.8652   4.0606
##  [12]  -4.9062   1.2330   0.4910  -2.6546  -0.3698   3.3620   1.7618  -0.5026  -5.6418  -0.4400   2.7490
##  [23]   9.3684   0.3084  -0.5116  -1.4822   8.1360   1.9942  -1.5026   3.5306  -8.9202  -2.7810   6.0938
##  [34]  -7.5396   4.5612 -10.6214  -5.6546   6.3096  -2.1118  -0.4796   0.1794   4.8448  -5.3672  -6.1782
##  [45]  -4.5294  -5.3264   3.9904   2.1628  -3.4822   0.5000   6.3314   2.0274  -1.5358  -2.2140   2.2088
##  [56]   1.9482  -1.7874  -1.5192   2.5472  -3.3532  -2.9164   0.7094  -0.5920  -8.6380   5.9968   5.9508
##  [67]  -1.1884   3.0338  -1.0620   6.5370  -2.9828   2.1462   0.6008  -2.6916  -2.1910  -1.3966   4.7630
##  [78]  11.7720   4.7822   2.6890 -11.0058  -0.7542   1.4604  -0.4554   8.9354  -1.1118   0.7988  -0.3902
##  [89]   0.9010  -1.3928   3.1768  -3.4196  -5.7210  -2.1642  -3.8296   0.5114  -5.4962   4.1372  -8.2012
## [100]   5.7030</code></pre>
<div class="sourceCode" id="cb1610"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1610-1"><a href="linear-regression.html#cb1610-1" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>residuals <span class="ot">&lt;-</span> df<span class="sc">$</span>dan.grump <span class="sc">-</span> Y.pred</span>
<span id="cb1610-2"><a href="linear-regression.html#cb1610-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1610-3"><a href="linear-regression.html#cb1610-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(df)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 6
##   dan.sleep baby.sleep dan.grump   day Y.pred residuals
##       &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;
## 1      7.59      10.2         56     1   58.1    -2.11 
## 2      7.91      11.7         60     2   55.2     4.76 
## 3      5.14       7.92        82     3   80.0     1.99 
## 4      7.71       9.61        55     4   57.0    -2.03 
## 5      6.68       9.75        67     5   66.2     0.759
## 6      5.99       5.04        72     6   72.4    -0.409</code></pre>
</div>
<div id="visualizing-the-residuals" class="section level3" number="12.3.2">
<h3><span class="header-section-number">12.3.2</span> Visualizing the Residuals</h3>
<p>Now we have a raw residual for all 100 datapoints in our data. The following plot is our same scatterplot, but this time we’ve also added a little red line connecting each observed datapoint to the regression line. The size of each of these red lines represents the residuals.</p>
<div class="sourceCode" id="cb1612"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1612-1"><a href="linear-regression.html#cb1612-1" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> dan.sleep, <span class="at">y =</span> dan.grump)) <span class="sc">+</span></span>
<span id="cb1612-2"><a href="linear-regression.html#cb1612-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb1612-3"><a href="linear-regression.html#cb1612-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span>   </span>
<span id="cb1612-4"><a href="linear-regression.html#cb1612-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">xend =</span> dan.sleep, <span class="at">yend =</span> dan.grump<span class="sc">-</span>residuals), <span class="at">alpha =</span> .<span class="dv">5</span>, <span class="at">color=</span><span class="st">&#39;red&#39;</span>) <span class="sc">+</span> </span>
<span id="cb1612-5"><a href="linear-regression.html#cb1612-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb1612-6"><a href="linear-regression.html#cb1612-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;OLSR best fit trendline&quot;</span>)</span>
<span id="cb1612-7"><a href="linear-regression.html#cb1612-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1612-8"><a href="linear-regression.html#cb1612-8" aria-hidden="true" tabindex="-1"></a>p1</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-654-1.png" width="672" /></p>
</div>
<div id="comparing-our-trendline-to-other-trendlines" class="section level3" number="12.3.3">
<h3><span class="header-section-number">12.3.3</span> Comparing our trendline to other trendlines</h3>
<p>An important question to consider is why did we end up with our trendline and not some other trendline? The answer is that ours is the ‘best fit’, but what does that really mean? In short, it means that the best-fit regression line is the one that has the smallest squared residuals. The squared residuals are calculated by squaring every residual and then summing these all up.</p>
<p>Let’s look at this by looking at one possible trendline that we could have used. The one that we’ll choose is a trendline that goes horizontally through the data with a y value that is the mean of Y.</p>
<div class="sourceCode" id="cb1614"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1614-1"><a href="linear-regression.html#cb1614-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(df<span class="sc">$</span>dan.grump)</span></code></pre></div>
<pre><code>## [1] 63.71</code></pre>
<p>So, the mean of the Y variable <code>dan.grump</code> is 63.71. Let’s put a trendline through our data that is horizontal at 63.71. The equation for this line would be:</p>
<p><span class="math inline">\(y&#39; = 63.71 + 0x\)</span></p>
<p>Let’s visualize this:</p>
<div class="sourceCode" id="cb1616"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1616-1"><a href="linear-regression.html#cb1616-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> dan.sleep, <span class="at">y =</span> dan.grump)) <span class="sc">+</span> </span>
<span id="cb1616-2"><a href="linear-regression.html#cb1616-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb1616-3"><a href="linear-regression.html#cb1616-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fu">mean</span>(df<span class="sc">$</span>dan.grump), <span class="at">color=</span><span class="st">&#39;blue&#39;</span>) </span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-656-1.png" width="672" /></p>
<p>It doesn’t look like using the mean value of Y is that good a predictor of each datapoint. We could actually visualize how good.bad it is by calculating the residual of each datapoint from this new trendline. This time each residual is equal to:</p>
<p><span class="math inline">\(residual = y - \overline{y}\)</span></p>
<p>Let’s calculate these residuals, and then graph them on this scatterplot:</p>
<div class="sourceCode" id="cb1617"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1617-1"><a href="linear-regression.html#cb1617-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we can work out what the &#39;residuals&#39; would be for this trendline:</span></span>
<span id="cb1617-2"><a href="linear-regression.html#cb1617-2" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>Ymean <span class="ot">&lt;-</span> <span class="fu">mean</span>(df<span class="sc">$</span>dan.grump)</span>
<span id="cb1617-3"><a href="linear-regression.html#cb1617-3" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>resid_Ymean <span class="ot">&lt;-</span> df<span class="sc">$</span>dan.grump <span class="sc">-</span> df<span class="sc">$</span>Ymean  <span class="co">#residual from Ymean</span></span>
<span id="cb1617-4"><a href="linear-regression.html#cb1617-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(df)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 8
##   dan.sleep baby.sleep dan.grump   day Y.pred residuals Ymean resid_Ymean
##       &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;
## 1      7.59      10.2         56     1   58.1    -2.11   63.7       -7.71
## 2      7.91      11.7         60     2   55.2     4.76   63.7       -3.71
## 3      5.14       7.92        82     3   80.0     1.99   63.7       18.3 
## 4      7.71       9.61        55     4   57.0    -2.03   63.7       -8.71
## 5      6.68       9.75        67     5   66.2     0.759  63.7        3.29
## 6      5.99       5.04        72     6   72.4    -0.409  63.7        8.29</code></pre>
<div class="sourceCode" id="cb1619"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1619-1"><a href="linear-regression.html#cb1619-1" aria-hidden="true" tabindex="-1"></a><span class="do">## visualize this:</span></span>
<span id="cb1619-2"><a href="linear-regression.html#cb1619-2" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> dan.sleep, <span class="at">y =</span> dan.grump)) <span class="sc">+</span></span>
<span id="cb1619-3"><a href="linear-regression.html#cb1619-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb1619-4"><a href="linear-regression.html#cb1619-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fu">mean</span>(df<span class="sc">$</span>dan.grump), <span class="at">color=</span><span class="st">&#39;blue&#39;</span>) <span class="sc">+</span></span>
<span id="cb1619-5"><a href="linear-regression.html#cb1619-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">xend =</span> dan.sleep, <span class="at">yend =</span> dan.grump<span class="sc">-</span>resid_Ymean), <span class="at">alpha =</span> .<span class="dv">5</span>, <span class="at">color=</span><span class="st">&#39;red&#39;</span>) <span class="sc">+</span> </span>
<span id="cb1619-6"><a href="linear-regression.html#cb1619-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># geom_point(aes(y = dan.grump+residuals), shape = 1) +</span></span>
<span id="cb1619-7"><a href="linear-regression.html#cb1619-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb1619-8"><a href="linear-regression.html#cb1619-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Residuals to the mean Y&quot;</span>)</span>
<span id="cb1619-9"><a href="linear-regression.html#cb1619-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1619-10"><a href="linear-regression.html#cb1619-10" aria-hidden="true" tabindex="-1"></a>p2</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-657-1.png" width="672" /></p>
<p>If we compare both graphs scatterplots side by side, it’s pretty clear that our best-fit trendline is doing a much better job of predicting each datapoint’s Y value. The horizontal trendline at the mean of Y looks pretty bad for the majority of datapoints - its residuals are much bigger.</p>
<div class="sourceCode" id="cb1620"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1620-1"><a href="linear-regression.html#cb1620-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb1620-2"><a href="linear-regression.html#cb1620-2" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(p1,p2,<span class="at">nrow=</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-658-1.png" width="672" /></p>
<p>In fact, let’s actually quantify the difference in residuals between these two trendlines.
Because we have both positive and negative residuals, if we just added them together we’d end up with 0. In statistics, one common way to make numbers positive is to square them. As we saw with standard deviation, this also has the advantage of emphasizing large values.</p>
<p>What we do to compare our residuals, is to therefore square them. We call the residuals from our trendline the raw residuals. We call the residuals from the horizontal line the total residuals.</p>
<div class="sourceCode" id="cb1621"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1621-1"><a href="linear-regression.html#cb1621-1" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>residuals2 <span class="ot">&lt;-</span> df<span class="sc">$</span>residuals <span class="sc">^</span> <span class="dv">2</span>               <span class="co"># raw residuals</span></span>
<span id="cb1621-2"><a href="linear-regression.html#cb1621-2" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>resid_Ymean2 <span class="ot">&lt;-</span> df<span class="sc">$</span>resid_Ymean <span class="sc">^</span> <span class="dv">2</span>           <span class="co"># total residuals</span></span>
<span id="cb1621-3"><a href="linear-regression.html#cb1621-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1621-4"><a href="linear-regression.html#cb1621-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(df[<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">6</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">10</span>)]) <span class="co"># just showing the relevant columns</span></span></code></pre></div>
<pre><code>## # A tibble: 6 x 6
##   dan.sleep dan.grump residuals resid_Ymean residuals2 resid_Ymean2
##       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;
## 1      7.59        56    -2.11        -7.71      4.43          59.4
## 2      7.91        60     4.76        -3.71     22.6           13.8
## 3      5.14        82     1.99        18.3       3.97         335. 
## 4      7.71        55    -2.03        -8.71      4.13          75.9
## 5      6.68        67     0.759        3.29      0.576         10.8
## 6      5.99        72    -0.409        8.29      0.168         68.7</code></pre>
<p>You can see the squared raw residual and squared total residual for the first six datapoints. Ideally, we want the raw residuals to be as small a fraction as possible of the total residuals. If we sum up the squared raw residuals and squared total residuals, we can determine this:</p>
<div class="sourceCode" id="cb1623"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1623-1"><a href="linear-regression.html#cb1623-1" aria-hidden="true" tabindex="-1"></a>SS.resid <span class="ot">&lt;-</span> <span class="fu">sum</span>(df<span class="sc">$</span>residuals2)</span>
<span id="cb1623-2"><a href="linear-regression.html#cb1623-2" aria-hidden="true" tabindex="-1"></a>SS.resid  <span class="co">#1838.722</span></span></code></pre></div>
<pre><code>## [1] 1838.75</code></pre>
<div class="sourceCode" id="cb1625"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1625-1"><a href="linear-regression.html#cb1625-1" aria-hidden="true" tabindex="-1"></a>SS.tot <span class="ot">&lt;-</span> <span class="fu">sum</span>(df<span class="sc">$</span>resid_Ymean2)</span>
<span id="cb1625-2"><a href="linear-regression.html#cb1625-2" aria-hidden="true" tabindex="-1"></a>SS.tot    <span class="co">#9998.59</span></span></code></pre></div>
<pre><code>## [1] 9998.59</code></pre>
<p>As you can clearly see, the summed squared residuals for our best fit regression line are much smaller than the summed squared residuals when using the mean of y as the regression line.</p>
</div>
<div id="coefficient-of-determination-r2" class="section level3" number="12.3.4">
<h3><span class="header-section-number">12.3.4</span> Coefficient of Determination R2</h3>
<p>One way to make these summed squares of residuals numbers more interpretable is to convert them to <span class="math inline">\(R^{2}\)</span>.</p>
<p>The logic goes as following. If the trendline is absolutely useless at predicting the y values, then the trendline would have residuals as high as the total residuals. If the trendline is perfect at predicting the y values, then the residual SS total would be 0.</p>
<p>If we look at the sum of the squares of the raw residuals as a ratio of a sum of the squared total residuals then we can work out how well our trendline fits.</p>
<p>We calculate this using the formula,</p>
<p><span class="math inline">\(R^{2} = 1 - \frac{SS_{raw}}{SS_{total}}\)</span></p>
<p>in the following way:</p>
<div class="sourceCode" id="cb1627"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1627-1"><a href="linear-regression.html#cb1627-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span>  (SS.resid<span class="sc">/</span>SS.tot)   <span class="co"># 0.816   (this is R2)</span></span></code></pre></div>
<pre><code>## [1] 0.816099</code></pre>
<p>So for our data, <span class="math inline">\(R^{2} = 0.816\)</span>. This means that our regression model (and trendline) is a very good fit to the data.</p>
<p><span class="math inline">\(R^{2}\)</span> ranges from 0 to 1. If its value was 1, then that would indicate that the best-fit trendline perfectly fits the data with no raw residuals. If its value was 0, then that would mean that the best-fit trendline is no better at fitting the data than the horizontal line at the mean of Y. Values of <span class="math inline">\(R^{2}\)</span> that get closer to 1 indicate that the model is doing a better job at estimating each value of Y. We say that the model has a better ‘fit’ to the data.</p>
<p>There is a way to directly get the value of <span class="math inline">\(R^{2}\)</span> in R. You may remember earlier in this chapter that we ran our linear model using the function <code>lm()</code>. We saved the output of this regression model as the object <code>mod1</code>. You can use <code>summary()</code> to get lots of information about the regression model.</p>
<div class="sourceCode" id="cb1629"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1629-1"><a href="linear-regression.html#cb1629-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)  <span class="co"># the R2 matches</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = dan.grump ~ dan.sleep, data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -11.025  -2.213  -0.399   2.681  11.750 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 125.9563     3.0161   41.76   &lt;2e-16 ***
## dan.sleep    -8.9368     0.4285  -20.85   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.332 on 98 degrees of freedom
## Multiple R-squared:  0.8161, Adjusted R-squared:  0.8142 
## F-statistic: 434.9 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The thing to focus on from this output right now, is the value called <code>Multiple R-squared</code>. This gives us the same value of <span class="math inline">\(R^{2}\)</span> that we calculated by hand: <span class="math inline">\(R^{2}=0.816\)</span>. The other value of <span class="math inline">\(R^{2}\)</span> is called <code>Adjusted R-squared</code>. This is relevant when you are conducting multiple regression with more than one predictor in the model. We only have one predictor in the model (<code>dan.sleep</code>) so we won’t talk more about Adjusted R squared here.</p>
<p>Finally, there is a shortcut way in which we can calculate <span class="math inline">\(R^{2}\)</span>. It is simply to square the Pearson’s correlation coefficient:</p>
<p><span class="math inline">\(R^{2} = r^{2}\)</span>.</p>
<div class="sourceCode" id="cb1631"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1631-1"><a href="linear-regression.html#cb1631-1" aria-hidden="true" tabindex="-1"></a>r <span class="ot">&lt;-</span> <span class="fu">cor</span>(df<span class="sc">$</span>dan.sleep, df<span class="sc">$</span>dan.grump)</span>
<span id="cb1631-2"><a href="linear-regression.html#cb1631-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1631-3"><a href="linear-regression.html#cb1631-3" aria-hidden="true" tabindex="-1"></a>r</span></code></pre></div>
<pre><code>## [1] -0.903384</code></pre>
<div class="sourceCode" id="cb1633"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1633-1"><a href="linear-regression.html#cb1633-1" aria-hidden="true" tabindex="-1"></a>r<span class="sc">^</span><span class="dv">2</span>  <span class="co"># same R2 as above</span></span></code></pre></div>
<pre><code>## [1] 0.8161027</code></pre>
</div>
</div>
<div id="standard-error-of-the-estimate" class="section level2" number="12.4">
<h2><span class="header-section-number">12.4</span> Standard Error of the Estimate</h2>
<p><span class="math inline">\(R^{2}\)</span> is one value that gives us a sense of how well our regression model is doing. Another method to assess ‘model fit’ is to examine the Standard Error of the Estimate. Unfortunately, this value has a number of names. You’ll see it referred to as the Standard Error of the Regression, Residual Standard Error, Regression Standard Error, <span class="math inline">\(S\)</span>, or <span class="math inline">\(\sigma_{est}\)</span>. We prefer to call it the Standard Error of the Estimate or <span class="math inline">\(\sigma_{est}\)</span>.</p>
<p>This is calculated as follows:</p>
<p><span class="math inline">\(\sigma_{est} = \sqrt{\frac{\Sigma (y - y&#39;)^{2}}{n-2}}\)</span></p>
<p>The <span class="math inline">\(\Sigma (y - y&#39;)^{2}\)</span> part of this equation is the Sum of the raw residuals squared that we already calculated when calculating <span class="math inline">\(R^{2}\)</span>.</p>
<p>We can therefore calculate <span class="math inline">\(\sigma_{est}\)</span> quite straightforwardly in R manually.</p>
<div class="sourceCode" id="cb1635"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1635-1"><a href="linear-regression.html#cb1635-1" aria-hidden="true" tabindex="-1"></a>s_est <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(SS.resid <span class="sc">/</span> <span class="dv">98</span>) <span class="co"># n=100</span></span>
<span id="cb1635-2"><a href="linear-regression.html#cb1635-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1635-3"><a href="linear-regression.html#cb1635-3" aria-hidden="true" tabindex="-1"></a>s_est</span></code></pre></div>
<pre><code>## [1] 4.3316</code></pre>
<p>Therefore <span class="math inline">\(\sigma_{est} = 4.332\)</span> for our model.</p>
<p>It’s actually possible to see this value in the output of the summary of the model in R. Here, it’s called the ‘residual standard error’:</p>
<div class="sourceCode" id="cb1637"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1637-1"><a href="linear-regression.html#cb1637-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1) </span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = dan.grump ~ dan.sleep, data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -11.025  -2.213  -0.399   2.681  11.750 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 125.9563     3.0161   41.76   &lt;2e-16 ***
## dan.sleep    -8.9368     0.4285  -20.85   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.332 on 98 degrees of freedom
## Multiple R-squared:  0.8161, Adjusted R-squared:  0.8142 
## F-statistic: 434.9 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<div id="what-to-do-with-the-standard-error-of-the-estimate" class="section level3" number="12.4.1">
<h3><span class="header-section-number">12.4.1</span> What to do with the Standard Error of the Estimate ?</h3>
<p>What we are generally looking for with <span class="math inline">\(\sigma_{est}\)</span> is a number as small as possible. This is because what essentially it is a measure of is an approximate estimate of the average raw residual. The question is, how small is small? This is difficult to answer. One reason is because <span class="math inline">\(\sigma_{est}\)</span> is actually in the original units of the Y-axis (the outcome variable). Therefore it’s not as simple as saying that it should be close to 0, because the Y-axis may be in units that are very large. In other ways, having this value in the original units of Y can be quite helpful as it is easy to envisage what the size of the average residual is.</p>
<p>However, a good rule of thumb is that approximately 95% of the observations (raw datapoints) should fall within plus or minus 2 times the standard error of the estimates from the regression line. We can illustrate this with the following plot:</p>
<div class="sourceCode" id="cb1639"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1639-1"><a href="linear-regression.html#cb1639-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> dan.sleep, <span class="at">y =</span> dan.grump)) <span class="sc">+</span></span>
<span id="cb1639-2"><a href="linear-regression.html#cb1639-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb1639-3"><a href="linear-regression.html#cb1639-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb1639-4"><a href="linear-regression.html#cb1639-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb1639-5"><a href="linear-regression.html#cb1639-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="fl">125.956</span><span class="sc">+</span>s_est<span class="sc">+</span>s_est, <span class="at">slope =</span> <span class="sc">-</span><span class="fl">8.937</span>, <span class="at">color =</span> <span class="st">&#39;red&#39;</span>, <span class="at">lty=</span><span class="dv">2</span>)<span class="sc">+</span></span>
<span id="cb1639-6"><a href="linear-regression.html#cb1639-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="fl">125.956</span><span class="sc">-</span>s_est<span class="sc">-</span>s_est, <span class="at">slope =</span> <span class="sc">-</span><span class="fl">8.937</span>, <span class="at">color =</span> <span class="st">&#39;red&#39;</span>, <span class="at">lty=</span><span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb1639-7"><a href="linear-regression.html#cb1639-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Regression with 2 x Standard Error of the Estimate&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-666-1.png" width="672" /></p>
<p>This is our original scatterplot again. The blue line is still the best fitting trendline. The two dashed red lines are two times <span class="math inline">\(\sigma_{est}\)</span> above and below the blue line respectively. That is they are 2 * 4.332 = 8.644 units of grumpiness above or below the trendline.</p>
<p>If we count up the number of datapoints that our outside of the dotted red lines, we can see that there are six datapoints out of our 100 datapoints that are just outside, although some of these are very close indeed to the red line. This is probably ok as we’re only expecting 95% of datapoints on average to be inside the red lines.</p>
</div>
</div>
<div id="goodness-of-fit-test---f-ratio" class="section level2" number="12.5">
<h2><span class="header-section-number">12.5</span> Goodness of Fit Test - F-ratio</h2>
<p><span class="math inline">\(R^{2}\)</span> and <span class="math inline">\(\sigma_{est}\)</span> are two calculations that help us determine if our regression model (and trendline) is indeed a good fit to the data. A more formal method is to run a statistical test called the Goodness of Fit Test. To do this we calculate an F-ratio that essentially examines how well our trendline (and model) fit the data compared to the null model which is the model where we use the mean of Y as our prediction.</p>
<p>As a reminder, these scatterplots show the difference in performance of our fitted model (left) compared to the null model (right).</p>
<div class="sourceCode" id="cb1640"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1640-1"><a href="linear-regression.html#cb1640-1" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(p1,p2,<span class="at">nrow=</span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-667-1.png" width="672" /></p>
<p>The F-ratio is essentially a method of determining the proportion of residual variance compared to total variance. We can calculate it using the following formula:</p>
<p><span class="math inline">\(F = \frac{SSM/d.f.SSM}{SSR/d.f.SSR}\)</span></p>
<p>Here, SSM refers to the sum of squares for the model (the model sum of squares). This is equal to:</p>
<p><span class="math inline">\(SSM = SST - SSR\)</span></p>
<p>That is, it’s the difference between the total sum of squares (the sum of the squared residuals for the null model) minus the residual sum of squares (the sum of the squared residuals for the fitted model).</p>
<p>The <code>d.f.SSM</code> refers to the degrees of freedom for the model sum of squares, which is equal to the number of predictors in the model. We only have one predictor (<code>dan.sleep</code>), so that means the degrees of freedom are 1.</p>
<p>The <code>d.f.SSR</code> refers to the degrees of freedom for the raw residuals. This is equal to the number of observations minus the number of predictors minus 1. Therefore it is equal to 100 - 1 - 1 = 98 as we had 100 datapoints (or observations).</p>
<p>Let’s calculate this in R. Firstly, we’ll calculate the model sum of squares:</p>
<div class="sourceCode" id="cb1642"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1642-1"><a href="linear-regression.html#cb1642-1" aria-hidden="true" tabindex="-1"></a>SS.mod <span class="ot">&lt;-</span> SS.tot <span class="sc">-</span> SS.resid</span>
<span id="cb1642-2"><a href="linear-regression.html#cb1642-2" aria-hidden="true" tabindex="-1"></a>SS.mod  <span class="co">#8159.868</span></span></code></pre></div>
<pre><code>## [1] 8159.84</code></pre>
<p>Next we divide the model sum of squares and the residual sum of squares by their respective degrees of freedom to get the mean sum of squares for each. <span class="math inline">\(F\)</span> is then calculated by dividing the former by the latter:</p>
<div class="sourceCode" id="cb1644"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1644-1"><a href="linear-regression.html#cb1644-1" aria-hidden="true" tabindex="-1"></a>MS.resid <span class="ot">&lt;-</span> SS.resid <span class="sc">/</span> <span class="dv">98</span></span>
<span id="cb1644-2"><a href="linear-regression.html#cb1644-2" aria-hidden="true" tabindex="-1"></a>MS.resid  <span class="co">#18.76</span></span></code></pre></div>
<pre><code>## [1] 18.76276</code></pre>
<div class="sourceCode" id="cb1646"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1646-1"><a href="linear-regression.html#cb1646-1" aria-hidden="true" tabindex="-1"></a>MS.mod <span class="ot">&lt;-</span> SS.mod <span class="sc">/</span> <span class="dv">1</span></span>
<span id="cb1646-2"><a href="linear-regression.html#cb1646-2" aria-hidden="true" tabindex="-1"></a>MS.mod    <span class="co">#8159.868</span></span></code></pre></div>
<pre><code>## [1] 8159.84</code></pre>
<div class="sourceCode" id="cb1648"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1648-1"><a href="linear-regression.html#cb1648-1" aria-hidden="true" tabindex="-1"></a>Fval <span class="ot">&lt;-</span>  MS.mod <span class="sc">/</span> MS.resid</span>
<span id="cb1648-2"><a href="linear-regression.html#cb1648-2" aria-hidden="true" tabindex="-1"></a>Fval  <span class="co">#434.9</span></span></code></pre></div>
<pre><code>## [1] 434.8955</code></pre>
<p>So <span class="math inline">\(F = 434.9\)</span> which is a large value. Larger values indicate that we were less likely to get a difference between the sum of squares for our fitted and null models by chance alone.</p>
<p>Essentially, the observed value of F is compared to the sampling distribution for F if the null hypothesis is true. The null is that our model (trendline) is no better than random in fitting the datapoints</p>
<p>Whether our <span class="math inline">\(F\)</span> value is sufficiently large can be looked up in an F-table (not recommended), or you can simply let R do the work for you. If you use <code>summary()</code> on your saved regression model, it will give you the <span class="math inline">\(F\)</span> value as well as a p-value to go along with it.</p>
<div class="sourceCode" id="cb1650"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1650-1"><a href="linear-regression.html#cb1650-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = dan.grump ~ dan.sleep, data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -11.025  -2.213  -0.399   2.681  11.750 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 125.9563     3.0161   41.76   &lt;2e-16 ***
## dan.sleep    -8.9368     0.4285  -20.85   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.332 on 98 degrees of freedom
## Multiple R-squared:  0.8161, Adjusted R-squared:  0.8142 
## F-statistic: 434.9 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>You can see from the model output that the F-statistic is given on the bottom row along with the degrees of freedom for the model (1) and the residuals (98). The p-value here is 0.0000000000000022 which is basically 0. Typically, if the p-value is less than 0.05 we will say that our fitted model is a better fit to the data than the null model.</p>
</div>
<div id="assumptions-of-linear-regression" class="section level2" number="12.6">
<h2><span class="header-section-number">12.6</span> Assumptions of Linear Regression</h2>
<p>Even if your model is a good fit to the data, there are still several things you need to check before progressing to examining whether your predictor is ‘significantly’ (a better term is probably ‘meaningfully’) predicting the outcome variable.</p>
<p>The last series of things we need to do is to check whether we have violated the assumptions of linear regression. In short, here are some of the assumptions that we need to adhere to:</p>
<ol style="list-style-type: decimal">
<li>Normality - Specifically the residuals are normally distributed.</li>
<li>Linearity - We need to be examining linear relationships between predictors and outcomes</li>
<li>Homogeneity of Variance (homoscedasticity)</li>
<li>Uncorrelated Predictors (only relevant if doing more than one predictor)</li>
<li>No overly influential datapoints</li>
</ol>
<p>Let’s discuss each of these in turn.</p>
<div id="normality-of-residuals" class="section level3" number="12.6.1">
<h3><span class="header-section-number">12.6.1</span> Normality of Residuals</h3>
<p>One assumption of linear regression is that our residuals are approximately normally distributed. We can check this in several ways. But first, we should probably own up and mention that there are several types of residuals. The residuals we have been dealing with from the regression model have been the <strong>raw residuals</strong>. We got these by simply subtracting the predicted value of y from the observed value of y <span class="math inline">\(residual = y - y&#39;\)</span>. However, statisticians like to modify these residuals. One modification they make is to turn these residuals into what are called <strong>standardized residuals</strong>. These are the raw residuals divided by the standard deviation of the residuals.</p>
<p>You can directly access the standardized residuals in R by using the <code>rstandard()</code> function with your model output:</p>
<div class="sourceCode" id="cb1652"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1652-1"><a href="linear-regression.html#cb1652-1" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>std_resids <span class="ot">&lt;-</span> <span class="fu">rstandard</span>(mod1)</span>
<span id="cb1652-2"><a href="linear-regression.html#cb1652-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1652-3"><a href="linear-regression.html#cb1652-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(df[<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">6</span>,<span class="dv">11</span>)]) <span class="co"># just showing the relevant columns</span></span></code></pre></div>
<pre><code>## # A tibble: 6 x 4
##   dan.sleep dan.grump residuals std_resids
##       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
## 1      7.59        56    -2.11     -0.494 
## 2      7.91        60     4.76      1.10  
## 3      5.14        82     1.99      0.467 
## 4      7.71        55    -2.03     -0.478 
## 5      6.68        67     0.759     0.172 
## 6      5.99        72    -0.409    -0.0991</code></pre>
<p>We can demonstrate that the standardized residuals really are highly correlated with the raw residuals by plotting them on a scatterplot:</p>
<div class="sourceCode" id="cb1654"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1654-1"><a href="linear-regression.html#cb1654-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x=</span>residuals, <span class="at">y=</span>std_resids)) <span class="sc">+</span></span>
<span id="cb1654-2"><a href="linear-regression.html#cb1654-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb1654-3"><a href="linear-regression.html#cb1654-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()<span class="sc">+</span></span>
<span id="cb1654-4"><a href="linear-regression.html#cb1654-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Raw Residuals&quot;</span>)<span class="sc">+</span></span>
<span id="cb1654-5"><a href="linear-regression.html#cb1654-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Standardized Residuals&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-672-1.png" width="672" /></p>
<p>They are effectively the same - just transformed. One advantage of standardized residuals is that they can help us look for unusual datapoints with large residuals. While raw residuals are always in the original units of the y-axis, standardized residuals are, well, standardized. Standardized residuals that are greater than 2 or less than -2 are quite large, those that are larger than 3 or less than 3 are very large indeed and should be looked at in more detail.</p>
<p>Let’s get back to checking the normality of these residuals.</p>
<p>First up, we could plot a histogram and see if we think it’s approximately normal:</p>
<div class="sourceCode" id="cb1655"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1655-1"><a href="linear-regression.html#cb1655-1" aria-hidden="true" tabindex="-1"></a><span class="co">#a) histogram plot</span></span>
<span id="cb1655-2"><a href="linear-regression.html#cb1655-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x=</span>std_resids)) <span class="sc">+</span> <span class="fu">geom_histogram</span>(<span class="at">color=</span><span class="st">&#39;white&#39;</span>) <span class="co"># possibly ok</span></span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-673-1.png" width="672" /></p>
<p>This looks possibly ok. It’s not <strong>too</strong> skewed, but it can be hard from a histogram with just 100 datapoints to get a sense of the shape. A second approach would be to use a Shapiro-Wilk test which more formally test whether the data are approximately normally distributed:</p>
<div class="sourceCode" id="cb1656"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1656-1"><a href="linear-regression.html#cb1656-1" aria-hidden="true" tabindex="-1"></a><span class="co">#b) Shapiro-Wilk test</span></span>
<span id="cb1656-2"><a href="linear-regression.html#cb1656-2" aria-hidden="true" tabindex="-1"></a><span class="fu">shapiro.test</span>(df<span class="sc">$</span>std_resids)  <span class="co"># shapiro test says normal.</span></span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  df$std_resids
## W = 0.99201, p-value = 0.8221</code></pre>
<p>Given that the p-value here is above our cut-off of 0.05, this suggests that we have no evidence to reject the hypothesis that our data came from a normal distribution. An easier way of saying this is, our residuals are likely approximately normally distributed.</p>
<p>The other method we can employ to check normality is to use a QQ plot:</p>
<div class="sourceCode" id="cb1658"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1658-1"><a href="linear-regression.html#cb1658-1" aria-hidden="true" tabindex="-1"></a><span class="co">#c) QQ plot</span></span>
<span id="cb1658-2"><a href="linear-regression.html#cb1658-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(df<span class="sc">$</span>std_resids)</span>
<span id="cb1658-3"><a href="linear-regression.html#cb1658-3" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(df<span class="sc">$</span>std_resids, <span class="at">col =</span> <span class="st">&quot;steelblue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)  <span class="co"># it&#39;s ok</span></span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-675-1.png" width="672" /></p>
<p>A discussion of precisely what these QQ plots are is beyond the scope here. However, in general terms, what we are plotting is the residual against the theoretical value of each residual that we would expect if our data were normally distributed. In practical terms, what we’re looking for is that the bulk of our data fit along the blue line. It’s ok to have a bit of wobble at the extremes - that just means that our data distribution probably has slightly fat tails.</p>
<p>It’s also possible to generate a version of the above plot quickly, directly from the saved linear model object.</p>
<div class="sourceCode" id="cb1659"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1659-1"><a href="linear-regression.html#cb1659-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>( <span class="at">x =</span> mod1, <span class="at">which =</span> <span class="dv">2</span> )  <span class="co"># fast way of getting same plot</span></span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-676-1.png" width="672" /></p>
</div>
<div id="linearity" class="section level3" number="12.6.2">
<h3><span class="header-section-number">12.6.2</span> 2. Linearity —</h3>
<p>The second major assumption of linear regression is that the relationship between our predictor and outcome is linear! So, data that look like the following would not have a linear relationship.</p>
<div class="figure">
<img src="img/curvi.png" alt="" />
<p class="caption">comparing a and b</p>
</div>
<p>One simple approach is to examine the scatterplot of your predictor (X) and outcome (Y) variables. We already did this, and our data looked pretty linear! Another approach is to examine the relationship between your observed Y values and the predicted Y values <span class="math inline">\(y&#39;\)</span>. This should also be a linear relationship.</p>
<p>Although we calculated the <span class="math inline">\(y&#39;\)</span> values earlier using the formula for the regression line, we can actually grab them directly from the model object with the <code>fitted.values()</code> command.</p>
<div class="sourceCode" id="cb1660"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1660-1"><a href="linear-regression.html#cb1660-1" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>Y.fitted <span class="ot">&lt;-</span> <span class="fu">fitted.values</span>(mod1)</span>
<span id="cb1660-2"><a href="linear-regression.html#cb1660-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1660-3"><a href="linear-regression.html#cb1660-3" aria-hidden="true" tabindex="-1"></a><span class="co"># plot this relationship</span></span>
<span id="cb1660-4"><a href="linear-regression.html#cb1660-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> Y.fitted, <span class="at">y =</span> dan.grump)) <span class="sc">+</span> </span>
<span id="cb1660-5"><a href="linear-regression.html#cb1660-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb1660-6"><a href="linear-regression.html#cb1660-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_smooth</span>(<span class="at">method=</span><span class="st">&#39;lm&#39;</span>, <span class="at">se=</span>F) </span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-677-1.png" width="672" /></p>
<p>As you can see from this plot, our fitted (predicted) values of Y on the x-axis have a strong linear relationship with our observed values of Y on the y-axis.</p>
</div>
<div id="homogeneity-of-variance-homoscedasticity" class="section level3" number="12.6.3">
<h3><span class="header-section-number">12.6.3</span> 3. Homogeneity of Variance / Homoscedasticity</h3>
<p>The third assumption that we need to check is homoscedasticity (also sometimes referred to as homogeneity of variance).</p>
<p>What this really means is that the model should be equally good at predicting Y’s across all values of X. Our regression model shouldn’t be better at predicting values of Y for e.g. small values of X but not for large values of X.</p>
<p>Practically, what this means, is that the size of the residuals should be equal across all values of X.</p>
<p>If we plot the values of X (or the predicted/fitted values of Y) on the x-axis against the residuals (in this case standardized residuals) on the Y-axis, then there should be no overall pattern. We should be equally likely to get small or large residuals for any value of X (or predicted/fitted value of Y). This would mean that any trendline on this graph should be a horizontal line.</p>
<div class="sourceCode" id="cb1661"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1661-1"><a href="linear-regression.html#cb1661-1" aria-hidden="true" tabindex="-1"></a><span class="co"># if true, this should be a straight line</span></span>
<span id="cb1661-2"><a href="linear-regression.html#cb1661-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> Y.fitted, <span class="at">y =</span> std_resids)) <span class="sc">+</span> </span>
<span id="cb1661-3"><a href="linear-regression.html#cb1661-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb1661-4"><a href="linear-regression.html#cb1661-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_smooth</span>(<span class="at">method=</span><span class="st">&#39;lm&#39;</span>, <span class="at">se=</span>F) </span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-678-1.png" width="672" /></p>
<p>As you can see, our plot is basically a random scatterplot and there is no overall pattern. That is good, it means we have homoscedasticity. If we did not have homoscedasticity, then we’d see a pattern in this scatterplot - such as the residuals getting larger or smaller for larger fitted (predicted) values of Y.</p>
<p>You can access a version of this plot directly from the model object like this:</p>
<div class="sourceCode" id="cb1662"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1662-1"><a href="linear-regression.html#cb1662-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod1, <span class="at">which =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-679-1.png" width="672" /></p>
<p>There are some more formal tests of homoscedasticity but we don’t need to worry about them.</p>
</div>
<div id="no-colinearity" class="section level3" number="12.6.4">
<h3><span class="header-section-number">12.6.4</span> No Colinearity</h3>
<p>This assumption of linear regression only applies when we have more than one predictor in the model. In our model, we do only have one predictor (<code>dan.sleep</code>) so we don’t need to worry about it. If we had added in another variable into the model, e.g. the amount of hours of baby sleep, then we’d have a second predictor. That means, we’re trying to predict <code>dan.grump</code> based on both <code>dan.sleep</code> and <code>baby.sleep</code>. In this case, <code>dan.sleep</code> and <code>baby.sleep</code> should not be correlated with each other.</p>
</div>
<div id="unusual-datapoints" class="section level3" number="12.6.5">
<h3><span class="header-section-number">12.6.5</span> Unusual Datapoints</h3>
<p>There are a number of ways that datapoints could be unusual. We will discuss data points that are:</p>
<ol style="list-style-type: lower-roman">
<li>outliers</li>
<li>have high leverage</li>
<li>have high influence</li>
</ol>
<p>Generally linear regression models should not be overly affected by individual data points. Usually the category that we most need to be concerned about are points with high influence.</p>
<div id="i.-outliers" class="section level4" number="12.6.5.1">
<h4><span class="header-section-number">12.6.5.1</span> i. Outliers</h4>
<p>Outliers are datapoints that are typically highly unusual in terms of outcome Y but not in terms of the predictor X. These will be datapoints that have high residuals.</p>
<p>Let’s look at an example dataset. We have a predictor ‘x’ and an outcome ‘y’. With ‘y1’ we have the same outcome variables, but have removed the value for one datapoint:</p>
<div class="sourceCode" id="cb1663"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1663-1"><a href="linear-regression.html#cb1663-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">4.1</span>,<span class="fl">4.2</span>,<span class="dv">5</span>,<span class="fl">5.5</span>,<span class="dv">6</span>,<span class="fl">6.1</span>,<span class="fl">6.15</span>,<span class="fl">6.4</span>,<span class="dv">7</span>,<span class="fl">7.2</span>,<span class="fl">7.7</span>,<span class="dv">8</span>,<span class="fl">9.7</span>)</span>
<span id="cb1663-2"><a href="linear-regression.html#cb1663-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">3.9</span>,<span class="fl">4.3</span>,<span class="dv">6</span>,<span class="fl">5.9</span>,<span class="fl">6.1</span>,<span class="fl">11.9</span>,<span class="fl">6.3</span>,<span class="fl">5.8</span>,<span class="fl">7.9</span>,<span class="fl">6.4</span>,<span class="fl">7.8</span>,<span class="dv">8</span>,<span class="fl">9.1</span>)</span>
<span id="cb1663-3"><a href="linear-regression.html#cb1663-3" aria-hidden="true" tabindex="-1"></a>y1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">3.9</span>,<span class="fl">4.3</span>,<span class="dv">6</span>,<span class="fl">5.9</span>,<span class="fl">6.1</span>,<span class="cn">NA</span>,<span class="fl">6.3</span>,<span class="fl">5.8</span>,<span class="fl">7.9</span>,<span class="fl">6.4</span>,<span class="fl">7.8</span>,<span class="dv">8</span>,<span class="fl">9.1</span>)</span>
<span id="cb1663-4"><a href="linear-regression.html#cb1663-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1663-5"><a href="linear-regression.html#cb1663-5" aria-hidden="true" tabindex="-1"></a>ddf <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x,y,y1)</span>
<span id="cb1663-6"><a href="linear-regression.html#cb1663-6" aria-hidden="true" tabindex="-1"></a>ddf</span></code></pre></div>
<pre><code>##       x    y  y1
## 1  4.10  3.9 3.9
## 2  4.20  4.3 4.3
## 3  5.00  6.0 6.0
## 4  5.50  5.9 5.9
## 5  6.00  6.1 6.1
## 6  6.10 11.9  NA
## 7  6.15  6.3 6.3
## 8  6.40  5.8 5.8
## 9  7.00  7.9 7.9
## 10 7.20  6.4 6.4
## 11 7.70  7.8 7.8
## 12 8.00  8.0 8.0
## 13 9.70  9.1 9.1</code></pre>
<p>And here, we’re plotting the best fitting regression line in blue. The dotted red line represents the best fitting trendline that we would have if we removed the datapoints that has the y value of 11.9. The dashed black line shows the distance from this datapoint to the trendline we would have if it was removed.</p>
<pre><code>## (Intercept)           x 
##   0.8247787   0.8785270</code></pre>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-681-1.png" width="672" /></p>
<p>As you can see from this small example, outliers are datapoints that have very large residuals from the trendline. They have an unusually large Y. Notice though that the slope of the trendline hasn’t change too much at all. It is slightly shifted down after you remove that outlier from the calculations, but overall the coefficient of ‘b’ is similar to before. This type of outlier is not necessarily a big deal.</p>
</div>
<div id="ii.-high-leverage" class="section level4" number="12.6.5.2">
<h4><span class="header-section-number">12.6.5.2</span> ii. High Leverage</h4>
<p>Datapoints that have high leverage are those that have a high influence on the regression line’s trajectory, but don’t necessarily affect the angle of the slope. They are typically unusual in terms of their X value, but not necessarily in terms of Y, meaning that they don’t have to have a high residual.</p>
<p>We can measure the leverage of datapoints using the function <code>hatvalues()</code> on the model object. For the scatterplot above, the hat values of each datapoint can be calculated as follows:</p>
<div class="sourceCode" id="cb1666"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1666-1"><a href="linear-regression.html#cb1666-1" aria-hidden="true" tabindex="-1"></a>mod.out <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>x, <span class="at">data =</span> ddf)</span>
<span id="cb1666-2"><a href="linear-regression.html#cb1666-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1666-3"><a href="linear-regression.html#cb1666-3" aria-hidden="true" tabindex="-1"></a>ddf<span class="sc">$</span>hat_val <span class="ot">&lt;-</span> <span class="fu">hatvalues</span>(mod.out)  </span>
<span id="cb1666-4"><a href="linear-regression.html#cb1666-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1666-5"><a href="linear-regression.html#cb1666-5" aria-hidden="true" tabindex="-1"></a>ddf</span></code></pre></div>
<pre><code>##       x    y  y1    hat_val
## 1  4.10  3.9 3.9 0.25535302
## 2  4.20  4.3 4.3 0.24009985
## 3  5.00  6.0 6.0 0.14260536
## 4  5.50  5.9 5.9 0.10381722
## 5  6.00  6.1 6.1 0.08206442
## 6  6.10 11.9  NA 0.07975810
## 7  6.15  6.3 6.3 0.07886047
## 8  6.40  5.8 5.8 0.07692761
## 9  7.00  7.9 7.9 0.08966480
## 10 7.20  6.4 6.4 0.09936183
## 11 7.70  7.8 7.8 0.13552914
## 12 8.00  8.0 8.0 0.16540649
## 13 9.70  9.1 9.1 0.45055168</code></pre>
<p>As you can see, the 6th datapoint was the outlier but it does not have a large leverage as it’s not overly influencing the trajectory of the regression line. Datapoint 13 on the other hand does have a higher leverage. Roughly speaking, a large hatvalue is one which is 2-3 times the average hat value. We can check the mean of the hat values like this:</p>
<div class="sourceCode" id="cb1668"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1668-1"><a href="linear-regression.html#cb1668-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">hatvalues</span>(mod.out))</span></code></pre></div>
<pre><code>## [1] 0.1538462</code></pre>
<p>So clearly, the 13th datapoint does have a high leverage.</p>
</div>
<div id="iii.-high-influence" class="section level4" number="12.6.5.3">
<h4><span class="header-section-number">12.6.5.3</span> iii. High Influence</h4>
<p>The third type of unusual datapoint are ones that you need to be most wary of. These are datapoints that have high influence. Essentially a high influence datapoint is a high leverage datapoint that is also an outlier.</p>
<p>Let’s look at these example data. We are looking at a predictor variable ‘x’ against an outcome variable ‘y2’. ‘y3’ is the same data as ‘y2’ with one datapoint’s value removed.</p>
<div class="sourceCode" id="cb1670"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1670-1"><a href="linear-regression.html#cb1670-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">4.1</span>,<span class="fl">4.2</span>,<span class="dv">5</span>,<span class="fl">5.5</span>,<span class="dv">6</span>,<span class="fl">6.15</span>,<span class="fl">6.4</span>,<span class="dv">7</span>,<span class="fl">7.2</span>,<span class="fl">7.7</span>,<span class="dv">8</span>,<span class="fl">9.7</span>)</span>
<span id="cb1670-2"><a href="linear-regression.html#cb1670-2" aria-hidden="true" tabindex="-1"></a>y2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">3.9</span>,<span class="fl">4.3</span>,<span class="dv">6</span>,<span class="fl">5.9</span>,<span class="fl">6.1</span>,<span class="fl">6.3</span>,<span class="fl">5.8</span>,<span class="fl">7.9</span>,<span class="fl">6.4</span>,<span class="fl">7.8</span>,<span class="dv">8</span>,<span class="fl">6.1</span>)</span>
<span id="cb1670-3"><a href="linear-regression.html#cb1670-3" aria-hidden="true" tabindex="-1"></a>y3 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">3.9</span>,<span class="fl">4.3</span>,<span class="dv">6</span>,<span class="fl">5.9</span>,<span class="fl">6.1</span>,<span class="fl">6.3</span>,<span class="fl">5.8</span>,<span class="fl">7.9</span>,<span class="fl">6.4</span>,<span class="fl">7.8</span>,<span class="dv">8</span>,<span class="cn">NA</span>)</span>
<span id="cb1670-4"><a href="linear-regression.html#cb1670-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1670-5"><a href="linear-regression.html#cb1670-5" aria-hidden="true" tabindex="-1"></a>ddf1 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x,y2,y3)</span>
<span id="cb1670-6"><a href="linear-regression.html#cb1670-6" aria-hidden="true" tabindex="-1"></a>ddf1</span></code></pre></div>
<pre><code>##       x  y2  y3
## 1  4.10 3.9 3.9
## 2  4.20 4.3 4.3
## 3  5.00 6.0 6.0
## 4  5.50 5.9 5.9
## 5  6.00 6.1 6.1
## 6  6.15 6.3 6.3
## 7  6.40 5.8 5.8
## 8  7.00 7.9 7.9
## 9  7.20 6.4 6.4
## 10 7.70 7.8 7.8
## 11 8.00 8.0 8.0
## 12 9.70 6.1  NA</code></pre>
<p>Let’s plot these data - I’m sparing you the code for this plot:</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-685-1.png" width="672" /></p>
<p>What you can see here is the scatterplot of ‘x’ against ‘y2’. The blue trendline is the best fit line to the regression of ‘x’ against ‘y2’. The red dotted line is the regression line when we remove the datapoint at x=9.7. This datapoint has both high leverage and is an outlier (has an overly large residual). As a consequence of this, when you remove this datapoint it significantly shifts the angle of the slope of the regression line. That means that the value of <code>b</code> changes quite considerably.</p>
<p>Let’s add the hat values (measuring leverage) to the data. We can also add what is called <code>Cook's Distance</code> which is a measure of the <strong>influence</strong> of each datapoint.</p>
<div class="sourceCode" id="cb1672"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1672-1"><a href="linear-regression.html#cb1672-1" aria-hidden="true" tabindex="-1"></a>mod.out2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y2<span class="sc">~</span>x, <span class="at">data =</span> ddf1)</span>
<span id="cb1672-2"><a href="linear-regression.html#cb1672-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1672-3"><a href="linear-regression.html#cb1672-3" aria-hidden="true" tabindex="-1"></a>ddf1<span class="sc">$</span>hat_val <span class="ot">&lt;-</span> <span class="fu">hatvalues</span>(mod.out2) </span>
<span id="cb1672-4"><a href="linear-regression.html#cb1672-4" aria-hidden="true" tabindex="-1"></a>ddf1<span class="sc">$</span>cooks_d <span class="ot">&lt;-</span><span class="fu">cooks.distance</span>(mod.out2)</span>
<span id="cb1672-5"><a href="linear-regression.html#cb1672-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1672-6"><a href="linear-regression.html#cb1672-6" aria-hidden="true" tabindex="-1"></a>ddf1</span></code></pre></div>
<pre><code>##       x  y2  y3    hat_val      cooks_d
## 1  4.10 3.9 3.9 0.26609280 0.2940686663
## 2  4.20 4.3 4.3 0.25062833 0.1201644267
## 3  5.00 6.0 6.0 0.15151904 0.0347793264
## 4  5.50 5.9 5.9 0.11178988 0.0026090653
## 5  6.00 6.1 6.1 0.08914853 0.0007585906
## 6  6.15 6.3 6.3 0.08568825 0.0029898530
## 7  6.40 5.8 5.8 0.08333867 0.0085341275
## 8  7.00 7.9 7.9 0.09512926 0.1169635836
## 9  7.20 6.4 6.4 0.10452756 0.0038328585
## 10 7.70 7.8 7.8 0.13998476 0.0808111497
## 11 8.00 8.0 8.0 0.16946124 0.1138881520
## 12 9.70 6.1  NA 0.45269169 2.8757588661</code></pre>
<p>As you can see, datapoint 12 (x=9.7) has high leverage and it’s also an outlier. Consequently it has a very large Cook’s Distance. Typically a Cook’s distance &gt; 1 requires more consideration about how to proceed with your model.</p>
<p>You can also quickly get a plot of the Cook’s Distances of all datapoints from a regression model like this:</p>
<div class="sourceCode" id="cb1674"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1674-1"><a href="linear-regression.html#cb1674-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod.out2, <span class="at">which =</span> <span class="dv">4</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-687-1.png" width="672" /></p>
</div>
<div id="checking-influence" class="section level4" number="12.6.5.4">
<h4><span class="header-section-number">12.6.5.4</span> Checking Influence:</h4>
<p>Let’s look for influential datapoints in our Sleep vs Grumpiness Data. At the beginning of this section, we saved the linear model as <code>mod</code>. We can look at the Cook’s Distances with the <code>cooks.distance()</code> function, or we could just plot them:</p>
<div class="sourceCode" id="cb1675"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1675-1"><a href="linear-regression.html#cb1675-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod1, <span class="at">which =</span> <span class="dv">4</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-688-1.png" width="672" /></p>
<p>This plot shows each of our 100 datapoints in the order they appear in the original dataframe along the x-axis. Clearly, looking at the y-axis, all our datapoints have Cook’s distances of far below 1, so we are fine to assume that we have no overly influential datapoints.</p>
</div>
</div>
</div>
<div id="examining-individual-predictor-estimates" class="section level2" number="12.7">
<h2><span class="header-section-number">12.7</span> Examining individual predictor estimates</h2>
<p>After checking if our model is a good fit and making sure that we did not violate any of the assumptions of the linear regression, we can finally move forward with what we came here to do. That is, we can check whether our predictor is meaningfully useful in predicting our outcome variable. This means, is our observed value of <code>b</code> sufficiently different from 0? To do this, we use two strategies. First, we can generate a 95% confidence interval around our estimate of <code>b</code>. That is the method that we prefer. Secondly, you can run a significance test.</p>
<div id="confidence-interval-of-b." class="section level3" number="12.7.1">
<h3><span class="header-section-number">12.7.1</span> 95% confidence interval of ‘b’.</h3>
<p>First, let’s do this the easy way. Then we’ll work out how we got the 95% confidence interval. You can simply find this by running the <code>confint()</code> function on our regression model object:</p>
<div class="sourceCode" id="cb1676"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1676-1"><a href="linear-regression.html#cb1676-1" aria-hidden="true" tabindex="-1"></a><span class="co"># easy way - gives us the confidence interval:</span></span>
<span id="cb1676-2"><a href="linear-regression.html#cb1676-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coefficients</span>(mod1)</span></code></pre></div>
<pre><code>## (Intercept)   dan.sleep 
##  125.956292   -8.936756</code></pre>
<div class="sourceCode" id="cb1678"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1678-1"><a href="linear-regression.html#cb1678-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(<span class="at">object =</span> mod1, <span class="at">level =</span> .<span class="dv">95</span>)</span></code></pre></div>
<pre><code>##                  2.5 %    97.5 %
## (Intercept) 119.971000 131.94158
## dan.sleep    -9.787161  -8.08635</code></pre>
<p>This tells us that the lower bound of the 95% confidence interval of <code>b</code> is -9.79, and the upper bound is -8.09, with our estimate of <code>b</code> being -8.94. This means that if we were to sample 100 days randomly over and over again and measure Dan’s sleep and grumpiness on these days, in 95% of the samples we collect we would have the true population parameter value of <code>b</code>. In lay terms, we can suggest that there is approximately a 95% likelihood that this true population value of <code>b</code> is between -9.79 and -8.09. The only way to really get the true population value would have been to measure Dan’s sleep and grumpiness for every day that Dan has been alive.</p>
<p>In big picture terms, what we can draw from this 95% confidence interval is that the relationship between sleep and grumpiness is highly negative and clearly strong. There really seems to be close to no chance that 0 could be the value of <code>b</code> - it is not inside the confidence interval. In fact, we could generate a ridiculously confidence confidence interval, such as a 99.9999% confidence interval:</p>
<div class="sourceCode" id="cb1680"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1680-1"><a href="linear-regression.html#cb1680-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(<span class="at">object =</span> mod1, <span class="at">level =</span> .<span class="dv">999999</span>)</span></code></pre></div>
<pre><code>##             0.00005 % 99.99995 %
## (Intercept) 110.21037 141.702208
## dan.sleep   -11.17398  -6.699536</code></pre>
<p>… and 0 is still nowhere near being included.</p>
<p>How is this 95% confidence interval calculated? To do this we need to think about those sampling distributions again, and the standard error of <code>b</code>.</p>
</div>
<div id="standard-error-of-b" class="section level3" number="12.7.2">
<h3><span class="header-section-number">12.7.2</span> Standard Error of b</h3>
<p>As we’ve discussed earlier, our one single estimate of <code>b</code> is just one possible estimate. We estimated this value based on our one single sample of 100 days. However, if we had sampled a different sample of 100 days, we would have got a slightly (or maybe greatly) different estimate of <code>b</code>. If we repeated this procedure thousands of times, then we’d have a sampling distribution of <code>b</code>’s.</p>
<p>This sampling distribution will be t-distribution shaped and has degrees of freedom = number of observations - the number of predictors (1) - 1. Therefore it is t-distribution shaped with d.f.=98. If we know the standard deviation of this sampling distribution (which is known as the standard error of <code>b</code> - <span class="math inline">\(s_{b}\)</span>), then we can create confidence intervals.</p>
<p>However, it turns out that calculating <span class="math inline">\(s_{b}\)</span> is a bit annoying. Fortunately, there is a quick way to find it out by looking at the summary of the model output:</p>
<div class="sourceCode" id="cb1682"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1682-1"><a href="linear-regression.html#cb1682-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = dan.grump ~ dan.sleep, data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -11.025  -2.213  -0.399   2.681  11.750 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 125.9563     3.0161   41.76   &lt;2e-16 ***
## dan.sleep    -8.9368     0.4285  -20.85   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.332 on 98 degrees of freedom
## Multiple R-squared:  0.8161, Adjusted R-squared:  0.8142 
## F-statistic: 434.9 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We can see here that <span class="math inline">\(s_{b} = 0.4285\)</span></p>
<p>If you’re interested, then this is the formula to calculate <span class="math inline">\(s_{b}\)</span>:</p>
<p><span class="math inline">\(s_{b} = \frac{\sigma_{est}}{\sqrt{\Sigma (x - \overline{x})}}\)</span></p>
<p>Here’s how we calculate it by hand according to this formula:</p>
<div class="sourceCode" id="cb1684"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1684-1"><a href="linear-regression.html#cb1684-1" aria-hidden="true" tabindex="-1"></a>x_dif <span class="ot">&lt;-</span> df<span class="sc">$</span>dan.sleep <span class="sc">-</span> <span class="fu">mean</span>(df<span class="sc">$</span>dan.sleep) <span class="co"># difference of each x from mean of x</span></span>
<span id="cb1684-2"><a href="linear-regression.html#cb1684-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1684-3"><a href="linear-regression.html#cb1684-3" aria-hidden="true" tabindex="-1"></a>ssx <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>(x_dif<span class="sc">^</span><span class="dv">2</span>))  <span class="co"># square root of the sum of these differences squared</span></span>
<span id="cb1684-4"><a href="linear-regression.html#cb1684-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1684-5"><a href="linear-regression.html#cb1684-5" aria-hidden="true" tabindex="-1"></a>sb <span class="ot">&lt;-</span> s_est<span class="sc">/</span>ssx  <span class="co">#s_est was calculated earlier</span></span>
<span id="cb1684-6"><a href="linear-regression.html#cb1684-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1684-7"><a href="linear-regression.html#cb1684-7" aria-hidden="true" tabindex="-1"></a>sb   <span class="co"># this is the standard error of b</span></span></code></pre></div>
<pre><code>## [1] 0.4285351</code></pre>
</div>
<div id="calculating-95-confidence-interval-of-b-by-hand" class="section level3" number="12.7.3">
<h3><span class="header-section-number">12.7.3</span> Calculating 95% confidence interval of ‘b’ by hand</h3>
<p>We can use the following formula to calculate the 95% CI for <code>b</code>:</p>
<p><span class="math inline">\(CI_{95} = b +/- t * s_{b}\)</span></p>
<p>As with all confidence intervals, what we do is to presume that our estimate of <code>b</code> is the mean of the sampling distribution. Then knowing that the distribution is t-shaped with d.f.=98, we need to find the value of <code>t</code> that will leave 2.5% of the distribution in each tail. We can look that up in R using <code>qt</code>:</p>
<div class="sourceCode" id="cb1686"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1686-1"><a href="linear-regression.html#cb1686-1" aria-hidden="true" tabindex="-1"></a>tval <span class="ot">&lt;-</span> <span class="fu">qt</span>(.<span class="dv">975</span>, <span class="at">df =</span> <span class="dv">98</span>)</span>
<span id="cb1686-2"><a href="linear-regression.html#cb1686-2" aria-hidden="true" tabindex="-1"></a>tval</span></code></pre></div>
<pre><code>## [1] 1.984467</code></pre>
<p>We’ve now calculated everything we need to for the Confidence Interval</p>
<div class="sourceCode" id="cb1688"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1688-1"><a href="linear-regression.html#cb1688-1" aria-hidden="true" tabindex="-1"></a>b <span class="sc">+</span> (tval <span class="sc">*</span> sb)  <span class="co">#upper bound</span></span></code></pre></div>
<pre><code>## [1] -8.086342</code></pre>
<div class="sourceCode" id="cb1690"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1690-1"><a href="linear-regression.html#cb1690-1" aria-hidden="true" tabindex="-1"></a>b <span class="sc">-</span> (tval <span class="sc">*</span> sb) <span class="co"># lower bound</span></span></code></pre></div>
<pre><code>## [1] -9.78717</code></pre>
<p>As we can see, these match up with the output when using <code>confint()</code>:</p>
<div class="sourceCode" id="cb1692"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1692-1"><a href="linear-regression.html#cb1692-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(mod1)</span></code></pre></div>
<pre><code>##                  2.5 %    97.5 %
## (Intercept) 119.971000 131.94158
## dan.sleep    -9.787161  -8.08635</code></pre>
</div>
<div id="signifcance-testing-b" class="section level3" number="12.7.4">
<h3><span class="header-section-number">12.7.4</span> Signifcance Testing b</h3>
<p>We can also use this sampling distribution to apply a significance test. Our null hypothesis will be that the population value of <code>b</code> is 0. Our alternative hypothesis will be that <code>b</code> is not equal to 0.</p>
<p><br>
<span class="math inline">\(H_{0}: b=0\)</span>
<br>
<span class="math inline">\(H_{1}: b\neq0\)</span>
<br></p>
<p>Given we know the standard deviation of this sampling distribution, <span class="math inline">\(s_{b}\)</span>, we can calculate how far away our observed sample value of <span class="math inline">\(b\)</span> is in terms of how many standard deviations from the mean of 0 it is. We call this value a t-statistic and it is calculated like this:</p>
<p><span class="math inline">\(t = \frac{b}{s_{b}}\)</span></p>
<p>Once we have calculated our t-statistic, then we can determine given the shape of the distribution and the degrees of freedom, what proportion of the distribution is more extreme than our observed value.</p>
<div class="sourceCode" id="cb1694"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1694-1"><a href="linear-regression.html#cb1694-1" aria-hidden="true" tabindex="-1"></a>tobs <span class="ot">&lt;-</span> b<span class="sc">/</span>sb</span>
<span id="cb1694-2"><a href="linear-regression.html#cb1694-2" aria-hidden="true" tabindex="-1"></a>tobs  <span class="co">#-20.854</span></span></code></pre></div>
<pre><code>## [1] -20.8542</code></pre>
<p>Our observed value is therefore <span class="math inline">\(t = -20.854\)</span>.</p>
<p>Let’s look at this graphically:</p>
<div class="figure">
<img src="img/tdf98.png" alt="" />
<p class="caption">comparing a and b</p>
</div>
<p>This shows the t-distribution for d.f. = 98. It looks very strange because we’ve extended the axes to -21 and +21. We did this so that we could include a dotted blue line for our observed t-value of -20.854. Notice that by the time the t-distribution of d.f.=98 gets to close to a value of t=-3 then there is almost nothing left in the tail of the distribution.</p>
<p>Just for completeness, we can calculate what proportion of times we observe a t-value of more extreme (i.e. less than) -20.854 using <code>pt()</code>.</p>
<div class="sourceCode" id="cb1696"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1696-1"><a href="linear-regression.html#cb1696-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pt</span>(<span class="sc">-</span><span class="fl">20.854</span>, <span class="at">df=</span><span class="dv">98</span>)</span></code></pre></div>
<pre><code>## [1] 4.094577e-38</code></pre>
<p>This essentially gives us a one-tailed p-value of p = 0.00000000000000000000000000000000000004094577, which is very small. To get a 2-tailed p-value we just double this value.</p>
<p>Essentially what we can conclude here is that our one value of <code>b</code> is extremely unlikely to have come from a population where <code>b=0</code>, and thus we reject our null hypothesis and accept the alternative.</p>
<p>The preceding information was provided to help you think about what’s really going on when we calculate the t-statistic. However, it turns out there is actually a shortcut way of calculating <code>t</code> using <code>n</code> and the Pearson’s correlation coefficient <code>r</code>.</p>
<p>It’s the following formula:</p>
<p><span class="math inline">\(t = \frac{r \times \sqrt{n-2}}{\sqrt{1 - r^{2}}}\)</span></p>
<div class="sourceCode" id="cb1698"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1698-1"><a href="linear-regression.html#cb1698-1" aria-hidden="true" tabindex="-1"></a>r <span class="ot">&lt;-</span> <span class="fu">cor</span>(df<span class="sc">$</span>dan.sleep, df<span class="sc">$</span>dan.grump)</span>
<span id="cb1698-2"><a href="linear-regression.html#cb1698-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1698-3"><a href="linear-regression.html#cb1698-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(df)</span>
<span id="cb1698-4"><a href="linear-regression.html#cb1698-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1698-5"><a href="linear-regression.html#cb1698-5" aria-hidden="true" tabindex="-1"></a>(r <span class="sc">*</span> <span class="fu">sqrt</span>(n<span class="dv">-2</span>)) <span class="sc">/</span> (<span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">-</span>r<span class="sc">^</span><span class="dv">2</span>))  <span class="co"># -20.85</span></span></code></pre></div>
<pre><code>## [1] -20.8544</code></pre>
<p>As you can see from the above code - it works!</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="correlation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="permutation-testing.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/jalapic/introstats/edit/master/12-Regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/jalapic/introstats/blob/master/12-Regression.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
